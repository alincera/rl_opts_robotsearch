[
  {
    "objectID": "tutorials/tutorial_imitation.html",
    "href": "tutorials/tutorial_imitation.html",
    "title": "Imitation learning",
    "section": "",
    "text": "In this brief tutorial we show how to reproduce the Figures 6 and 7c of the Appendix of our paper. The goal is to show that projective simulation (PS) can imitate the policy of an agent moving following a certain step length distribution. We focus on two distributions: Lévy and Bi-exponential.\nFirst, let’s load the needed libraries and functions. See that rl_opts needs to be already installed (see instructions in the README file)."
  },
  {
    "objectID": "tutorials/tutorial_imitation.html#lévy-distributions-fig.-6a",
    "href": "tutorials/tutorial_imitation.html#lévy-distributions-fig.-6a",
    "title": "Imitation learning",
    "section": "Lévy distributions (Fig. 6a)",
    "text": "Lévy distributions (Fig. 6a)\nWe consider distribution of the type \\(P(L)=L^{-1-\\beta}\\), with various \\(\\beta\\). To understand how the following code works, you can check the example shown in the documentation of PS_imitation. Here we just do the same but looping over various \\(\\beta\\).\n\nNUM_STATES = 100 # size of the state space\nEPOCHS = 100 # number of epochs\nNUM_STEPS = 1000 # number of learning steps per episode\n\n\nbetas = [0.5, 1, 1.5, 2]\nhmatrix_pw = np.zeros(tqdm(len(betas), NUM_STATES))\n\nfor idxb, beta in enumerate((betas)):\n    \n    # For every beta, we sample steps from the corresponding powerlaw (Levy dist.)    \n    steps = pdf_discrete_sample(beta = beta,\n                                pdf_func = pdf_powerlaw,                                \n                                L_max = NUM_STATES,  \n                                num_samples = (EPOCHS, NUM_STEPS))\n    \n    # We define the imitator and train it\n    imitator = PS_imitation(num_states = NUM_STATES,\n                            eta = int(1e-7),\n                            gamma = 0)\n    for e in (range(EPOCHS)):\n        imitator.reset()\n        for s in steps[e]:    \n            imitator.update(length = s)\n            \n    # We only save the turn probability\n    hmatrix_pw[idxb] = imitator.h_matrix[1]/imitator.h_matrix.sum(0)\n\nNow hmatrix_pw contains the turn probability of an imitator agent for every \\(\\beta\\). We can now plot this and compare it with the theoretical prediction, calculated by means of the get_policy_from_dist function.\n\nfig, ax_pw = plt.subplots(figsize = (3,3))\ncolor = plt.cm.plasma(np.linspace(0,1,len(betas)+1))\n\nfor idx, (h, beta) in enumerate(zip(hmatrix_pw, betas)):\n    \n    #---- Analytical solution ----#\n    theory = get_policy_from_dist(n_max = NUM_STATES, \n                                  func = pdf_powerlaw,\n                                  beta = beta)\n    ax_pw.plot(np.arange(2, NUM_STATES+1), 1-theory[1:], c = color[idx])\n    \n    #---- Numerical solution ----#\n    ax_pw.plot(np.arange(2, NUM_STATES+2), h, 'o', \n               c = color[idx],  label = fr'$\\beta$ = {beta}', alpha = 0.8, markeredgecolor='None', lw = 0.05)\n\n#---- Plot features ----#\nplt.setp(ax_pw, xlim = (1.8, 30), ylim = (0.0, 1.01),\n         xlabel =r'$n$', ylabel = r'$\\pi_s(\\Rsh|n)$', \n         yticks = np.round(np.arange(0.2, 1.01, 0.2),1),\n         yticklabels = np.round(np.arange(0.2, 1.01, 0.2),1).astype(str),\n         xscale = 'log')\nax_pw.plot(10, 10, label = 'Theory', c = 'k')   \nax_pw.legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "tutorials/tutorial_imitation.html#bi-exponentials-fig.-6b",
    "href": "tutorials/tutorial_imitation.html#bi-exponentials-fig.-6b",
    "title": "Imitation learning",
    "section": "Bi-exponentials (Fig. 6b)",
    "text": "Bi-exponentials (Fig. 6b)\nWe consider here distributions of the form \\[\n\\Pr(L) = \\sum_{i=1,2} \\omega_i (1-e^{-1/\\lambda_i}) e^{-(L-1)/\\lambda_i} \\, ,\n\\] with \\(\\omega = [0.94, 0.06]\\), \\(\\lambda_2 = 5000\\) and varying \\(\\lambda_1\\).\n\nNUM_STATES = 100 # size of the state space\nEPOCHS = 100 # number of epochs\nNUM_STEPS = 1000 # number of learning steps per episode\n\n# Bi-exponential parameters (lambda_1 will vary)\nomegas = np.array([0.94, 0.06])\nlambdas = np.array([0, 5000]).astype(float)\nlambdas_1 = [0.6, 0.6*2, 0.6*8, 0.6*16]\n\n# Array saving the results\nhmatrix_bi = np.zeros((len(lambdas_1), NUM_STATES))\n\nfor idx_l, lambda_1 in enumerate(tqdm(lambdas_1)):\n    \n    lambdas[0] = lambda_1\n    steps = pdf_discrete_sample(pdf_func = pdf_multimode,\n                                lambdas = lambdas,\n                                probs = omegas,\n                                L_max = NUM_STATES,  \n                                num_samples = (EPOCHS, NUM_STEPS))\n\n    imitator = PS_imitation(num_states = NUM_STATES,\n                            eta = int(1e-7),\n                            gamma = 0)\n\n\n    for e in (range(EPOCHS)):\n        imitator.reset()\n        for s in steps[e]:    \n            imitator.update(length = s)\n    # We only save the turn probability\n    hmatrix_bi[idx_l] = imitator.h_matrix[1]/imitator.h_matrix.sum(0)\n\n\n\n\n\nfig, ax_bi = plt.subplots(figsize = (4,4))\ncolor = plt.cm.inferno(np.linspace(0,1,len(lambdas_1)+1))\n\n############# Powerlaw #################\nfor idx, (lambda_1, h) in enumerate(zip(lambdas_1, hmatrix_bi)):  \n    \n    #---- Analytical solution ----#\n    lambdas[0] = lambda_1     \n    theory = get_policy_from_dist(n_max = NUM_STATES, \n                        func = pdf_multimode,\n                        lambdas = lambdas,\n                        probs = omegas,)\n    ax_bi.plot(np.arange(2, NUM_STATES+1), 1-theory[1:], c = color[idx])\n    \n    #---- Numerical solution ----#\n    ax_bi.plot(np.arange(2, NUM_STATES+2), h, 'o',               \n               c = color[idx], \n               label = fr'$d_1$ = {np.round(lambda_1,1)}',\n               alpha = 0.8, markeredgecolor='None')\n\n\n#---- Plot features ----#\nplt.setp(ax_bi, xlim = (1.8, 30), ylim = (0.0, 1.01),\n         xlabel =r'$n$', ylabel = r'$\\pi_s(\\Rsh|n)$', \n         yticks = np.round(np.arange(0.2, 1.01, 0.2),1),\n         yticklabels = np.round(np.arange(0.2, 1.01, 0.2),1).astype(str),\n         xscale = 'log')\nax_bi.plot(10, 10, label = 'Theory', c = 'k')   \nax_bi.legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "tutorials/tutorial_imitation.html#effect-of-a-cutoff-l_max-fig.-7c",
    "href": "tutorials/tutorial_imitation.html#effect-of-a-cutoff-l_max-fig.-7c",
    "title": "Imitation learning",
    "section": "Effect of a cutoff \\(L_{max}\\) (Fig. 7c)",
    "text": "Effect of a cutoff \\(L_{max}\\) (Fig. 7c)\nIn our paper we explain that introducing a cutoff in the distribution used by the expert in the imitation scheme affects the resulting policy of the agent. Here we show this effect, using one of the previous bi-exponential policies as example:\n\n# Simulation parameters\nNUM_STATES = 1000 # size of the state space\nEPOCHS = 100 # number of epochs\nNUM_STEPS = 1000 # number of learning steps per episode\n\n# Distribution paremeters\nomegas = np.array([0.94, 0.06])\nlambdas = np.array([0.6, 5000])\n# Get theoretical policy (without cutoff)\ntheory_nocutoff =  get_policy_from_dist(n_max = NUM_STATES, \n                              func = pdf_multimode,\n                              lambdas = lambdas,\n                              probs = omegas,)\n\n\n# Setting a max step length\nL_cutoffs = [30, 150, 500, 2000, 10000]\n# To make the loop more efficient, we sample now all steps,\n# which we will then cut if the are bigger than L_cutoff\nsteps_og = pdf_discrete_sample(pdf_func = pdf_multimode,\n                                lambdas = lambdas,\n                                probs = omegas,\n                                L_max = NUM_STATES,  \n                                num_samples = EPOCHS*NUM_STEPS)\n\n\nhmatrix_co = np.zeros((len(L_cutoffs), NUM_STATES))\nfor idx_c, L_cutoff in enumerate(tqdm(L_cutoffs)):\n\n    # Copy steps we sampled above and apply cutoff. \n    # We re-generate the cutted steps\n    steps = steps_og.copy()\n    while np.max(steps) > L_cutoff:\n        steps[steps > L_cutoff] = pdf_discrete_sample(pdf_func = pdf_multimode,\n                                                      lambdas = lambdas,\n                                                      probs = omegas,\n                                                      L_max = NUM_STATES,\n                                                      num_samples = len(steps[steps > L_cutoff]))\n    steps = steps.reshape(EPOCHS, NUM_STEPS)\n    \n    # Define PS imitator\n    imitator = PS_imitation(num_states = NUM_STATES,\n                            eta = int(1e-7),\n                            gamma = 0)\n    # Training\n    for e in (range(EPOCHS)):\n        imitator.reset()\n        for s in steps[e]:         \n            imitator.update(length = s)\n    # Saving\n    hmatrix_co[idx_c] = imitator.h_matrix[1]/imitator.h_matrix.sum(0)\n\n\n\n\n\nfig, ax_co = plt.subplots(figsize = (6, 3))\ncolor = plt.cm.cividis(np.linspace(0,1,len(L_cutoffs)+1))\n\nfor idx, (h, L_cutoff) in enumerate(zip(hmatrix_co, L_cutoffs)):  \n    #---- Numerical solutions ----#\n    ax_co.plot(np.arange(2, NUM_STATES+2), h, 'o',               \n               c = color[idx],  label = r'$L_{max}$ = '+f'{L_cutoff}', \n               alpha = 0.8, markeredgecolor='None', rasterized=True)\n    ax_co.plot(np.arange(2, NUM_STATES+2), h,                \n               c = color[idx],  alpha = 0.2)\n\n#---- Analytical solutions ----#\nax_co.plot(np.arange(2, NUM_STATES+1), 1-theory_nocutoff[1:], '-', \n           c = 'k', alpha = 0.8, label = r'$L_{max}\\rightarrow \\infty$') \n\n#---- Plot features ----#\nax_co.axhline(0.5, c = 'k', ls = '--', alpha = 0.5, zorder = -1)\nax_co.text(1.5, 0.52, r'$\\pi_0$', alpha = 0.5)\nplt.legend(loc = 'upper right', fontsize = 8)    \nplt.setp(ax_co, xlabel =r'$n$', ylabel = r'$\\pi(\\Rsh|n)$', xscale = 'log')\n\n[Text(0.5, 0, '$n$'), Text(0, 0.5, '$\\\\pi(\\\\Rsh|n)$'), None]"
  },
  {
    "objectID": "tutorials/tutorial_learning.html",
    "href": "tutorials/tutorial_learning.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "All the following tutorials require the installation of rl_opts (see instructions in the README file).\nIn order to train an RL agent, we need to have (i) an environment and (ii) a learning method. In this work, we define a foraging environment where the goal of the agent is to find as many targets as possible in a given time. We consider environments with non-destructive -or replenishable- targets, which we implement by displacing the agent a distance \\(l_\\textrm{c}\\) from the center of the found target.\nAs for the agent, we use Projective Simulation (PS) to model its decision making process and learning method. However, other algorithms that work with stochastic policies can also be used.\nFirst, we import the classes that define the environment (TargetEnv), the forager dynamics (Forager), and its learning method.\nNote: the class Forager as it currently is inherits the methods of a PS agent for decision making and learning. However, other learning algorithms can be directly implemented by changing this inheritance. The learning algorithm should contain a method for decision making, called deliberate, which inputs a state; and another one for updating the policy, called learn, which inputs a reward.\nWe set up the parameters defining the length of the episodes (number of RL steps) and the number of episodes.\nWe initialize the environment.\nWe initialize the agent. As states, the agent perceives the value of an internal counter that keeps track of the number of small steps that it has performed without turning. The possible actions are continue walking in the same direction or turning. The agent performs a small step of length \\(d=1\\) in any case after making a decision. Let’s define the parameters of the PS forager agent and initialize it:\nWe run the learning process.\nNote: the code can directly accomodate environments with several agents that interact. For this reason, you will find methods in both the environment class TargetEnv and the forager class Forager that deal with agents that have visual cones and can perceive the presence of other agents in their surroundings. However, these features are not used in this work."
  },
  {
    "objectID": "tutorials/tutorial_learning.html#reproduction-of-results",
    "href": "tutorials/tutorial_learning.html#reproduction-of-results",
    "title": "Reinforcement Learning",
    "section": "Reproduction of results",
    "text": "Reproduction of results\nHere, we explain how to reproduce the results of the paper that concern the training of RL agents in the foraging environment.\n\nTraining\nYou can run the training that is detailed above by means of the method learning, which also saves the agent’s memory periodically.\nFirst, import learning:\n\nfrom rl_opts.learn_and_bench import learning\n\nlearning inputs a configuration dictionary (config), a path to the folder where the results are saved (results_path) and the agent’s identifier (run). Let us detail each input separately.\n\nConfiguration dictionary (config): it contains the parameters to initialize both the environment and the agent. For each set of parameters we ran, there is an identifier of the form “exp_numconfig” (e.g. exp_0) that uniquely identifies the config file. The config files for the experiments that give the results of the paper can be found in the directory ‘configurations/learning/’.\n\nThese are the parameters that you can find in the config files:\nNUM_TARGETS : number of targets\nWORLD_SIZE : side of the square that defines the world (with periodic boundary conditions)\nr : target detection radius\nlc : cutoff length\nMAX_STEP_L : maximum value of the step counter (which coincides with the number of RL steps per episode)\nNUM_BINS : number of bins in which the state space is split. This is set to have one state per value of the counter\nNUM_ACTIONS : number of actions\nGAMMA : forgetting parameter \\(\\gamma\\) in PS\nETA_GLOW : glow damping parameter \\(\\eta_g\\) in PS\nPI_INIT : policy initialization \\(\\pi_0\\) (\\(\\forall n\\)). Note that it is given as \\(\\pi_0(\\uparrow|n)\\)\nNUM_EPISODES : number of episodes\nWe study foraging in enviroments with different cutoff lengths \\(l_\\textrm{c}\\). Exp_0 corresponds to \\(l_\\textrm{c}=0.6\\). Exp_1..10 correspond to \\(l_\\textrm{c}=1..10\\), respectively. In experiments exp_0..10, the initialization policy is \\(\\pi_0(\\Rsh|n)=0.01\\) \\(\\forall n\\). Exp_11 and exp_12 correspond to experiments where the initialization policy is \\(\\pi_0(\\Rsh|n)=0.5\\) \\(\\forall n\\). Each experiment is run with 10 independent, different agents (run \\(\\in [0,9]\\)).\nAs an example, you can import the configuration from experiment exp_8 by running:\n\nfrom rl_opts.utils import get_config\n\nconfig = get_config('exp_8.cfg')\n\nAlternatively, you can also define your own config dictionary with the parameters detailed above:\n\nmy_config = {'NUM_TARGETS' : 100,\n             'WORLD_SIZE' : 100,\n             'r' : 0.5,\n             'lc' : 2,\n             'MAX_STEP_L' : 100,\n             'NUM_BINS' : 100,\n             'NUM_ACTIONS' : 2,\n             'GAMMA' : 0.00001,\n             'ETA_GLOW' : 0.1,\n             'PI_INIT' : 0.99,\n             'NUM_EPISODES' : 500}\n\n\nResults path (results_path): Path where you want to save the results. The agent’s memory (h matrix) is saved every 500 episodes on the file ‘memory_agent…’ (e.g. ‘memory_agent_0_episode_500.npy’).\n\n\nresults_path = 'results/learning/test/'\n\n\nAgent’s identifier (run): integer that identifies the agent. With this identifier, you can later retrieve the agent’s memory or its performance (see the following section on Postlearning analysis).\n\nAfter defining the inputs, you can run the learning:\n\nlearning(my_config, results_path, run=0)\n\nOnce the training is finished, you can get the policy of the agent (as \\(\\pi(\\uparrow|n)\\)) at any of the episodes in which the memory was saved by running:\n\nfrom rl_opts.utils import get_policy\n\nsaved_policy = get_policy(results_path, run=0, training_episode=500)\n\nNote: in the code, the policies are always given as \\(\\pi(\\uparrow|n)\\).\nFig. 3 and Fig. 4 show the policies of the agents at the end of a training consisting of 12000 episodes of 20000 RL steps each. The policies can be retrieved with get_policy as detailed above, by setting training_episode = 12000 and the corresponding agent identifier.\n\n\nPostlearning analysis\nIn order to fairly compare the performance of the RL agents throughout the training with that of the benchmark models (Fig. 2), we need to run the same number of walks. In the training, the agent’s policy changes from one episode to the next one, and taking the efficiency of just one episode -i.e. one walk- is not enough since we consider \\(10^4\\) walks for the benchmark policies. Thus, we save the agent’s policy at different stages of the training and then, in a postlearning analysis, we run \\(10^4\\) walks with that frozen policy to get a more accurate evaluation of its performance.\nThis performance analysis is done with the method agent_efficiency, which is imported by running:\n\nfrom rl_opts.learn_and_bench import agent_efficiency\n\nTo run it, you first need to define:\n\nThe results path from where it retrieves the agent’s memory at different stages of the training. Thus, it needs to be the same path where you saved the results of the training. The results of this analysis are also saved there.\nThe configuration file you used to train the agent. To reproduce the results from Fig. 2, first get the corresponding config file as detailed in the previous section.\nThe agent’s identifier.\nThe number of walks. To reproduce the results from Fig. 2, set this parameter to 10000.\nAn episode interval. This function analyzes the performance of the agent at different stages of the training. To reproduce our results from Fig. 2, you should set this parameter to 2000, which means the performance is analyzed every 2000 episodes, until the end of the training.\n\nTo do the postlearning analysis on the example of the previous section, you run:\n\nagent_efficiency(results_path, my_config, run=0, num_walks=100, episode_interval=500)\n\nEssentially, this analysis is carried out by the method walk_from_policy, which inputs a policy (that is not changing) and runs the walks in parallel. It outputs a list with the efficiency achieved in each walk.\nYou can find the results of, for example, the last episode, in the file ‘performance_post_training_agent_0_episode_500.npy’.\nTo get an array with the average performances (over the number of walks) of several agents throughout the training, you can run:\n\nfrom rl_opts.utils import get_performance\n\nag_list = [0] #in this example, we only ran one agent, but you can input here the identifiers of all the agents you ran.\nep_list = [500] #get the performance at episode 500 of the agents in ag_list.\n\nav_performance, sem = get_performance(results_path, agent_list=ag_list, episode_list=ep_list)"
  },
  {
    "objectID": "tutorials/tutorial_benchmarks.html",
    "href": "tutorials/tutorial_benchmarks.html",
    "title": "Benchmarks",
    "section": "",
    "text": "We consider as benchmark models the discrete Lévy distribution and the bi-exponential distribution, given by equations\n\\[\\Pr(L)=\\zeta^{-1}_{(1+\\beta, 1)} L^{-1-\\beta}\\,,\\] and \\[\\Pr(L) = \\sum_{i=1,2} \\omega_i (1-e^{-1/d_i}) e^{-(L-1)/d_i} \\, ,\\] respectively, where \\(\\zeta_{(1+\\beta, 1)}=\\sum_{\\ell=0}^\\infty (\\ell+1)^{-1-\\beta}\\) is the Riemann zeta function, \\(d_i\\) are length scales and the mode weights satisfy \\(\\sum_{i=1,2} \\omega_i=1\\).\nWe transform the step length distributions into policies with Eq. (5), which is implemented in the code with the method policy_from_distr.\nThis method inputs (i) the parameter/s of the model, which is the exponent \\(\\beta\\) for the Lévy distribution and \\(d_1\\), \\(d_2\\), \\(\\omega_1\\) for the bi-exponential; (ii) the maximum value of step counter; and (iii) the model (either ‘powerlaw’ or ‘double_exp’).\nWe employ the library Tune for parameter optimization, which allows us to optimize the average search efficiency over a number a walks mean_eff with respect to the model parameters."
  },
  {
    "objectID": "tutorials/tutorial_benchmarks.html#change",
    "href": "tutorials/tutorial_benchmarks.html#change",
    "title": "Benchmarks",
    "section": "CHANGE:",
    "text": "CHANGE:\nThe function mean_eff to optimize is computed via average_search_efficiency, and then reported to tune. All this\n\nfrom rl_opts.learn_and_bench import average_search_efficiency\n\nThe efficiency of each walk is again computed with walk_from_policy (see also tutorial on learning), but in this case, the policy is obtained from the benchmark distribution. Note that we save the efficiency achieved in each walk to be able to later retrieve the mean and the SEM.\nIn this case, the input config is a dictionary with the parameter ranges that the optimization algorithm will consider.\nThe parameters of that dictionary are described below:\nModel (we input parameter ranges here):\n\nd_int : small scale (\\(d_1\\), first mode) of the bi-exponential distribution\n\nd_ext : large scale (\\(d_2\\), second mode) of the bi-exponential distribution\n\np : weight of the first mode (\\(\\omega_1\\)) in the bi-exponential distribution\n\nbeta : exponent of the Lévy distribution\n\nmodel : model description (either ‘powerlaw’ or ‘double_exp’)\n\nWalks (we input a single value that is fixed throughout the optimization):\n\ntime_ep : number of (small, \\(d=1\\)) steps per walk. We choose the same value for the benchmarks as for the episodes in the RL training\n\nn : number of walks (also referred to as agents in the code, but there is no relation to RL agents)\n\nEnvironment (we input a single value that is fixed throughout the optimization):\n\nlc : cutoff length\n\nNt : number of targets\n\nL : world size\n\nr : target detection radius\n\ndestructive : whether targets are destructive or not (always set to False)\n\nOnce we define the parameter ranges, we choose the optimization algorithm. Among the different possibilities that Tune offers, we chose Grid Search for the Lévy distribution and Bayesian Optimization for the bi-exponential distributions.\n\nExample\nLet us take the example with \\(l_\\textrm{c}=3\\).\nFor the Lévy distribution, the config dictionary looks like:\n\nconfig = {'d_int': None,\n          'd_ext': None,\n          'p': None,\n          'beta': tune.grid_search(np.linspace(0.01,1.,20)), \n          'model': 'powerlaw',\n          'time_ep': 20000,\n          'n': 10000,\n          'lc': 3.0,\n          'Nt': 100,\n          'L': 100,\n          'r': 0.5,\n          'destructive': False\n          }\n\nWe do a grid search over 20 parameters, linearly spaced in the interval \\([0.01, 1]\\). Parameters that correspond to the other model are set to ‘None’.\nThen, we initialize the tuner, which by default does a grid search over the input parameters.\n\nfrom ray import tune\n\ntuner = tune.Tuner(average_search_efficiency,\n                   tune_config=tune.TuneConfig(num_samples=1),\n                   param_space=config)\n\nAnd we run the algorithm:\n\nresult_grid = tuner.fit()\n\nFor the bi-exponential distribution, the config dictionary looks like:\n\nconfig = {'d_int': tune.uniform(0.00001, 20.0),\n          'd_ext': 100.0,\n          'p': tune.uniform(0.0, 1.0),\n          'beta': None,\n          'model': 'double_exp',\n          'time_ep': 20000,\n          'n': 10000,\n          'lc': 3.0,\n          'Nt': 100,\n          'L': 100,\n          'r': 0.5,\n          'destructive': False\n          }\n\nIn this case, since we choose a Bayesian optimization method, we do not specify the parameters to try, but just the ranges. For the small scale, we consider a range that is of the order of the scale of \\(l_\\textrm{c}\\). We fix the value for \\(d_2\\) to further guide the search and make it more time efficient. We do the search with \\(d_2=100\\), which is the scale of the average distance between targets, and with \\(d_2=10^5\\). Again, the parameter \\(\\beta\\) that corresponds to the other model is set to ‘None’.\nWe first initialize the Bayesian optimization method, and then the tuner.\n\nfrom ray import tune\nfrom ray.tune.search.bayesopt import BayesOptSearch\nfrom ray.tune.search import ConcurrencyLimiter\n\nbayesopt = BayesOptSearch(metric=\"mean_eff\", mode=\"max\")\nbayesopt = ConcurrencyLimiter(bayesopt, max_concurrent=3)\ntuner = tune.Tuner(average_search_efficiency, \n                   tune_config=tune.TuneConfig(search_alg=bayesopt, num_samples=100), \n                   param_space=config)\n\nNote that we limit the number of concurrent processes to 3, so that the method can update itself more times within the 100 samples.\nAnd we run it:\n\nresult_grid = tuner.fit()\n\n\n\nResults\nThe results are saved as a panda dataframe in the folder ‘results/benchmark_models/’. We further identify each run of the algorithm with the parameter run, in case there is more than one run per model (e.g. for the bi-exponential, we run it twice, for each value of \\(d_2\\)).\n\nrun = '0'\nresults_path = 'results/benchmark_models/' + config['model'] + '/'+ str(config['lc']) + '/run_'+run+'/'\n\nresults_df = result_grid.get_dataframe()\nresults_df.to_csv(results_path+'df'+run+'_'+config['model']+'_lc_'+str(config['lc'])+'.csv')\n\nTo retrieve these results, run:\n\nimport pandas as pd\n\nresults_df = pd.read_csv(results_path+'df'+run+'_'+config['model']+'_lc_'+str(config['lc'])+'.csv')\n\nIn addition, the model parameters that achieved the best performance, together with the corresponding mean search efficiency, and its standard error of the mean (which is the only value that is not contained in the dataframe results_df), can be obtained by running the method get_opt:\n\nfrom rl_opts.utils import get_opt\n\nmain_path = 'results/benchmark_models/'\nlc = 3.0 #note that this notation has to match with how you have input it in the config dictionary\nmodel = 'powerlaw'\nrun = '0'\n\nmean_eff, sem, parameters = get_opt(main_path, lc, model, run)\n\nThe config dictionaries can also be saved and retrieved:\n\nconfig_path = 'configurations/benchmark_models/'\n\nnp.save(config_path+'config_'+config['model']+'_lc_'+str(config['lc'])+'_run_'+run+'.npy', config)\nnp.load(config_path+'config_'+config['model']+'_lc_'+str(config['lc'])+'_run_'+run+'.npy', allow_pickle=True).item()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RL-OptS",
    "section": "",
    "text": "Reinforcement Learning of Optimal Search strategies\n\n\n  \n\nThis library builds the necessary tools needed to study, replicate and develop the results of the paper: “Optimal foraging strategies can be learned and outperform Lévy walks” by G. Muñoz-Gil, A. López-Incera, L. J. Fiderer and H. J. Briegel.\n\nInstallation\nYou can access all these tools installing the python package rl_opts via Pypi:\npip install rl-opts\nYou can also opt for cloning the source repository and executing the following on the parent folder you just cloned the repo:\npip install -e rl_opts\nThis will install both the library and the necessary packages.\n\n\nTutorials\nIn the left sidebar you will a Tutorials tab, with notebooks that will help you navigate the package as well as reproducing the results of our paper via minimal examples. In particular, we have three tutorials:\n\nReinforcement learning  : shows how to train a RL agent based on Projective Simulation agents to search targets in randomly distributed environments as the ones considered in our paper.\nImitation learning  : shows how to train a RL agent to imitate the policy of an expert equipped with a pre-trained policy. The latter is based on the benchmark strategies common in the literature.\nBenchmarks  : shows how launch various benchmark strategies with which to compare the trained RL agents.\n\n\n\nPackage structure\nThe package contains a set of modules for:\n\nReinforcement learning framework (rl_opts.rl_framework) : building foraging environments as well as the RL agents moving on them.\nLearning and benchmarking (rl_opts.learn_and_bench) : training RL agents as well as benchmarking them w.r.t. to known foraging strategies.\nImitation learning (rl_opts.imitation): training RL agents in imitation schemes via foraging experts.\nAnalytical functions (rl_opts.analytics): builiding analytical functions for step length distributions as well as tranforming these to foraging policies.\nUtils (rl_opts.utils): helpers used throughout the package.\n\n\n\nCite\nWe kindly ask you to cite our paper if any of the previous material was useful for your work, here is the bibtex info:\nsoon"
  },
  {
    "objectID": "lib_nbs/analytics.html",
    "href": "lib_nbs/analytics.html",
    "title": "Analytical functions",
    "section": "",
    "text": "pdf_multimode (L:int, lambdas:list, probs:list)\n\nComputes the discrete PDF of multi-mode exponential of the form\n\\[\n\\Pr(L) = \\sum_{i=1,2} \\omega_i (1-e^{-1/\\lambda_i}) e^{-(L-1)/\\lambda_i} \\, ,\n\\] where \\(\\omega\\) is the probability of each mode and \\(\\lambda\\) it’s scale.\n\n\n\n\nType\nDetails\n\n\n\n\nL\nint\nEither int or array for which pdf is calculated\n\n\nlambdas\nlist\nScales of each modes\n\n\nprobs\nlist\nProbability weight of each mode\n\n\nReturns\narray\nArray with probability of each L\n\n\n\n\nlambdas = np.array([2,15])\nprobs = np.array([0.99, 0.01])\nL_max = 100\nplt.loglog(np.arange(1, L_max),\n           pdf_multimode(L = np.arange(1, L_max), lambdas=lambdas, probs=probs)\n          )\nplt.xlabel('L'); plt.ylabel('P(L)')\n\nText(0, 0.5, 'P(L)')\n\n\n\n\n\n\n\n\n\n\n pdf_powerlaw (L:float, beta:float=1)\n\nComputes the discrete PDF of a powerlaw of the form \\[\n\\Pr(L)\\sim L^{-1-\\mathrm{beta}}.\n\\]\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nL\nfloat\n\nEither int or array for which pdf is calculated\n\n\nbeta\nfloat\n1\nExponent of the power law\n\n\nReturns\narray\n\nArray with probability of each L\n\n\n\n\nplt.loglog(np.arange(1, 1000),\n           pdf_powerlaw(L = np.arange(1,1000), beta = 1)\n          )\nplt.xlabel('L'); plt.ylabel('P(L)')\n\nText(0, 0.5, 'P(L)')\n\n\n\n\n\n\n\n\n\n\n pdf_discrete_sample (pdf_func:object, num_samples:int, **args_func)\n\nSamples discrete values from a given PDF\n\n\n\n\nType\nDetails\n\n\n\n\npdf_func\nobject\nFunction generating the pdf\n\n\nnum_samples\nint\nNumber of samples to create\n\n\nargs_func\n\n\n\n\nReturns\narray\nSamples\n\n\n\n\nsamples = pdf_discrete_sample(pdf_func = pdf_multimode, \n                              num_samples=10000, \n                              L_max = L_max, lambdas=lambdas, probs=probs)\n\ncounts = np.bincount(samples)[1:]\n\nplt.loglog(np.arange(1, len(counts)+1),\n           counts/counts.sum(), 'o', \n           label = 'Histogram samples')\n\nplt.loglog(np.arange(1, L_max),\n           pdf_multimode(L_max = L_max, lambdas=lambdas, probs=probs),\n           label = 'Theory'\n          )\nplt.xlabel('L'); plt.ylabel('P(L)'); plt.legend()\n\n<matplotlib.legend.Legend>"
  },
  {
    "objectID": "lib_nbs/analytics.html#calculation-of-policy-from-step-length-pdf",
    "href": "lib_nbs/analytics.html#calculation-of-policy-from-step-length-pdf",
    "title": "Analytical functions",
    "section": "Calculation of policy from step length PDF",
    "text": "Calculation of policy from step length PDF\n\n\nget_policy_from_dist\n\n get_policy_from_dist (n_max, func, renorm=True, **args_func)\n\nGiven a PDF of step lengths, calculates the corresponding policy\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_max\n\n\nMaximum counter n_max for which the policy is calculated\n\n\nfunc\n\n\nFunction generating the pdf\n\n\nrenorm\nbool\nTrue\nIf true, we check whether the distribution has a boundary N, for which _n=N^Pr(L=nd) = 0\n\n\nargs_func\n\n\n\n\n\nReturns\narray\n\nPolicy at each counter value\n\n\n\n\n\n\nget_policy\n\n get_policy (n_max, func, ps_0:int=1, **args_func)\n\nGiven a PDF of step lengths, calculates the corresponding policy\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_max\n\n\nMaximum counter n_max for which the policy is calculated\n\n\nfunc\n\n\nFunction generating the pdf\n\n\nps_0\nint\n1\nValue of the policy at L = 0 (should be one)\n\n\nargs_func\n\n\n\n\n\nReturns\narray\n\nPolicy at each counter value\n\n\n\n\nL = 100\nbetas = np.linspace(0.1, 2, 10)\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\n\nfig, ax = plt.subplots()\nfor beta, color in zip(betas, colors):\n    \n    policy = get_policy(n_max = L,\n                        func = pdf_powerlaw,\n                        beta = beta)\n    ax.plot(np.arange(2, L+1), policy[1:], c = color)\n\n\n# Plot features    \nplt.setp(ax, xlabel =r'$l$', ylabel = r'$P_s(l)$', xscale = 'log')\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm= mcolors.Normalize(vmin=betas.min(), \n                                                                  vmax=betas.max()),\n                                          cmap=plt.cm.plasma),\n                    ax = ax)\ncbar.set_label(r'$\\beta$')"
  },
  {
    "objectID": "lib_nbs/imitation_learning.html#example",
    "href": "lib_nbs/imitation_learning.html#example",
    "title": "Imitation learning",
    "section": "Example",
    "text": "Example\nWe showcase how to imitate the policy based on a given step length distribution, an in particular of a Lévy distribution. For further examples, see the Tutorials section.\n\nfrom rl_opts.analytics import pdf_powerlaw, pdf_discrete_sample, get_policy_from_dist\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n\nNUM_STATES = 100 # size of the state space\nEPOCHS = 100 # number of epochs\nNUM_STEPS = 1000 # number of learning steps per episode\n\nsteps = pdf_discrete_sample(pdf_func = pdf_powerlaw,\n                            beta = 1,\n                            L_max = NUM_STATES,  \n                            num_samples = (EPOCHS, NUM_STEPS))\n\nimitator = PS_imitation(num_states = NUM_STATES,\n                        eta = int(1e-7),\n                        gamma = 0)\n    \n                               \nfor e in tqdm(range(EPOCHS)):\n    imitator.reset()\n    for s in steps[e]:    \n        imitator.update(length = s)\n\n100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 53.18it/s]\n\n\n\npolicy_theory = get_policy_from_dist(n_max = NUM_STATES,\n                                     func = pdf_powerlaw,\n                                     beta = 1)\npolicy_imitat = imitator.h_matrix[0,:]/imitator.h_matrix.sum(0)\n\n\n_ , ax = plt.subplots(figsize = (5,3))\nax.plot(policy_imitat ,'o')\nax.plot(policy_theory[1:])\nplt.setp(ax, \n         xscale = 'log', xlim = (0.9, NUM_STATES/2), xlabel = r'Counter $n$',\n         ylim = (0.5, 1.1), ylabel = 'Policy');"
  },
  {
    "objectID": "lib_nbs/mfpt.html",
    "href": "lib_nbs/mfpt.html",
    "title": "Mean First Passage Times in 1D and 2D environments",
    "section": "",
    "text": "constant_velocity_generator (N, T, time_sampler, velocity=1,\n                              **sample_args)\n\nGiven a sampler for length of time steps, generates a trajectory considering a constant velocity in the sampled times. After each time step, we sample a new direction.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\n\n\nNumber of trajectories\n\n\nT\n\n\nLength of trajectories\n\n\ntime_sampler\n\n\nSampler of time of constant velocity\n\n\nvelocity\nint\n1\nVelocity\n\n\nsample_args\n\n\n\n\n\n\n\nN = 5; T = 150\n\ntrajs_lw = constant_velocity_generator(N,\n                                       T, \n                                       time_sampler = pdf_discrete_sample, \n                                       pdf_func=pdf_powerlaw,\n                                       L_max = T,                                       \n                                       beta = 1)\n\ndef single_steps(num_samples):\n    return np.ones(num_samples)\n\ntrajs_rw = constant_velocity_generator(N, T, time_sampler = single_steps, velocity = 1.2)\n\n\nfig, ax = plt.subplots(1, 2, figsize = (5,3))\nax[0].plot(trajs_lw.transpose())\nax[0].set_title('LW')\nax[1].plot(trajs_rw.transpose())\nax[1].set_title('RW');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n mfpt_rw (N:int, T:int, x0:float, Ls:list, traj_generator:Callable,\n          max_loop=5, save=None, **args_generator)\n\nCalculates MFPT to boundaries at 0 and L starting in x0\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nNumber of trajectories\n\n\nT\nint\n\nLengths of trajectories\n\n\nx0\nfloat\n\nStarting point of walk\n\n\nLs\nlist\n\nBoundary position\n\n\ntraj_generator\nCallable\n\nFunction generating the walk (must start in x0 = zero)\n\n\nmax_loop\nint\n5\nMax number of while loop if some walks do not reach the boundary\n\n\nsave\nNoneType\nNone\n\n\n\nargs_generator\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n mfpt_informed_rw (N:int, T:int, x0:float, Ls:list,\n                   traj_generator:Callable, max_loop=5, time_sampler=None,\n                   save=None, **args_generator)\n\nCalculates MFPT to boundaries at 0 and L starting in x0 with a traj generator informed on the scales of the sysmte (x0 and L)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nNumber of trajectories\n\n\nT\nint\n\nLengths of trajectories\n\n\nx0\nfloat\n\nStarting point of walk\n\n\nLs\nlist\n\nBoundary position\n\n\ntraj_generator\nCallable\n\nFunction generating the walk (must start in x0 = zero)\n\n\nmax_loop\nint\n5\nMax number of while loop if some walks do not reach the boundary\n\n\ntime_sampler\nNoneType\nNone\n\n\n\nsave\nNoneType\nNone\n\n\n\nargs_generator\n\n\n\n\n\n\n\n\n\n\n\nIn this case we are already doing constant velocities, hence we can use the generator below:\n\n\n\n\n\n\n rw_generator (N, T)\n\n\nN = int(1e2)\nT = int(1e6)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                        traj_generator = rw_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\n\nmean = results.mean(0)\n\nplt.plot(Ls, mean[0]*(Ls/Ls[0])**2,c = 'k', label = r'$\\sim L^2$')\nplt.plot(Ls, mean[0]*(Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\nplt.loglog(Ls, mean,'o', alpha = 0.3)\nplt.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\n\n\n\n\n\n exp_time_generator (num_samples)\n\n\nN = int(1e2)\nT = int(1e4)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls,\n                                                        traj_generator = constant_velocity_generator,\n                                                        time_sampler = exp_time_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\n\nmean_exp = results.mean(0)\n\nplt.plot(Ls, mean_exp[0]*(Ls/Ls[0])**2,c = 'k', label = r'$\\sim L^2$')\nplt.plot(Ls, mean_exp[0]*(Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\nplt.loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.xlabel('MFPT'); plt.ylabel('L'); plt.title(fr'$x_0 =$ {x0}') \nplt.ylim(ymax = 500)\n\n(23.655963721815578, 500)\n\n\n\n\n\n\n\n\n\n\n\n\n\n Biexp (informed=False, **args)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n biexp_time_generator (num_samples, **sample_kwargs)\n\n\nbi = Biexp(informed = False, w1 = 0.1, d1 = 1, d2 = 200)\nbi_rng = bi.sample(int(1e6))\n\nh, e = np.histogram(bi_rng, bins = np.linspace(0.1,100, 2000), density=True)\nplt.loglog(e[:-1], h)\nplt.plot(e, bi.pdf(e)*(h[0]/bi.pdf(e[0])), alpha = 0.8)\n\n\n\n\n\n\n\nN = int(1e2)\nT = int(1e4)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls,\n                                                        traj_generator = constant_velocity_generator,\n                                                        time_sampler = biexp_time_generator, w1 = 0.5, d1 = 1, d2 = 5)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_uninf_biexp = results.mean(0)\n\n\nplt.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nplt.plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n\nplt.loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ 3')\n\n\n\n\n\n\n\n\n\nN = int(1e2)\nT = int(1e4)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=10)(delayed(mfpt_informed_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                                 traj_generator = constant_velocity_generator,\n                                                                 time_sampler = biexp_time_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_biexp = results.mean(0)\n\n\nplt.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nplt.plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n\n\nplt.loglog(Ls, mean_biexp/mean_biexp[0],'o', alpha = 0.3, label = 'Informed bi-exponential')\nplt.loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ 3')\n\n\n\n\n\n\n\n\n\n\\(\\beta=1\\), multiple repetitions\n\nN = int(1e2)\nT = int(1e5)\nx0 = 3\nLs = np.arange(15, 100)\n\nbeta = 1\n\nresults = np.array(Parallel(n_jobs=10)(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                        traj_generator = constant_velocity_generator,\n                                                        time_sampler = pdf_discrete_sample, \n                                                        pdf_func=pdf_powerlaw,\n                                                        L_max = T,\n                                                        beta = beta)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_pw = results.mean(0)\n\nMultiple \\(\\beta\\)\n\nN = int(1e3)\nT = int(1e5)\nx0 = 3\nLs = np.arange(15, 100)\n\nbetas = np.linspace(0.1, 1.5, 10)\n\n\nresults = np.array(Parallel(n_jobs=len(betas))(delayed(mfpt_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                                traj_generator = constant_velocity_generator,\n                                                                time_sampler = pdf_discrete_sample, \n                                                                pdf_func=pdf_powerlaw,\n                                                                L_max = T,\n                                                                beta = beta)\n                for beta in tqdm(betas)), dtype = object)\n\n\nmean_pw_betas = results.mean(0)\n\n\nfig, ax = plt.subplots()\nax.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\npw_scaling = np.log(Ls)*(Ls**(1/2))\nax.plot(Ls, pw_scaling/pw_scaling[0], c = 'k',ls = '--', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\n\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor res, c in zip(results, colors):\n    ax.loglog(Ls, res/res[0], 'o', c = c, alpha = 0.3)\n\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm= mcolors.Normalize(vmin=betas.min(), \n                                                                  vmax=betas.max()),\n                                          cmap=plt.cm.plasma),\n                    ax = ax)\ncbar.set_label(r'$\\beta$')\n\nax.legend()\nplt.setp(ax, ylabel = 'MFPT', xlabel = 'L', title = fr'$x_0 =$ {x0}')\n\n[Text(0, 0.5, 'MFPT'), Text(0.5, 0, 'L'), Text(0.5, 1.0, '$x_0 =$ 3')]\n\n\n\n\n\n\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor res, c, beta in zip(results, colors, betas):\n    if round(beta, 1) == 1: c = 'k'\n    plt.loglog(Ls, res, 'o', c = c, alpha = 0.3)\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ 3')\n\n\n\n\n\nSmaller \\(\\beta\\)\n\nfig, ax = plt.subplots()\nax.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\npw_scaling = np.log(Ls)*(Ls**(1/2))\nax.plot(Ls, pw_scaling/pw_scaling[0], c = 'k',ls = '--', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\n\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor res, c in zip(results, colors):\n    ax.loglog(Ls, res/res[0], 'o', c = c, alpha = 0.3)\n\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm= mcolors.Normalize(vmin=betas.min(), \n                                                                  vmax=betas.max()),\n                                          cmap=plt.cm.plasma),\n                    ax = ax)\ncbar.set_label(r'$\\beta$')\nplt.setp(ax, ylabel = 'MFPT', xlabel = 'L', title = fr'$x_0 =$ {x0}')\n\n[Text(0, 0.5, 'MFPT'), Text(0.5, 0, 'L'), Text(0.5, 1.0, '$x_0 =$ 0.02')]\n\n\n\n\n\n\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor res, c in zip(results, colors):\n    plt.loglog(Ls, res, 'o', c = c, alpha = 0.3)\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ 0.02')\n\n\n\n\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n  0%|          | 0/100 [00:00<?, ?it/s]\n\n\n\n\n\nSaving the data for future tests\n\na,b,c = [0]*3\n\n\nfor k in [a,b,c]:\n    print(k)\n    k = 12\n\n0\n0\n0\n\n\n\nb\n\n0\n\n\n\nnames = ['discrete', 'exp', 'inf_biexp', 'uninf_biexp', 'pw']\ntry: \n    datas = [mean, mean_exp, mean_biexp, mean_uninf_biexp, mean_pw]\n    for data, name in zip(datas, names):\n        np.save('scalings_mfpt/'+name+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\nelse: \n    mean = np.load('scalings_mfpt/'+names[0]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n    mean_exp = np.load('scalings_mfpt/'+names[1]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n    mean_biexp = np.load('scalings_mfpt/'+names[2]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n    mean_uninf_biexp = np.load('scalings_mfpt/'+names[3]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n    mean_pw = np.load('scalings_mfpt/'+names[4]+f'_N_{N}_T_{T}_x0_{x0}_L_{min(Ls)}-{max(Ls)}.npy', data)\n\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\n\nax[0].plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nax[0].plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\npw_scaling = np.log(Ls)*Ls**(1/2)\nax[0].plot(Ls, pw_scaling/pw_scaling[0], c = 'k',ls = ':', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\nax[0].loglog(Ls, mean_pw/mean_pw[0],' o', alpha = 0.3, label = r'Power-law $\\alpha = 1$')\nax[0].loglog(Ls, mean_biexp/mean_biexp[0],'o', alpha = 0.3, label = 'Informed bi-exponential')\nax[0].loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nax[0].loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nax[0].loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nax[0].legend()\n\n\nax[1].loglog(Ls, mean_pw,' o', alpha = 0.3, label = r'Power-law $\\alpha = 1$')\nax[1].loglog(Ls, mean_biexp,'o', alpha = 0.3, label = 'Informed bi-exponential')\nax[1].loglog(Ls, mean_uninf_biexp,'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nax[1].loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nax[1].loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\n\n\n\nplt.setp(ax, ylabel = 'MFPT', xlabel = 'L', title = fr'$x_0 =$ {x0}')\n\n[Text(0, 0.5, 'MFPT'),\n Text(0.5, 0, 'L'),\n Text(0.5, 1.0, '$x_0 =$ 3'),\n Text(0, 0.5, 'MFPT'),\n Text(0.5, 0, 'L'),\n Text(0.5, 1.0, '$x_0 =$ 3')]\n\n\n\n\n\nReading results\n\nmean_fpt = []\nnames_read = names#[:3]+[names[-1]]*len(betas)\nidxbeta = 6\nfor idx, name in enumerate(names_read):\n    try: del collect_mean\n    except: pass    \n\n    if name == 'biexp_inf': reps = 5500\n    else: reps = 500\n    \n    for rep in range(reps):\n        if name == 'pw':\n            current_m = np.load(path+name+f'_beta{round(betas[idxbeta],3)}_N_{np.log10(Ns[-1]).astype(int)}_L_{np.log10(Ts[-1]).astype(int)}_{rep}.npy')\n                \n        else:\n            current_m = np.load(path+name+f'_N_{np.log10(Ns[idx]).astype(int)}_L_{np.log10(Ts[idx]).astype(int)}_{rep}.npy')\n            \n        # current_s = np.load(path+name+f'_N_{np.log10(Ns[idx]).astype(int)}_L_{np.log10(Ts[idx]).astype(int)}_{rep}_stats.npy')\n        try:\n            collect_mean = np.vstack((collect_mean, current_m))\n            # collect_stat = np.vstack((collect_stat, current_s))\n        except:\n            collect_mean = current_m\n            # collect_stat = current_s\n    mean_fpt.append(collect_mean)\n\n\nfig, ax = plt.subplots()\nminL = 60\nLsplot = Ls[minL:]\nfor m, n  in zip(mean_fpt, names_read):\n    if n == 'pw': n = r'pw $\\beta \\approx 1$'\n    mean = m.mean(0)#[minL:]\n    ax.plot(Ls, mean/mean[0], label = n)\n\n\nax.plot(Lsplot, 2.7*(Lsplot/Lsplot[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nax.plot(Lsplot, 5*(Lsplot/Lsplot[0]), c = 'k',ls = '--', label = r'$\\sim L$')\npw_scaling = np.log(Lsplot)*Lsplot**(1/2)\nax.plot(Lsplot, 3.5*pw_scaling/pw_scaling[0], c = 'k',ls = ':', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\nplt.setp(ax, xscale = 'log', yscale = 'log', xlabel = 'L', ylabel = 'MFPT', title = fr'$x_0 = {x0}$')\nax.legend()\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\nmean_fpt_pw = []\nname = 'pw'#[:3]+[names[-1]]*len(betas)\n\nfor idxbeta, beta in enumerate(betas):\n    try: del collect_mean\n    except: pass    \n\n        \n    for rep in range(reps):\n        current_m = np.load(path+name+f'_beta{round(betas[idxbeta],3)}_N_{np.log10(Ns[-1]).astype(int)}_L_{np.log10(Ts[-1]).astype(int)}_{rep}.npy')\n       \n        try:\n            collect_mean = np.vstack((collect_mean, current_m))\n            # collect_stat = np.vstack((collect_stat, current_s))\n        except:\n            collect_mean = current_m\n            # collect_stat = current_s\n    mean_fpt_pw.append(collect_mean)\n\n\nfig, ax = plt.subplots()\nminL = 60\nLsplot = Ls[minL:]\ncolors = plt.cm.plasma(np.linspace(0,1,len(betas)+2))\nfor m, c  in zip(mean_fpt_pw, colors):\n    mean = m.mean(0)[minL:]\n    ax.plot(Lsplot, mean/mean[0], c = c)\n\n\nax.plot(Lsplot, (Lsplot/Lsplot[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nax.plot(Lsplot, (Lsplot/Lsplot[0]), c = 'k',ls = '--', label = r'$\\sim L$')\npw_scaling = np.log(Lsplot)*Lsplot**(1/2)\nax.plot(Lsplot, pw_scaling/pw_scaling[0], c = 'k',ls = ':', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\nplt.setp(ax, xscale = 'log', yscale = 'log', xlabel = 'L', ylabel = 'MFPT', title = fr'$x_0 = {x0}$')\nax.legend()\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm= mcolors.Normalize(vmin=betas.min(), \n                                                                  vmax=betas.max()),\n                                          cmap=plt.cm.plasma),\n                    ax = ax)\ncbar.set_label(r'$\\beta$')\n\n\n\n\n\nfit = []\nfor m, c  in zip(mean_fpt_pw, colors):\n    mean = m.mean(0)[minL:]\n    fit.append(np.polyfit(np.log(Lsplot), np.log(mean/mean[0]), deg=1)[0])\n\n\n# Approx fit log(L)\napp_log =  np.polyfit(np.log(Lsplot), np.log(pw_scaling/pw_scaling[0]), deg=1)[0]\n\n# Fit informed\nmftp_inf = mean_fpt[-2].copy()\nmean_inf = mftp_inf.mean(0)[minL:]\nfit_informed = np.polyfit(np.log(Lsplot), np.log(mean_inf/mean_inf[0]), deg=1)[0]\n\n# Plot\nfig, ax = plt.subplots()\n# plot powerlaw\nax.plot(betas, fit, c = 'k')\nax.scatter(betas, fit, c = colors[:len(betas)], zorder = 10)\nfor line, name, ls in zip([1, app_log, 1/2], [r'$L$', r'$\\sim \\sqrt{L}\\ln{L}$', r'$\\sqrt{L}$'], ['-', '--',':']):\n    ax.axhline(line, label = name, ls = ls, alpha = 0.5, c = 'k')\n# plot informed    \nax.axhline(fit_informed, label = 'Fit Exp informed', lw = 2)\n    \nax.legend()\nplt.setp(ax, xlabel = r'$\\beta$', ylabel = 'Fit long time MFPT')\n\n[Text(0.5, 0, '$\\\\beta$'), Text(0, 0.5, 'Fit long time MFPT')]\n\n\n\n\n\n\nminL = 60\nmean_inf = mftp_inf.mean(0)[minL:]\nLsplot = Ls[minL:]\nplt.loglog(Lsplot, mean_inf/mean_inf[0])\nplt.loglog(Lsplot, (Lsplot/Lsplot[0])**(0.6))"
  },
  {
    "objectID": "lib_nbs/mfpt.html#d-not-conclusive-results",
    "href": "lib_nbs/mfpt.html#d-not-conclusive-results",
    "title": "Mean First Passage Times in 1D and 2D environments",
    "section": "2D (not conclusive results)",
    "text": "2D (not conclusive results)\n\n\nconstant_velocity_generator_2D\n\n constant_velocity_generator_2D (N, T, time_sampler, velocity=1,\n                                 **sample_args)\n\nGiven a sampler for length of time steps, generates a 2D trajectory considering a constant velocity in the sampled times. After each time step, we sample a new direction.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\n\n\nNumber of trajectories\n\n\nT\n\n\nLength of trajectories\n\n\ntime_sampler\n\n\nSampler of time of constant velocity\n\n\nvelocity\nint\n1\nVelocity\n\n\nsample_args\n\n\n\n\n\n\n\nN = 5; T = 150\ntrajs_lw_x, trajs_lw_y = constant_velocity_generator_2D(N, T, time_sampler = lw_step, beta = 1)\n\ndef single_steps(num_samples):\n    return np.ones(num_samples)\n\ntrajs_rw_x, trajs_rw_y = constant_velocity_generator_2D(N, T, time_sampler = single_steps, velocity = 1.2)\n\nFinished generating trajectories\nFinished generating trajectories\n\n\n\nfig, ax = plt.subplots(1, 2)\nfor x, y in zip(trajs_lw_x, trajs_lw_y):\n    ax[0].plot(x, y)\nax[0].set_title('LW')\nfor x, y in zip(trajs_rw_x, trajs_rw_y):\n    ax[1].plot(x, y)\nax[1].set_title('RW');\n\n\n\n\n\n\nMFPT calculator\n\n\n\nmfpt_rw_2D\n\n mfpt_rw_2D (N:int, T:int, x0:list, Ls:list, traj_generator:Callable,\n             max_loop=5, **args_generator)\n\nCalculates MFPT to boundaries at 0 and L starting in x0 in 2D\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nNumber of trajectories\n\n\nT\nint\n\nLengths of trajectories\n\n\nx0\nlist\n\nStarting point of walk (in 2d)\n\n\nLs\nlist\n\nBoundary position\n\n\ntraj_generator\nCallable\n\nFunction generating the walk (must start in x0 = zero)\n\n\nmax_loop\nint\n5\nMax number of while loop if some walks do not reach the boundary\n\n\nargs_generator\n\n\n\n\n\n\n\n\nResults\n\nDiscrete walking\nIn this case we are already doing constant velocities, hence we can use the generator below:\n\nN = int(1e4)\nT = int(1e2)\nx0 = [1, 1]\nLs = np.linspace(5, 150, 20)\n\nreps = 10\nresults = np.array(Parallel(n_jobs=10)(delayed(mfpt_rw_2D)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                           traj_generator = constant_velocity_generator_2D,\n                                                           time_sampler = single_steps)\n                for _ in tqdm(range(reps))), dtype = object)\n\n\nmean = results.mean(0)\n\n\nplt.plot(Ls, mean[0]*(Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim L^{1/2}$')\nplt.plot(Ls, mean[0]*(Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n# plt.loglog(Ls, mean,'o', alpha = 0.8)\nplt.errorbar(Ls, mean, yerr = results.astype(float).std(0))\n\nplt.legend()\n\n# plt.xlim(xmax = 150)\n\n<matplotlib.legend.Legend>\n\n\n\n\n\n\n\n\nExponential\n\nN = int(1e2)\nT = int(1e4)\nx0 = [0.5, 0.5]\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw_2D)(N = N, T = T, x0 = x0, Ls = Ls,\n                                                           traj_generator = constant_velocity_generator_2D,\n                                                           time_sampler = exp_time_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\n\nmean_exp = results.mean(0)\n\nplt.plot(Ls, mean_exp[0]*(Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim L^2$')\nplt.plot(Ls, mean_exp[0]*(Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\nplt.loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.xlabel('MFPT'); plt.ylabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ [0.5, 0.5]')\n\n\n\n\n\n\n\nBiexponential\n\nUninformed\n\nN = int(1e3)\nT = int(1e5)\nx0 = [0.5, 0.5]\nLs = np.arange(5, 30)\n\nreps = 20*4\nresults = np.array(Parallel(n_jobs=5)(delayed(mfpt_rw_2D)(N = N, T = T, x0 = x0, Ls = Ls,\n                                                        traj_generator = constant_velocity_generator_2D,\n                                                        time_sampler = biexp_time_generator, w1 = 0.5, d1 = 1, d2 = 5)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_uninf_biexp = results.mean(0)\n\n\nplt.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\n# plt.plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n\nplt.loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ [0.5, 0.5]')\n\n\n\n\n\n\nplt.loglog(Ls, mean_uninf_biexp,'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\nText(0.5, 1.0, '$x_0 =$ [0.5, 0.5]')\n\n\n\n\n\n\n\nInformed\n\n\n\n\nmfpt_informed_rw_2D\n\n mfpt_informed_rw_2D (N:int, T:int, x0:list, Ls:list,\n                      traj_generator:Callable, max_loop=5,\n                      **args_generator)\n\nCalculates MFPT to boundaries at 0 and L starting in x0 in 2D\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nN\nint\n\nNumber of trajectories\n\n\nT\nint\n\nLengths of trajectories\n\n\nx0\nlist\n\nStarting point of walk (in 2d)\n\n\nLs\nlist\n\nBoundary position\n\n\ntraj_generator\nCallable\n\nFunction generating the walk (must start in x0 = zero)\n\n\nmax_loop\nint\n5\nMax number of while loop if some walks do not reach the boundary\n\n\nargs_generator\n\n\n\n\n\n\n\nN = int(1e2)\nT = int(1e4)\nx0 = 3\nLs = np.arange(15, 100)\n\nreps = 20\nresults = np.array(Parallel(n_jobs=10)(delayed(mfpt_informed_rw)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                                 traj_generator = constant_velocity_generator,\n                                                                 time_sampler = biexp_time_generator)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_biexp = results.mean(0)\n\n\nplt.plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nplt.plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\n\n\nplt.loglog(Ls, mean_biexp/mean_biexp[0],'o', alpha = 0.3, label = 'Informed bi-exponential')\nplt.loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nplt.loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nplt.loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nplt.legend()\nplt.ylabel('MFPT'); plt.xlabel('L'); plt.title(fr'$x_0 =$ {x0}')\n\n\n\nPower-law\n\nN = int(1e3)\nT = int(1e4)\nx0 = [0.5, 0.5]\nLs = np.arange(15, 100)\n\nreps = 20\nbeta = 1\n\nresults = np.array(Parallel(n_jobs=20)(delayed(mfpt_rw_2D)(N = N, T = T, x0 = x0, Ls = Ls, \n                                                        traj_generator = constant_velocity_generator_2D,\n                                                        time_sampler = pdf_discrete_sample, \n                                                           pdf_func=pdf_powerlaw,\n                                                           L_max = T, beta = beta,)\n                for _ in tqdm(range(reps))), dtype = object)\n\nmean_pw = results.mean(0)\n\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\n\nax[0].plot(Ls, (Ls/Ls[0])**(1/2),c = 'k', label = r'$\\sim \\sqrt{L}$')\nax[0].plot(Ls, (Ls/Ls[0]), c = 'k',ls = '--', label = r'$\\sim L$')\npw_scaling = np.log(Ls)*Ls**(1/2)\nax[0].plot(Ls, pw_scaling/pw_scaling[0], c = 'k',ls = ':', label = r'$\\sim \\sqrt{L}\\ln{L}$')\n\nax[0].loglog(Ls, mean_pw/mean_pw[0],' o', alpha = 0.3, label = r'Power-law $\\alpha = 1$')\n# ax[0].loglog(Ls, mean_biexp/mean_biexp[0],'o', alpha = 0.3, label = 'Informed bi-exponential')\nax[0].loglog(Ls, mean_uninf_biexp/mean_uninf_biexp[0],'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nax[0].loglog(Ls, mean_exp/mean_exp[0],'o', alpha = 0.3, label = 'Exponential')\nax[0].loglog(Ls, mean/mean[0],'o', alpha = 0.3, label = 'Discrete')\nax[0].legend()\n\n\nax[1].loglog(Ls, mean_pw,' o', alpha = 0.3, label = r'Power-law $\\alpha = 1$')\n# ax[1].loglog(Ls, mean_biexp,'o', alpha = 0.3, label = 'Informed bi-exponential')\nax[1].loglog(Ls, mean_uninf_biexp,'o', alpha = 0.3, label = 'Uninformed bi-exponential')\nax[1].loglog(Ls, mean_exp,'o', alpha = 0.3, label = 'Exponential')\nax[1].loglog(Ls, mean,'o', alpha = 0.3, label = 'Discrete')\n\n\n\nplt.setp(ax, ylabel = 'MFPT', xlabel = 'L', title = fr'$x_0 =$ {x0}')\n\n[Text(0, 0.5, 'MFPT'),\n Text(0.5, 0, 'L'),\n Text(0.5, 1.0, '$x_0 =$ [0.5, 0.5]'),\n Text(0, 0.5, 'MFPT'),\n Text(0.5, 0, 'L'),\n Text(0.5, 1.0, '$x_0 =$ [0.5, 0.5]')]"
  },
  {
    "objectID": "lib_nbs/learning.html",
    "href": "lib_nbs/learning.html",
    "title": "Learning and benchmarking",
    "section": "",
    "text": "This notebook gathers the functions needed to train agents to forage optimally, as well as tools to calculate their foraging efficiency as well as their comparison to benchmark foraging strategies.\n\nLearning\n\n\nlearning\n\n learning (config, results_path, run)\n\nTraining of the RL agent\n\n\n\n\nType\nDetails\n\n\n\n\nconfig\ndict\nDictionary with all the parameters\n\n\nresults_path\nstr\nPath to save the results\n\n\nrun\nint\nAgent identifier\n\n\n\n\n\n\nGenerate walk from a policy\n\n\nwalk_from_policy\n\n walk_from_policy (policy, time_ep, n, L, Nt, r, lc, destructive=False,\n                   with_bound=False, bound=100)\n\nWalk of foragers given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npolicy\nlist\n\nStarting from counter=1, prob of continuing for each counter value.\n\n\ntime_ep\nint\n\nNumber of steps (decisions).\n\n\nn\nint\n\nNumber of agents that walk in parallel (all with the same policy, they do not interact). This is “number of walks” in the paper.\n\n\nL\nint\n\nWorld size.\n\n\nNt\nint\n\nNumber of targets.\n\n\nr\nfloat\n\nTarget radius.\n\n\nlc\nfloat\n\nCutoff length. Agent is displaced a distance lc from the target when it finds it.\n\n\ndestructive\nbool\nFalse\nTrue if targets are destructive. The default is False.\n\n\nwith_bound\nbool\nFalse\nTrue if policy is cut. The default is False.\n\n\nbound\nint\n100\nBound of the policy (maximum value for the counter). The default is 20.\n\n\nReturns\nlist, len(rewards)=n\n\nNumber of targets found by each agent in time_ep steps of d=1.\n\n\n\n\n\n\nEfficiency computation\n\n\nagent_efficiency\n\n agent_efficiency (results_path, config, run, num_walks, episode_interval)\n\nComputes the agent’s average search efficiency over a number of walks where the agent follows a fixed policy. This is repeated with the policies at different stages of the training to analyze the evolution of its performance.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nresults_path\nstr\nPath to the results folder, from which to extract the agent’s policies\n\n\nconfig\ndict\nDictionary with all the parameters. It needs to be the same configuration file as the one used to train the agent.\n\n\nrun\nint\nId of the agent\n\n\nnum_walks\nint\nNumber of (independent) walks\n\n\nepisode_interval\nint\nEvery ‘episode_interval’ training episodes, the policy of the agent is taken and its performance is analyzed.\n\n\n\n\n\n\nBenchmarks\nCode to get the search efficiency of the benchmark models. We consider Lévy and bi-exponential distributions and obtain the model parameters that achieve the highest search efficiency. We use the library Tune for the efficiency optimization within given parameter ranges.\n\n\naverage_search_efficiency\n\n average_search_efficiency (config)\n\nGet the average search efficiency, considering the benchmark model defined in config.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nconfig\ndict\nDictionary with the configuration of the benchmark model.\n\n\n\n\n\nExample\nSet up the configuration, run and type of search\n\nfrom rl_opts.learn_and_bench import average_search_efficiency\nfrom ray import tune\nfrom ray.tune.search.bayesopt import BayesOptSearch\nfrom ray.tune.search import ConcurrencyLimiter\nimport numpy as np\n\n\n#### Minimal example #####\nrun = '0'\nsearch_type = 'Bayesian'\nconfig = {'num_raytune_samples': 3,\n          'd_int': tune.uniform(0.00001, 20.0),\n          'd_ext': 100.0,\n          'p': tune.uniform(0.0, 1.0),\n          'beta': None,\n          'model': 'double_exp',\n          'time_ep': 20,\n          'n': 10,\n          'lc': 3.0,\n          'Nt': 100,\n          'L': 100,\n          'r': 0.5,\n          'destructive': False,\n         }\n\nInitialize Tune\n\nif search_type == 'Bayesian': #Bayesian optimization\n    \n    bayesopt = BayesOptSearch(metric=\"mean_eff\", mode=\"max\")\n    bayesopt = ConcurrencyLimiter(bayesopt, max_concurrent=3)\n    tuner = tune.Tuner(average_search_efficiency, \n                        tune_config=tune.TuneConfig(search_alg=bayesopt, num_samples=config['num_raytune_samples']), \n                        param_space=config)\n    \nelif search_type == 'Grid':#Grid search\n\n    tuner = tune.Tuner(average_search_efficiency,\n                        tune_config=tune.TuneConfig(num_samples=1),\n                        param_space=config)\n\nRun the algorithm\n\nresult_grid = tuner.fit()\n\nSave the results and the configuration in the corresponding folders\n\n# Results path\nresults_path = 'results/benchmark_models/' + config['model'] + '/'+ str(config['lc']) + '/run_'+run+'/'\npathlib.Path(results_path).mkdir(parents=True, exist_ok=True)\n\n# Configuration path\nconfig_path = 'configurations/benchmark_models/'\npathlib.Path(config_path).mkdir(parents=True, exist_ok=True)\n\n# Save results\nresults_df = result_grid.get_dataframe()\nresults_df.to_csv(results_path+'df'+run+'_'+config['model']+'_lc_'+str(config['lc'])+'.csv')\nnp.save(config_path+'config_'+config['model']+'_lc_'+str(config['lc'])+'_run_'+run+'.npy', config)"
  },
  {
    "objectID": "lib_nbs/rl_framework.html",
    "href": "lib_nbs/rl_framework.html",
    "title": "Reinforcement learning framework",
    "section": "",
    "text": "This notebook gathers the functions creating the RL framework proposed in our work. Namely, it can be use to generate both the foraging environment as well as the agents moving on them.\n\nEnvironment\nClass that defines the foraging environment\n\n\nTargetEnv\n\n TargetEnv (Nt, L, r, lc, agent_step=1, boundary_condition='periodic',\n            num_agents=1, high_den=5, destructive=False)\n\nClass defining the foraging environment. It includes the methods needed to place several agents to the world.\n\n\n\nProjective Simulation agent\n\n\nPSAgent\n\n PSAgent (num_actions, num_percepts_list, gamma_damping=0.0,\n          eta_glow_damping=0.0, policy_type='standard', beta_softmax=3,\n          initial_prob_distr=None, fixed_policy=None)\n\nBase class of a Reinforcement Learning agent based on Projective Simulation, with two-layered network. This class has been adapted from https://github.com/qic-ibk/projectivesimulation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_actions\nint >=1\n\nNumber of actions.\n\n\nnum_percepts_list\nlist of integers >=1, not nested\n\nCardinality of each category/feature of percept space.\n\n\ngamma_damping\nfloat\n0.0\nForgetting/damping of h-values at the end of each interaction. The default is 0.0.\n\n\neta_glow_damping\nfloat\n0.0\nControls the damping of glow; setting this to 1 effectively switches off glow. The default is 0.0.\n\n\npolicy_type\nstr\nstandard\nToggles the rule used to compute probabilities from h-values. See probability_distr. The default is ‘standard’.\n\n\nbeta_softmax\nint\n3\nProbabilities are proportional to exp(beta*h_value). If policy_type != ‘softmax’, then this is irrelevant. The default is 3.\n\n\ninitial_prob_distr\nNoneType\nNone\nIn case the user wants to change the initialization policy for the agent. This list contains, per percept, a list with the values of the initial h values for each action. The default is None.\n\n\nfixed_policy\nNoneType\nNone\nIn case the user wants to fix a policy for the agent. This list contains, per percept, a list with the values of the probabilities for each action. Example: Percept 0: fixed_policy[0] = [p(a0), p(a1), p(a2)] = [0.2, 0.3, 0.5], where a0, a1 and a2 are the three possible actions. The default is None.\n\n\n\n\n\n\nGeneral forager agent\n\n\nForager\n\n Forager (state_space, num_actions, visual_cone=3.141592653589793,\n          visual_radius=1.0, **kwargs)\n\nThis class extends the general PSAgent class and adapts it to the foraging scenario·\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstate_space\nlist\n\nList where each entry is the state space of each perceptual feature.E.g. [state space of step counter, state space of density of successful neighbours].\n\n\nnum_actions\nint\n\nNumber of actions.\n\n\nvisual_cone\nfloat\n3.141592653589793\nVisual cone (angle, in radians) of the forager, useful in scenarios with ensembles of agents. The default is np.pi.\n\n\nvisual_radius\nfloat\n1.0\nRadius of the visual region, useful in scenarious with ensembles of agents. The default is 1.0.\n\n\nkwargs"
  },
  {
    "objectID": "lib_nbs/utils.html",
    "href": "lib_nbs/utils.html",
    "title": "Utils",
    "section": "",
    "text": "This notebook gathers multiple functions used as helpers in the library.\n\nHelpers for the environments\n\n\nisBetween_c_Vec\n\n isBetween_c_Vec (a, b, c, r)\n\nChecks whether point c is crossing the line formed with point a and b.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\na\ntensor, shape = (1,2)\nPrevious position.\n\n\nb\ntensor, shape = (1,2)\nCurrent position.\n\n\nc\ntensor, shape = (Nt,2)\nPositions of all targets.\n\n\nr\nint/float\nTarget radius.\n\n\nReturns\ntorch.tensor of boolean values\nTrue at the indices of found targets.\n\n\n\n\n\n\ncoord_mod\n\n coord_mod (coord1, coord2, mod)\n\nComputes the distance difference between two coordinates, in a world with size ‘mod’ and periodic boundary conditions.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncoord1\nvalue, np.array, torch.tensor (can be shape=(n,1))\nFirst coordinate.\n\n\ncoord2\nnp.array, torch.tensor – shape=(1,1)\nSecond coordinate, substracted from coord1.\n\n\nmod\nint\nWorld size.\n\n\nReturns\nfloat\nDistance difference (with correct sign, not absolute value).\n\n\n\n\n\n\nisBetween_c_Vec_nAgents\n\n isBetween_c_Vec_nAgents (a, b, c, r)\n\nChecks whether point c is crossing the line formed with point a and b. Code to run several agents in parallel.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\na\narray, shape = (n,2)\nPrevious position of all n agents.\n\n\nb\narray, shape = (n,2)\nCurrent position of all n agents.\n\n\nc\narray, shape = (Nt,2)\nPositions of all targets.\n\n\nr\nfloat\nTarget radius\n\n\nReturns\narray of boolean values, shape = (Nt, n)\nTrue at the indices of found targets.\n\n\n\n\n\n\nget_encounters\n\n get_encounters (agent_previous_pos, agent_pos, target_positions, L, r)\n\nConsidering the agent walks, it checks whether the agent finds a target while walking the current step. Code to run several agents in parallel.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nagent_previous_pos\narray, shape = (n,2)\nPosition of the n agents before taking the step.\n\n\nagent_pos\narray, shape = (n,2)\nPosition of the n agents.\n\n\ntarget_positions\narray, shape = (Nt,2)\nPositions of the targets.\n\n\nL\nint\nWorld size.\n\n\nr\nfloat\nRadius of the targets.\n\n\nReturns\narray of boolean values, shape = (Nt, n)\nTrue at the indices of found targets.\n\n\n\n\n\n\nLoading data\n\n\nget_config\n\n get_config (config, config_path='configurations/learning/')\n\nGet the configuration file of the given experiment and config name (e.g. exp_0).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\nstr\n\nConfig name (e.g. exp_0)\n\n\nconfig_path\nstr\nconfigurations/learning/\npath to configurations\n\n\nReturns\ndict\n\nDictionary with the parameters of the loaded configuration.\n\n\n\n\n\n\nget_policy\n\n get_policy (results_path, agent, episode)\n\nGets the policy of an agent at a given episode.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nresults_path\nstr\nPath of the folder from which to extract the data.\n\n\nagent\nint\nAgent index.\n\n\nepisode\nint\nEpisode.\n\n\nReturns\nlist\nPolicy.\n\n\n\n\n\n\nget_performance\n\n get_performance (results_path, agent_list, episode_list)\n\nExtract data with the efficiencies obtained in the postlearning analysis.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nresults_path\nstr\nPath of the folder from which to extract the data.\n\n\nagent_list\nlist\nList with the agent indices.\n\n\nepisode_list\nlist\nList with the episodes.\n\n\nReturns\nnp.array, shape=(len(agent_list), len(episode_list))\nAverage performances obtained by the agents in the postlearning analysis.\n\n\n\n\n\n\nget_opt\n\n get_opt (main_path, lc, model='powerlaw', run='0')\n\nGet the highest efficiency obtained by the benchmark models and the corresponding parameters.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmain_path\nstr\n\nPath from which to get the data.\n\n\nlc\nfloat\n\nCutoff length.\n\n\nmodel\nstr\npowerlaw\nBenchmark model. The default is ‘powerlaw’.\n\n\nrun\nstr\n0\nRun index. The default is ‘0’.\n\n\nReturns\nfloat\n\nMean efficiency.\n\n\n\n\nfrom nbdev import nbdev_export; nbdev_export()"
  }
]