{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be70a58-5e4d-4b9b-9e0f-8f60aff70737",
   "metadata": {},
   "source": [
    "# Robot search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50860bf-b0b5-4003-bf10-e528104a98b3",
   "metadata": {},
   "source": [
    "In this notebook we introduce the RL framework for target searching with robots. It is based on the agents and environments presented in `rl_opts.rl_framework`. Some features of the code are:\n",
    "\n",
    "- we use `numba` to improve speed.\n",
    "- we implement more efficient ways of updating the H and G matrix (contribution by Dr. Michele Caraglio).\n",
    "- we consider as base case `num_agents = 1`. \n",
    "- we modify the old version of our agents to make them more realistic and more similar to the dynamics of real robots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee201d24-d37c-44a4-8168-1cff008a29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp robot_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d9282-bc0a-495c-a424-dac1e0eb8c4e",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b502b-ca4b-40d3-a091-3a87fa2f6d4b",
   "metadata": {},
   "source": [
    "Input libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2d9fc-79c7-4324-9031-4cf5de678f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba.experimental import jitclass\n",
    "from numba import jit, float64, int64, bool_, prange, njit\n",
    "import math\n",
    "import random\n",
    "#from rl_opts.utils import isBetween_c_Vec, coord_mod\n",
    "NOPYTHON = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c81e1f-aca9-4307-8ea9-f3be43f7cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| exec: false\n",
    "# for debugging\n",
    "NOPYTHON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b5b0c-9a99-4388-a20c-638ace2f3689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2b189-f187-40dc-a91d-56a91438f536",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825fff41-5fdb-4f25-9be4-268703b23078",
   "metadata": {},
   "source": [
    "## Crossing targets in straight line (isBetween)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a79bc-253d-4462-826d-9b77760eeb4e",
   "metadata": {},
   "source": [
    "Function to evaluate whether the robot walking in a straight line from point a to point b crosses point c (targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bea7a-fe68-4724-93f2-1fac08f443a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def isBetween_c_vec_numba(a, b, c, r):\n",
    "        \"\"\"\n",
    "        Checks whether point c is crossing the line formed with point a and b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : tensor, shape = (1,2)\n",
    "            Previous position.\n",
    "        b : tensor, shape = (1,2)\n",
    "            Current position.\n",
    "        c : tensor, shape = (Nt,2)\n",
    "            Positions of all targets.\n",
    "        r : int/float\n",
    "            Target radius.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask : array of boolean values\n",
    "            True at the indices of found targets.\n",
    "\n",
    "        \"\"\"\n",
    "        if (a == b).all():\n",
    "            return np.array([False]*c.shape[0])\n",
    "\n",
    "        mask = np.array([True]*c.shape[0])\n",
    "        \n",
    "        dotproduct = (c[:, 0] - a[0]) * (b[0] - a[0]) + (c[:, 1] - a[1])*(b[1] - a[1])\n",
    "        squaredlengthba = (b[0] - a[0])*(b[0] - a[0]) + (b[1] - a[1])*(b[1] - a[1])\n",
    "        \n",
    "        #exclude the targets whose vertical projection of the vector c-a w.r.t. the vector b-a is larger than the target radius.\n",
    "        idx = np.argwhere(np.abs(numba.np.arraymath.cross2d(b-a, c-a))/np.linalg.norm(b-a) > r) \n",
    "        for i1 in idx:\n",
    "            mask[i1] = False        \n",
    "        \n",
    "        #exclude the targets whose scalar product is negative (they are on the other side of the step direction)\n",
    "        for i2 in np.argwhere(dotproduct < 0):\n",
    "            mask[i2] = False\n",
    "\n",
    "        #exclude the targets that are beyond the step.\n",
    "        for i3 in np.argwhere(dotproduct > squaredlengthba):\n",
    "            mask[i3] = False\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08913849-3f79-4381-846c-879f42eab244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "compiling = isBetween_c_vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658b787-7668-4fbb-92b0-eeed53a0dbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.48 µs ± 16.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit isBetween_c_vec_numba(np.array([0.1,1]), np.array([1,5]), np.random.rand(100,2), 0.00001)\n",
    "# Run time of new version:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9b311-2498-4574-a5df-26840f8ecd33",
   "metadata": {},
   "source": [
    "## Crossing targets while walking along an arc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d04b15-4950-49ca-8d48-4f9e67377983",
   "metadata": {},
   "source": [
    "Functions to evaluate whether the robot crosses a target while reorienting itself (action turn). During reorientation, the robot walks along a circumference arc.\n",
    "- rotation_arc: creates the points of the arc (how many points are created can be chosen).\n",
    "- arc_in_target_numba: evaluates whether the arc points are within a target area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8103ff-a5f4-4daf-a511-a36d5dc2a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def rotation_arc(theta : float, # Angle in rad (from origin, counterclockwise) describing the current orientation of the robot.\n",
    "                 dtheta : float, # Angle difference in rad (with appropriate sign!! Left turn: dtheta>0, Right turn: dtheta<0). How much the robot rotates when it decides to turn.\n",
    "                 num_points_arc:int=50, # Number of points that form the arc that is going to be evaluated.\n",
    "                 R:float=1, # Rotation radius.\n",
    "                 x:float=0, # Horizontal component of the robot's current position.\n",
    "                 y:float=0 # Vertical component of the robot's current position.\n",
    "                ):\n",
    "    \"\"\"Creates the points of the circumference arc covered by the robot while it reorients itself (action \"turn\").\"\"\"\n",
    "    \n",
    "    phi = np.pi - theta\n",
    "    \n",
    "    # Get origin of rotation\n",
    "    origin_x = x -  R * np.cos(phi)\n",
    "    origin_y = y +  R * np.sin(phi)\n",
    "    \n",
    "    # Generate points for the arc in rad: from start point to final point = start point + length of arc\n",
    "    angles = np.linspace(-phi, -phi + dtheta, num_points_arc)\n",
    "    x_arc = origin_x + R * np.cos(angles) #x positions of all the arc points\n",
    "    y_arc = origin_y + R * np.sin(angles) #y positions of all the arc points\n",
    "    \n",
    "    return x_arc, y_arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1810e-02f5-457d-9ea1-4f29a9d16d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def arc_in_target_numba(target_coord : np.array, # Array (number targets, 2) with the coordinates of all targets.\n",
    "                        target_radius : float, # Radius of each target.\n",
    "                        x_arc: np.array, # Horizontal components of arc points.\n",
    "                        y_arc: np.array, # Vertical components of arc points.\n",
    "                       )->np.array: # 1D array of size (number targets) with 1 on the indices of the found targets.\n",
    "    \"\"\"\n",
    "    Checks which targets have been found by the agent when it reorients itself.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape the arc arrays to make them broadcastable\n",
    "    x_arc_broadcasted = x_arc[:, np.newaxis]  # Shape (num_points_arc, 1)\n",
    "    y_arc_broadcasted = y_arc[:, np.newaxis]  # Shape (num_points_arc, 1)\n",
    "\n",
    "    # Compute the squared distances from all arc points to all target centers\n",
    "    distances_squared = (x_arc_broadcasted - target_coord[:, 0])**2 + (y_arc_broadcasted - target_coord[:, 1])**2\n",
    "\n",
    "    # Check if these distances are within the target radius squared\n",
    "    mask_in_target = distances_squared <= target_radius**2\n",
    "\n",
    "    # Summing over the arc points axis (axis=0), find which targets are found\n",
    "    mask_found_targets = np.sign(np.sum(mask_in_target, axis=0))\n",
    "    \n",
    "    return mask_found_targets.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03202ead-4f3a-42b6-9fb1-f4dd2c649079",
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = np.random.rand(100, 2)* 100\n",
    "target_r = 1\n",
    "a_x, a_y = rotation_arc(np.pi/6, np.pi/3, num_points_arc=50, R=1, x=0, y=0)\n",
    "compile_first = arc_in_target_numba(targ, target_r, a_x, a_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e336ff-8325-455e-8946-92aef3e20c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.96 µs ± 14 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit arc_in_target_numba(targ, target_r, a_x, a_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8ac1c-f694-40da-ab68-3cea5abcfb7d",
   "metadata": {},
   "source": [
    "## Random sampling from array with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80744d-f8e6-4c4a-85a9-21b73d13615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def rand_choice_nb(arr : np.array, # 1D numpy array of values to sample from.\n",
    "                   prob : np.array # 1D numpy array of probabilities for the given samples.\n",
    "                  ): # Random sample from the given array with a given probability.    \n",
    "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd29058-7bd7-4878-ba46-520e0052d9d4",
   "metadata": {},
   "source": [
    "# RL agent (Projective Simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fab216-2991-4ad4-99ff-5262a20b6d12",
   "metadata": {},
   "source": [
    "We use Projective Simulation (PS) as a framework for Reinforcement Learning (RL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533a0be-28fa-4e33-8f2a-eb1ed7a7beee",
   "metadata": {},
   "source": [
    "## Forager \n",
    "This agent is an efficient version of rl_opts.rl_framework.Forager with:\n",
    "\n",
    "- `numba` implementation\n",
    "-  H and G matrix efficient updates (contribution by Michele Caraglio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa129b71-2ae5-46fb-a327-45d14a317cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from rl_opts.rl_framework.numba.agents import Forager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeddba7-98e1-45a2-8d52-2ce658364b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Forager(3,np.array([100], dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf65e4c-8a28-42d2-b5e4-bf8fa227562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.act(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9287061-beb9-4ef1-9556-ac99fc770eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.agent_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc2a39-7403-4a48-91b7-8dbaa3f5dede",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac9792-10e9-43fc-9732-d9494ee5440d",
   "metadata": {},
   "source": [
    "## Single Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814ea68-75e1-4adc-8743-9f6f4038608d",
   "metadata": {},
   "source": [
    "Environment where a simulated robot (RL agent) searches for targets. The robot cannot see the targets. It gets a signal from the environment in the form of a positive reward whenever its position is within a target area.\n",
    "\n",
    "- States: the agent perceives the counter of steps that it has been walking straight, i.e. it decided to continue in the same direction.\n",
    "\n",
    "- Actions: \"continue\" , \"turn left\" or \"turn right\".\n",
    "    - For \"continue\": walks straight in the same direction (no reorientation) for a distance $v$ sampled from a normal distribution. $N(\\alpha, \\sigma_\\alpha)$\n",
    "    - For \"turning\" actions: normal distribution centered on some angle to the left or to the right. $N(\\beta, \\sigma_\\beta)$\n",
    "    - When it turns, it just reorients (NO EXTRA DISPLACEMENT). The reorientation of the target detector occurs with respect to the robot's center, so (in contrast to our previous ideal agent) it also displaces its effective position when it reorients. Here, we consider that the effective position of the agent is that of the detector (not of the robot's center).\n",
    "\n",
    "- Instead of kick, here we implement a time delay $\\tau$. The agent needs to wait $\\tau$ time steps from the moment it gets a target until it can get the same target again. \n",
    "\n",
    "- When the agent crosses a target, it resets the counter of steps walking straight. Its position doesn't change, it continues from where it was when it found the target. No extra reorientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902917eb-6068-4978-80dd-98bfd978a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"tau_allagents\", float64[:]) ,\n",
    "           (\"depleted\", float64[:,:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"mask\", bool_[:])])\n",
    "           \n",
    "class RobotSearch():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    tau : float\n",
    "    agent_radius:float\n",
    "    avg_vel:float\n",
    "    std_vel:float\n",
    "    avg_turn_angle:float\n",
    "    std_turn_angle:float\n",
    "    num_agents : int\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    tau_allagents : np.array\n",
    "    depleted : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt:int, # Number of targets.\n",
    "                 L:float, # Size of the (squared) world.\n",
    "                 r:float, # Radius with center the target position. It defines the area in which agent detects the target.\n",
    "                 tau:float, # Time it takes for the targets to replenish.\n",
    "                 agent_radius:float, # Distance from robot's center to detector.\n",
    "                 avg_vel:float, # Mean displacement of the agent per time step (v). Normal distr.: N(v,std_v).\n",
    "                 std_vel:float, # Std for the normal distr. of the agent's velocity (std_v).\n",
    "                 avg_turn_angle:float, # Mean turning angle (in rad) when agent decides to turn (Theta). Normal distr.: N(Theta,std_theta).\n",
    "                 std_turn_angle:float, # Std for the normal distr. of the agent's turning velocity (std_theta).\n",
    "                 num_agents:int=1 # Number of agents that forage at the same time.\n",
    "                 ):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Class defining the foraging environment with replenishable targets modeled with a time delay instead of a kick. \n",
    "        It includes the methods to simulate the robot dynamics.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.agent_radius = agent_radius\n",
    "        self.avg_vel = avg_vel\n",
    "        self.std_vel = std_vel\n",
    "        self.avg_turn_angle = avg_turn_angle\n",
    "        self.std_turn_angle = std_turn_angle\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # same replenishing time for all agents\n",
    "        self.tau_allagents = tau * np.ones(self.num_agents)\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #number of steps since last target encounter, for each agent.\n",
    "        self.depleted = np.zeros((self.Nt, self.num_agents))\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, action, agent_index = 0):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            0 (continue), 1 (left), 2 (right).\n",
    "        agent_index : int, optional\n",
    "            Index of the given agent. The default is 0.\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        (x_arc, y_arc): (np.array, np.array)\n",
    "            Positions of the turning arc. In case the agent does not turn, it returns (None, None).\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if action == 0: #continue\n",
    "            # Update the velocity of the agent\n",
    "            v = np.random.normal( max(0,self.avg_vel), max(0,self.std_vel))\n",
    "            # Get the positional changes\n",
    "            dx = v * np.cos(self.current_directions[agent_index])\n",
    "            dy = v * np.sin(self.current_directions[agent_index])\n",
    "            \n",
    "            x_arc = None\n",
    "            y_arc = None\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if action == 1: #left turn\n",
    "                # Get angle difference\n",
    "                dtheta = np.random.normal(self.avg_turn_angle, self.std_turn_angle)\n",
    "\n",
    "            elif action == 2: #right turn\n",
    "                # Get angle difference\n",
    "                dtheta = - np.random.normal(self.avg_turn_angle, self.std_turn_angle)\n",
    "\n",
    "            # Get new orientation of the agent\n",
    "            new_theta = self.current_directions[agent_index] + dtheta\n",
    "            # Get the positional changes\n",
    "            dx = self.agent_radius * (np.cos(self.current_directions[agent_index]) - np.cos(new_theta))\n",
    "            dy = self.agent_radius * (np.sin(self.current_directions[agent_index]) - np.sin(new_theta))\n",
    "            \n",
    "            \n",
    "            # Get arc coordinates (default: 50 points)\n",
    "            x_arc, y_arc = rotation_arc(self.current_directions[agent_index], dtheta, R=self.agent_radius, x=self.positions[agent_index][0], y=self.positions[agent_index][1])\n",
    "            \n",
    "            # Update agent's orientation\n",
    "            self.current_directions[agent_index] = new_theta\n",
    "        \n",
    "       \n",
    "        # Update the position\n",
    "        self.positions[agent_index][0] += dx\n",
    "        self.positions[agent_index][1] += dy\n",
    "        \n",
    "        return x_arc, y_arc\n",
    "        \n",
    "       \n",
    "    def check_encounter(self, action, x_arc, y_arc, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            Action index. Continue (0), turn left (1) or turn right (2).\n",
    "        x_arc : np.array or None\n",
    "            Horizontal components of the turning arc coordinates. None if agent walked straight.\n",
    "        y_arc : np.array or None\n",
    "            Vertical components of the turning arc coordinates. None if agent walked straight.\n",
    "        agent_index : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True if the agent found a target.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if there was a target encounter\n",
    "        if action == 0:\n",
    "            encounters = isBetween_c_vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        else:\n",
    "            encounters = arc_in_target_numba(self.target_positions, self.r, x_arc, y_arc)\n",
    "            \n",
    "        #In case there were encounters:\n",
    "        if np.sum(encounters) > 0: \n",
    "            # If there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "                \n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "           \n",
    "            # If the agent is NOT waiting for the recently found target to replenish:\n",
    "            if not self.depleted[first_encounter, agent_index]:\n",
    "                # We add the information that this agent got a reward\n",
    "                self.current_rewards[agent_index] = 1\n",
    "                # The replenishing time for this target and this agent starts.\n",
    "                self.depleted[first_encounter, agent_index] += 1\n",
    "                \n",
    "                # The env issues a positive reward.\n",
    "                return 1\n",
    "            else:\n",
    "                self.current_rewards[agent_index] = 0\n",
    "                return 0\n",
    "                    \n",
    "        else:\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "    \n",
    "    def update_replenishing_times(self, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Updates the replenishing times of the targets for a given agent.\n",
    "        \"\"\"\n",
    "        self.depleted[np.argwhere(self.depleted[:, agent_index] != 0).flatten(), agent_index] += 1\n",
    "        self.depleted[:, agent_index] = self.depleted[:, agent_index] % (self.tau_allagents[agent_index] + 1)\n",
    "        \n",
    "    def check_bc(self, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of a given agent to fulfill fixed boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        self.positions[agent_index][0]  = max( 0, min( self.positions[agent_index][0] , self.L ) )\n",
    "        self.positions[agent_index][1]  = max( 0, min( self.positions[agent_index][1] , self.L ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fea60d-a54f-4ef2-b637-24d969e2e8bf",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f3129-cdd6-4a8a-b339-6e646f7ad8f4",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de1bb1-523b-4041-863c-8f3f7e596382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def train_loop_robot(episodes : int, # Number of episodes to train\n",
    "                   time_ep : int, # Length of episode\n",
    "                   agent : object, # Agent class\n",
    "                   env : object, # Environment class\n",
    "                   time_steps_save_h = None #Optional: int, every time_steps_save_h, the h_matrix is stored. Default: time_ep (at the end of episode)\n",
    "                  )-> tuple: # Rewards and h-matrix of the trained agent      \n",
    "    '''    \n",
    "    Given an agent and environment, performs a loop train for the `RobotSearch` environment.\n",
    "    '''\n",
    "    \n",
    "    if time_steps_save_h == None:\n",
    "        time_steps_save_h = time_ep\n",
    "    #Initialize storage of h matrices (it overwrites every episode)\n",
    "    number_stored_h_matrices = int(time_ep / time_steps_save_h)\n",
    "    save_h_matrices = np.zeros((number_stored_h_matrices, agent.h_matrix.shape[-2],agent.h_matrix.shape[-1]))\n",
    "    \n",
    "    #Initialize storage of rewards\n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            \n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                action = rand_choice_nb(arr = np.arange(1, agent.num_actions), prob = np.array([1/len(np.arange(1,agent.num_actions))]*len(np.arange(1,agent.num_actions))))\n",
    "                _,_ = env.update_pos(action)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                \n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "                \n",
    "                #update positions and get coordinates for turning arc (in case agent turned, otherwise these are None)\n",
    "                x_arc, y_arc = env.update_pos(action)\n",
    "                \n",
    "                #update replenishing times\n",
    "                env.update_replenishing_times()\n",
    "                \n",
    "                #check if target was found \n",
    "                reward = env.check_encounter(action, x_arc, y_arc)\n",
    "                \n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                \n",
    "                #learn\n",
    "                if reward == 1:\n",
    "                    agent._learn_post_reward(reward)\n",
    "                    \n",
    "                    #reset counter and g matrix\n",
    "                    agent.agent_state = 0\n",
    "                    agent.reset_g()\n",
    "                    \n",
    "            #If H matrix has not been updated for max_no_H_update steps, it updates.\n",
    "            if agent.N_upd_H == agent.max_no_H_update-1:\n",
    "                agent._learn_post_reward(reward)\n",
    "            \n",
    "            #Saving time step\n",
    "            if (t+1) % time_steps_save_h == 0:\n",
    "                \n",
    "                #Update of the H matrix, in case it was not updated in the last time step\n",
    "                if agent.N_upd_H != 0:\n",
    "                    agent._learn_post_reward(0)\n",
    "                    \n",
    "                #Save updated h matrix\n",
    "                save_h_matrices[int((t+1) / time_steps_save_h -1)] = agent.h_matrix\n",
    "             \n",
    "            #Saving\n",
    "            save_rewards[ep, t] = reward\n",
    "            \n",
    "        #Update of the H matrix at the end of the episode in case it was not updated in the last time steps\n",
    "        if agent.N_upd_H != 0:\n",
    "            agent._learn_post_reward(0)\n",
    "        \n",
    "    return (save_rewards, save_h_matrices)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36705d-36e6-45a5-9a95-f6c7510ab0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ep = 100\n",
    "\n",
    "env = RobotSearch(Nt=100, \n",
    "                 L=100, \n",
    "                 r=1, \n",
    "                 tau=3, \n",
    "                 agent_radius=2, # Size of the agent (radius, from the center to the side).\n",
    "                 avg_vel=1, # Mean displacement of the agent per time step (v). Normal distr.: N(v,std_v).\n",
    "                 std_vel=0.3, # Std for the normal distr. of the agent's velocity (std_v).\n",
    "                 avg_turn_angle=np.pi/6, # Mean turning angle (in rad) when agent decides to turn (Theta). Normal distr.: N(Theta,std_theta).\n",
    "                 std_turn_angle=np.pi/18) # Std for the normal distr. of the agent's turning velocity (std_theta).\n",
    "\n",
    "agent = Forager(num_actions = 3, # From here are props of the agent (see Forager for details)\n",
    "                gamma_damping = 0.2,\n",
    "                size_state_space = np.array([time_ep], dtype=np.int64),\n",
    "                initial_prob_distr = (np.array([0.988, 0.012/2, 0.012/2])*np.ones((3, time_ep)).transpose()).transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efc4cb-4846-4666-8042-7cbfc6f216d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, h_matrices = train_loop_robot(episodes = 1, time_ep = time_ep, agent = agent, env = env, time_steps_save_h = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d3116-3590-4205-9e32-79c3f64b5e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(h_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e4c84-c84e-4465-84d9-14fe8f1387c9",
   "metadata": {},
   "source": [
    "## Parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca276db6-3318-4577-87eb-fce17438c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit(nopython = NOPYTHON, parallel = True)\n",
    "def run_robot_training_parallel(episodes, # Number of episodes\n",
    "                                time_ep, # Length of episode\n",
    "                                N_agents, # Number of agents               \n",
    "                                time_steps_save_h = None, # Every how many steps is the h matrix stored\n",
    "                                Nt = 100, # From here are props of the environment (see TargetEnv for details) \n",
    "                                L = 100, \n",
    "                                r = 0.5, \n",
    "                                tau = 3,\n",
    "                                agent_radius = 2,\n",
    "                                avg_vel=1,\n",
    "                                std_vel=0.3,\n",
    "                                avg_turn_angle=np.pi/6,\n",
    "                                std_turn_angle=np.pi/18,\n",
    "                                num_actions = 3, # From here are props of the agent (see Forager for details)\n",
    "                                size_state_space = np.array([100], dtype=np.int64), \n",
    "                                gamma_damping = 0.00001,\n",
    "                                eta_glow_damping = 0.1,\n",
    "                                initial_prob_distr = np.array([[],[],[]]),\n",
    "                                policy_type = 'standard', \n",
    "                                beta_softmax = 3,  \n",
    "                                fixed_policy = np.array([[],[]]),\n",
    "                                max_no_H_update = int(1e4),\n",
    "                                g_update = 's'\n",
    "                               ):\n",
    "    #mean reward per episode\n",
    "    save_rewards = np.zeros((N_agents, episodes))\n",
    "    \n",
    "    #reward per time step (for single but long episodes)\n",
    "    #save_rewards = np.zeros((N_agents, time_ep))\n",
    "    \n",
    "    #storage of h matrices\n",
    "    if time_steps_save_h == None:\n",
    "        time_steps_save_h = time_ep\n",
    "    \n",
    "    number_stored_h_matrices = int(time_ep / time_steps_save_h)\n",
    "    save_h_matrix = np.zeros((N_agents, number_stored_h_matrices, num_actions, size_state_space[0]))  \n",
    "     \n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = RobotSearch(Nt, L, r, tau, agent_radius, avg_vel, std_vel, avg_turn_angle, std_turn_angle,1)\n",
    "        \n",
    "        agent = Forager(num_actions,size_state_space,gamma_damping,\n",
    "                        eta_glow_damping,policy_type,beta_softmax,\n",
    "                        initial_prob_distr,fixed_policy,max_no_H_update,g_update)\n",
    "                          \n",
    "        rews, mat = train_loop_robot(episodes, time_ep, agent, env, time_steps_save_h)\n",
    "             \n",
    "        #mean reward per episode\n",
    "        for t in range(episodes):\n",
    "            save_rewards[n_agent, t] = np.mean(rews[t])\n",
    "            \n",
    "        #reward per time step (for single but long episodes)\n",
    "        #save_rewards[n_agent,:] = rews[0,:]\n",
    "        \n",
    "        save_h_matrix[n_agent] = mat\n",
    "        \n",
    "    return save_rewards, save_h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2dbfd8-1555-490b-9393-facba3ea4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, h_matrix = run_robot_training_parallel(episodes = 2, time_ep = 10, N_agents = 3, time_steps_save_h = 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a1fcc-c197-4515-bff7-90d0b249ee2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 3, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(h_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12677d-9b61-46da-bc2a-275d44327d85",
   "metadata": {},
   "source": [
    "# Walk from policy\n",
    "\n",
    "These replicate what we were doing in `rl_opts.learn_and_bench.walk_from_policy` and help get efficiencies for fixed policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242babee-e1ff-42d9-80e6-c918bc34c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "@jit(nopython = NOPYTHON)\n",
    "def single_robot_trajectory(time_ep : int, # Length of each run / episode\n",
    "                            policy : np.array, # Policy of the walker (rows: states. columns:prob of doing each action)\n",
    "                            env : object # Environment where the walker moves\n",
    "                           )-> tuple: # number of visited targets, agent and target positions\n",
    "\n",
    "    '''\n",
    "    Walk of a single agent in env of type RobotSearch given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    '''\n",
    "    \n",
    "    number_visited_targets = 0\n",
    "    agent_positions = np.zeros((time_ep, 2))\n",
    "    rew_per_timestep = np.zeros(time_ep)\n",
    "    \n",
    "    #initialize environment and agent's counter and g matrix\n",
    "    env.init_env()\n",
    "    agent_state = 0\n",
    "\n",
    "    for t in range(time_ep):\n",
    "        \n",
    "        agent_positions[t,:] = env.positions[0] #position of agent with index 0\n",
    "        \n",
    "        #step to set counter to its min value n=1\n",
    "        if t == 0:\n",
    "            #do one step with random direction (no learning in this step)\n",
    "            action = rand_choice_nb(arr = np.arange(1, len(policy[0])), prob = np.array([1/len(np.arange(1,len(policy[0])))]*len(np.arange(1,len(policy[0])))))\n",
    "            _,_ = env.update_pos(action)\n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            \n",
    "\n",
    "        else: \n",
    "            # decide       \n",
    "            action = rand_choice_nb(arr = np.arange(len(policy[0])), prob = policy[agent_state])\n",
    "            #update positions\n",
    "            x_arc, y_arc = env.update_pos(action)\n",
    "            #update replenishing times\n",
    "            env.update_replenishing_times()\n",
    "            \n",
    "            #check if target was found \n",
    "            reward = env.check_encounter(action, x_arc, y_arc)\n",
    "            \n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            \n",
    "            # update agent_state\n",
    "            if action == 0: #continue\n",
    "                agent_state += 1\n",
    "            else: #turn\n",
    "                agent_state = 0\n",
    "                \n",
    "            if reward == 1:\n",
    "                agent_state = 0\n",
    "\n",
    "            number_visited_targets += reward\n",
    "            rew_per_timestep[t] += reward\n",
    "                \n",
    "    return (number_visited_targets, agent_positions, env.target_positions, rew_per_timestep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8403723-95e9-457a-89c2-e8f386025b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit(nopython = NOPYTHON, parallel = True)\n",
    "def parallel_robots_trajectories(time_ep : int, # Length of each run / episode\n",
    "                                 N_agents : int, # Number of agents to consider\n",
    "                                 Nt = 100, # Number of targets in the environment\n",
    "                                 L = 100, # Size of the environment\n",
    "                                 r = 0.5, # Radius of the targets\n",
    "                                 tau = 3, # Time delay until target is available again\n",
    "                                 agent_radius = 2,\n",
    "                                 avg_vel=1,\n",
    "                                 std_vel=0.3,\n",
    "                                 avg_turn_angle=np.pi/6,\n",
    "                                 std_turn_angle=np.pi/18,\n",
    "                                 policies = np.array([[1.0,0.0,0.0,0.0], [0.0,1.0,0.0,0.0]]) # Policy of the agents\n",
    "                                )-> tuple: # number of visited targets, agent and target positions\n",
    "    \"\"\"\n",
    "    Runs in parallel single_robot_trajectory. Due to numba props, we need to give all parameters as inputs (see source).\n",
    "    \"\"\"\n",
    "    \n",
    "    agent_positions = np.zeros((N_agents, time_ep, 2))\n",
    "    number_visited_targets = np.zeros(N_agents)\n",
    "    target_positions = np.zeros((N_agents, Nt, 2))\n",
    "    reward_per_timestep = np.zeros((N_agents, time_ep))\n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = RobotSearch(Nt, L, r, tau, agent_radius, avg_vel, std_vel, avg_turn_angle, std_turn_angle,1)\n",
    "        \n",
    "        num_tar, pos, tar_pos, rew_t = single_robot_trajectory(time_ep, policies[n_agent], env) \n",
    "    \n",
    "        agent_positions[n_agent] = pos\n",
    "        number_visited_targets[n_agent] = num_tar\n",
    "        target_positions[n_agent] = tar_pos\n",
    "        reward_per_timestep[n_agent] = rew_t\n",
    "        \n",
    "    return (number_visited_targets, agent_positions, target_positions, reward_per_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a841da4-030a-415e-8bcf-42d3c8deacdb",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f310a72-f356-4689-ad09-96c9747e912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a026aa8-0ac8-4fa6-aa59-3c74f15cb552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
