{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we update and improve the agents and environments presented in `rl_opts.rl_framework`. Two main changes:\n",
    "\n",
    "- we use `numba` to improve speed.\n",
    "- we implement more efficient ways of updating the H and G matrix (contribution by Dr. Michele Caraglio).\n",
    "- we consider as base case `num_agents = 1`. In previous versions we had this as an input which overcomplicated all functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp numba.rl_framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that defines the foraging environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba.experimental import jitclass\n",
    "from numba import jit, float64, int64, bool_, prange, njit\n",
    "import math\n",
    "import random\n",
    "#from rl_opts.utils import isBetween_c_Vec, coord_mod\n",
    "NOPYTHON = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| exec: false\n",
    "# for debugging\n",
    "NOPYTHON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## isBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def isBetween_c_Vec_numba(a, b, c, r):\n",
    "        \"\"\"\n",
    "        Checks whether point c is crossing the line formed with point a and b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : tensor, shape = (1,2)\n",
    "            Previous position.\n",
    "        b : tensor, shape = (1,2)\n",
    "            Current position.\n",
    "        c : tensor, shape = (Nt,2)\n",
    "            Positions of all targets.\n",
    "        r : int/float\n",
    "            Target radius.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask : array of boolean values\n",
    "            True at the indices of found targets.\n",
    "\n",
    "        \"\"\"\n",
    "        if (a == b).all():\n",
    "            return np.array([False]*c.shape[0])\n",
    "\n",
    "        mask = np.array([True]*c.shape[0])\n",
    "        \n",
    "        dotproduct = (c[:, 0] - a[0]) * (b[0] - a[0]) + (c[:, 1] - a[1])*(b[1] - a[1])\n",
    "        squaredlengthba = (b[0] - a[0])*(b[0] - a[0]) + (b[1] - a[1])*(b[1] - a[1])\n",
    "        \n",
    "        #exclude the targets whose vertical projection of the vector c-a w.r.t. the vector b-a is larger than the target radius.\n",
    "        idx = np.argwhere(np.abs(numba.np.arraymath.cross2d(b-a, c-a))/np.linalg.norm(b-a) > r) \n",
    "        for i1 in idx:\n",
    "            mask[i1] = False        \n",
    "        \n",
    "        #exclude the targets whose scalar product is negative (they are on the other side of the step direction)\n",
    "        for i2 in np.argwhere(dotproduct < 0):\n",
    "            mask[i2] = False\n",
    "\n",
    "        #exclude the targets that are beyond the step.\n",
    "        for i3 in np.argwhere(dotproduct > squaredlengthba):\n",
    "            mask[i3] = False\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "compiling = isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24 µs ± 7.62 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,5]), np.random.rand(100,2), 0.00001)\n",
    "# Run time of new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.utils import isBetween_c_Vec as oldbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.2 µs ± 2.9 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit oldbetween(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)\n",
    "# Run time of older version:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def pareto_sample(alpha : float, # Exponent of the power law\n",
    "                  xm : float, # Minimun value of the distribution\n",
    "                  size : int=1 # Number of samples\n",
    "                 )-> np.array : # Samples from the distribution\n",
    "    ''' Generates samples from a Pareto distribution of given parameters '''\n",
    "    samples = np.zeros(size)\n",
    "    for ii in range(size):\n",
    "        u = random.random()  # Uniform random variable between 0 and 1\n",
    "        x = xm / (u ** (1 / alpha))\n",
    "        samples[ii] = x\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sampling from array with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jit(nopython = NOPYTHON)\n",
    "def rand_choice_nb(arr : np.array, # 1D numpy array of values to sample from.\n",
    "                   prob : np.array # 1D numpy array of probabilities for the given samples.\n",
    "                  ): # Random sample from the given array with a given probability.    \n",
    "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"kicked\", float64[:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"lc\", float64[:,:]),\n",
    "           (\"mask\", bool_[:]),\n",
    "           (\"first_encounter\", float64[:,:])])\n",
    "class TargetEnv():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    lc : np.array\n",
    "    agent_step : float\n",
    "    num_agents : int\n",
    "    destructive_targets : bool\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    kicked : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    first_encounter : np.array\n",
    "    lc_distribution : str\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt = 10, # blabla\n",
    "                 L = 1.3,\n",
    "                 r = 1.5,\n",
    "                 lc = np.array([[1.0],[1]]),\n",
    "                 agent_step = 1,\n",
    "                 num_agents = 1,\n",
    "                 destructive = False,\n",
    "                 lc_distribution = 'constant'):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Class defining the foraging environment. It includes the methods needed to place several agents to the world.\n",
    "        \n",
    "        Updated from `rl_framework.TargetEnv`:        \n",
    "            > `lc_distribution`: now allows to consider different distributions. lc now means different things depending on the distribution.\n",
    "        \n",
    "        **Inputs**\n",
    "        \n",
    "        `Nt` : (int) \n",
    "            Number of targets.\n",
    "            \n",
    "        `L` : (int)\n",
    "            Size of the (squared) world.\n",
    "            \n",
    "        `r` : (int) \n",
    "            Radius with center the target position. It defines the area in which agent detects the target.\n",
    "            \n",
    "        `lc` \n",
    "            Cutoff length. Displacement away from target (to implement revisitable targets by displacing agent away from the visited target).\n",
    "            \n",
    "        `agent_step`: (int, optional)\n",
    "            Displacement of one step. The default is 1.\n",
    "            \n",
    "        `num_agents`: (int, optional)\n",
    "            Number of agents that forage at the same time. The default is 1.\n",
    "            \n",
    "        `destructive`: (bool, optional)\n",
    "            True if targets are destructive. The default is False.\n",
    "            \n",
    "        `lc_distribution`: (str) Chosee between 'power_law', 'pareto' and None. Depending on the previous, lc has different meanings:\n",
    "        \n",
    "        > `power_law` : lc is sampled from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0] \n",
    "        \n",
    "        > `pareto` : lc is sampled from a Pareto distribution with alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "        \n",
    "        > `None` : if len(lc) == 1, then that's the lc. If len(lc) > 1, then samples an lc considering vals = lc[0] and probabilities = lc[1]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.lc = lc\n",
    "        self.agent_step = agent_step \n",
    "        self.num_agents = num_agents\n",
    "        self.destructive_targets = destructive\n",
    "        self.lc_distribution = lc_distribution\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #signal whether agent has been kicked\n",
    "        self.kicked = np.zeros(self.num_agents)\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, change_direction):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        change_direction : bool\n",
    "            Whether the agent decided to turn or not.\n",
    "        agent_index : int, optional\n",
    "            Index of the given agent. The default is 0.\n",
    "        \"\"\"\n",
    "        agent_index = 0\n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if change_direction:\n",
    "            self.current_directions[agent_index] = random.uniform(0,1)*2*math.pi\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "       \n",
    "    def check_encounter(self):\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_index : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True if the agent found a target.\n",
    "\n",
    "        \"\"\"       \n",
    "        agent_index = 0\n",
    "        encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        \n",
    "        if sum(encounters) > 0: \n",
    "            \n",
    "            #if there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "            if self.destructive_targets:\n",
    "                self.target_positions[first_encounter] = np.random.rand(2)*self.L\n",
    "            else:\n",
    "                #----KICK----\n",
    "                # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                kick_direction = np.random.uniform(low = 0, high = 2*np.pi)\n",
    "                for idx_first in first_encounter: # This is super weird!\n",
    "                    if self.lc_distribution == 'power_law':\n",
    "                        # when we have the power law, the first value of lc is considered to be the exponent.\n",
    "                        # The following samples from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0]                        \n",
    "                        current_lc = (1-random.uniform(0,1))**(-1/self.lc.flatten()[0])\n",
    "\n",
    "                    elif self.lc_distribution == 'pareto':\n",
    "                        # Sampling from Pareto. Here alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "                        current_lc = pareto_sample(self.lc[0,0], self.lc[1,0])[0]\n",
    "                    else:\n",
    "                        # if lc has a single element, take that one as lc, if not sample\n",
    "                        current_lc = self.lc.flatten()[0] if len(self.lc.flatten()) == 2 else rand_choice_nb(arr = self.lc[0], prob = self.lc[1])\n",
    "                    self.positions[agent_index][0] = self.target_positions[idx_first, 0] + current_lc*np.cos(kick_direction)\n",
    "                    self.positions[agent_index][1] = self.target_positions[idx_first, 1] + current_lc*np.sin(kick_direction)\n",
    "                self.kicked[agent_index] = 1\n",
    "                #------------\n",
    "                \n",
    "            #...and we add the information that this agent got to the target\n",
    "            self.current_rewards[agent_index] = 1              \n",
    "            return 1\n",
    "        \n",
    "        else: \n",
    "            self.kicked[agent_index] = 0\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "        \n",
    "    def check_bc(self):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of agent agent_index to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_index=0\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.numba.rl_framework import TargetEnv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TargetEnv(Nt = 1000,\n",
    "                      L = 123,\n",
    "                      r = 50,\n",
    "                      lc = np.array([[0.1],[1]]),\n",
    "                      lc_distribution = 'pareto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19 µs ± 7.96 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit env.check_encounter()\n",
    "# Runtime of env.check_encounter():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.rl_framework import TargetEnv as TargetEnv_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oenv = TargetEnv_classic(Nt = 100,\n",
    "                         L = 123,\n",
    "                         r = 0.2,\n",
    "                         lc = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 µs ± 211 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit oenv.check_encounter()\n",
    "# Runtime of oenv.check_encounter():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk from policy\n",
    "\n",
    "These replicate what we were doing in `rl_opts.learn_and_bench.walk_from_policy` and help get efficiencies for fixed policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def single_agent_walk(N_runs : int, # Total number of runs / episodes to evaluate\n",
    "                      time_ep : int, # Length of each run / episode\n",
    "                      policy : np.array, # Policy of the walker\n",
    "                      env : object# Environment where the walker moves\n",
    "                     )-> np.array :  # Array containing the number of targets from in each run\n",
    "\n",
    "    \"\"\"\n",
    "    Walk of a single in env of type TargetEnv given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_rewards = np.zeros(N_runs)\n",
    "    \n",
    "    for ep in range(N_runs):\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent_state = 0\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            \n",
    "            if t == 0 or env.kicked[0]:\n",
    "                # change direction\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                \n",
    "            else: \n",
    "                # decide\n",
    "                action = 0 if policy[0, agent_state] > np.random.rand() else 1\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                # update agent_state\n",
    "                agent_state += 1\n",
    "                \n",
    "                save_rewards[ep] += reward\n",
    "                \n",
    "    return save_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit(nopython = NOPYTHON, parallel = True)\n",
    "def multi_agents_walk(N_runs : int, # Total number of runs / episodes to evaluate\n",
    "                      time_ep : int, # Length of each run / episode\n",
    "                      N_agents : int, # Number of agents to consider\n",
    "                      Nt = 100, # Number of targets in the environment\n",
    "                      L = 100, # Size of the environment\n",
    "                      r = 0.5, # Radius of the targets\n",
    "                      lc = 1.0, # Parameters of lc distribution or lc itself \n",
    "                      agent_step = 1, # Length of agent's step\n",
    "                      destructive_targets = False, # True if targets are destructive. The default is False. \n",
    "                      lc_distribution = 'constant', # lc distribution\n",
    "                      policy = [[1,1], [0,0]] # Policy of the agents\n",
    "              )-> np.array : # Array containing number of targets found for each agent at each run.\n",
    "    \"\"\"\n",
    "    Runs in parallel single_agent_walk. Due to numba props, we need to give all parameters as inputs (see source).\n",
    "    \"\"\"\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, N_runs))\n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = TargetEnv(Nt,L,r,lc,agent_step,1,destructive_targets,lc_distribution)\n",
    "        \n",
    "        rews = single_agent_walk(N_runs, time_ep, policy, env) \n",
    "    \n",
    "        save_rewards[n_agent] = rews\n",
    "        \n",
    "    return save_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projective Simulation agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "## Base Forager\n",
    "Here we do the numba implementation of `rl_opts.rl_framework.Forager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@jitclass([(\"num_percepts_list\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           ])\n",
    "class _Forager_original():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    num_percepts_list : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_actions, # Number of actions\n",
    "                 state_space, \n",
    "                 # List where each entry is the state space of each perceptual feature. \n",
    "                 # In general we only consider one perceptual feature (counter)\n",
    "                 gamma_damping=0.0, # Gamma of PS\n",
    "                 eta_glow_damping=0.0, # Glow of PS\n",
    "                 policy_type='standard', # Sampling of policy\n",
    "                 beta_softmax=3, # Parameters if policy is softmax\n",
    "                 initial_prob_distr = np.array([[],[]]), # Initial h-matrix\n",
    "                 fixed_policy=np.array([[],[]]) # If considering a fixed policy\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Adapted to numba from rl_framework.Forager. This is an intermediate step to Forager with no efficient H and G updates.\n",
    "        To improve clarity we have changed num_percepts_list variable to state_space\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions       \n",
    "\n",
    "        \n",
    "        self.num_percepts_list = np.array([len(i) for i in state_space], dtype = np.int64) # change w.r.t PSAGENT\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy\n",
    "        \n",
    "        self.num_percepts = int(np.prod(self.num_percepts_list)) # total number of possible percepts\n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for which_feature in range(len(observation)):\n",
    "            percept += int(observation[which_feature] * np.prod(self.num_percepts_list[:which_feature]))\n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.probability_distr(percept))\n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept):\n",
    "        \"\"\"\n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept]\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept]\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No fixed policy was given to the agent. The action will be selected randomly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "#| exec : false\n",
    "agent = _Forager_original(num_actions = 2, state_space = np.array([np.arange(100)]))\n",
    "agent.percept_preprocess([0]*agent.num_percepts_list)\n",
    "agent.probability_distr(0)\n",
    "observation = [0]*agent.num_percepts_list[0]\n",
    "agent.deliberate(np.array(observation))\n",
    "agent.learn(1)\n",
    "agent.reset_g()\n",
    "agent.deliberate_fixed_policy(np.array(observation))\n",
    "agent.act(0)\n",
    "agent.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "\n",
    "## Forager with efficient H update\n",
    "We use the formula $H_{t+i} = (1-\\gamma)^i H_t + \\gamma H_0 \\sum_{j=1}^i(1-\\gamma)^{j-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "@jitclass([(\"num_percepts_list\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           (\"prefactor_1\", float64[:]),\n",
    "           (\"prefactor_2\", float64[:])\n",
    "          ])\n",
    "class _Forager_efficient_H():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    max_no_update : int\n",
    "    counter_upd : int\n",
    "    num_percepts_list : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    prefactor_1: np.array\n",
    "    prefactor_2: np.array\n",
    "    \n",
    "    def __init__(self, num_actions, \n",
    "                 state_space, \n",
    "                 gamma_damping=0.0, \n",
    "                 eta_glow_damping=0.0, \n",
    "                 policy_type='standard', \n",
    "                 beta_softmax=3, \n",
    "                 initial_prob_distr = np.array([[],[]]), \n",
    "                 fixed_policy=np.array([[],[]]),\n",
    "                 max_no_update = int(1e4)\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Improved agent from _Agent_original with efficient H update implemented\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.num_percepts_list = np.array([len(i) for i in state_space], dtype = np.int64) # change w.r.t PSAGENT\n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy\n",
    "        \n",
    "        self.num_percepts = int(np.prod(self.num_percepts_list)) # total number of possible percepts\n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "        self.max_no_update = max_no_update      \n",
    "        self.counter_upd = 0\n",
    "        self.prefactor_1 = (1-self.gamma_damping)**(np.arange(self.max_no_update+1)) \n",
    "        # This is the slow / easy to understand way of computing prefactor_2\n",
    "        # self.prefactor_2 = np.zeros(self.max_no_H_update+1)       \n",
    "        # for i in range(self.max_no_H_update):\n",
    "        #     self.prefactor_2[i+1] = self.gamma_damping*np.sum((1-self.gamma_damping)**np.arange(i+1))\n",
    "        # and this it the efficient way\n",
    "        sum_term = (1-self.gamma_damping)**np.arange(self.max_no_H_update+1)\n",
    "        self.prefactor_2 = self.gamma_damping*(np.cumsum(sum_term)-sum_term)\n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def _learn_post_reward(self, reward):\n",
    "        if self.counter_upd == 0:\n",
    "            print('Counter for h_matrix is zero, check that your are properly updating it!')\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix = self.prefactor_1[self.counter_upd ] * self.h_matrix + self.prefactor_2[self.counter_upd] * self.h_0 + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix = self.prefactor_1[self.counter_upd ] * self.h_matrix + self.prefactor_2[self.counter_upd] + reward * self.g_matrix\n",
    "        self.counter_upd = 0\n",
    "        \n",
    "    def _hmat_upd_single_percept(self, t, percept):\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] * self.h_0[:, percept]\n",
    "        else:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] \n",
    "            \n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for which_feature in range(len(observation)):\n",
    "            percept += int(observation[which_feature] * np.prod(self.num_percepts_list[:which_feature]))\n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        \n",
    "        # Probabilities must be of update h_matrix. We feed the prob distr the update h_matrix\n",
    "        # for the percept, but don't update the h_matrix\n",
    "        current_h_mat = self._hmat_upd_single_percept(self.counter_upd, percept)\n",
    "        probs = self.probability_distr(percept, h_matrix = current_h_mat)\n",
    "        \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = probs)\n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept, h_matrix = None):\n",
    "        \"\"\"\n",
    "        UPDATE (added the optional input)\n",
    "         \n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_train_loop_Heff(efficient, agent, episodes):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        \n",
    "        if efficient:\n",
    "            agent.counter_upd += 1\n",
    "        \n",
    "        state = np.array([i])\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            action = 0\n",
    "        else: 1\n",
    "        \n",
    "        # here is where glow matrix updates:\n",
    "        agent.g_matrix = (1 - agent.eta_glow_damping) * agent.g_matrix\n",
    "        agent.g_matrix[action, i] += 1 #record latest decision in g_matrix\n",
    "        \n",
    "        if i == 2 or i == 6:\n",
    "            reward = 1\n",
    "        else: reward = 0\n",
    "        \n",
    "        if efficient:\n",
    "            if reward == 1:\n",
    "                agent._learn_post_reward(reward)\n",
    "                agent.counter_upd = 0\n",
    "        else:\n",
    "            agent.learn(reward)\n",
    "\n",
    "    if efficient:\n",
    "        agent._learn_post_reward(reward)\n",
    "            \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Value testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from rl_opts.rl_framework import Forager as Forager_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "eps = 10\n",
    "gamma_damping = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.90236435, 2.90236435, 2.90236435, 1.970299  , 1.970299  ,\n",
       "        1.970299  , 1.970299  , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(eps)]),\n",
    "                              gamma_damping = gamma_damping)\n",
    "\n",
    "trained_noopt = test_train_loop_Heff(efficient = False, agent = agent_noopt, episodes = eps)\n",
    "trained_noopt.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.90236435, 2.90236435, 2.90236435, 1.970299  , 1.970299  ,\n",
       "        1.970299  , 1.970299  , 1.        , 1.        , 1.        ],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_opt = _Forager_efficient_H(num_actions = 2,\n",
    "                                 state_space = np.array([np.arange(eps)]),\n",
    "                                 gamma_damping = gamma_damping)\n",
    "\n",
    "trained = test_train_loop_Heff(efficient=True, agent = agent_opt, episodes = eps)\n",
    "trained.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comparison old and efficient: -7.438494264988549e-15 ||||| IF value != 0, something is wrong!!!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "f'comparison old and efficient: {(trained.h_matrix-trained_noopt.h_matrix).sum()} ||||| IF value != 0, something is wrong!!!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "Manual testing: we define an h-matrix and let it damp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "eps = 10\n",
    "len_hmat = 5\n",
    "gamma_damping = 0.001\n",
    "rand_h = [[2.0,2,4,5,1],\n",
    "          [3,3,1,1,1]]\n",
    "rand_h = np.array(rand_h)\n",
    "\n",
    "\n",
    "#np.random.rand(2, len_hmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(len_hmat)]),\n",
    "                              gamma_damping = gamma_damping)\n",
    "agent_noopt.h_matrix = rand_h.copy()\n",
    "\n",
    "for e in range(eps):\n",
    "    agent_noopt.learn(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99004488, 1.99004488, 3.97013464, 4.96017952, 1.        ],\n",
       "       [2.98008976, 2.98008976, 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_noopt.h_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "agent_opt = _Forager_efficient_H(num_actions = 2,\n",
    "                                 state_space = np.array([np.arange(len_hmat)]),\n",
    "                                 gamma_damping = gamma_damping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99004488, 1.99004488, 3.97013464, 4.96017952, 1.        ],\n",
       "       [2.98008976, 2.98008976, 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "agent_opt.h_matrix = rand_h.copy()\n",
    "\n",
    "agent_opt.counter_upd = 10\n",
    "agent_opt._learn_post_reward(0)\n",
    "agent_opt.h_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forager \n",
    "This agent is an efficient version of rl_opts.rl_framework.Forager with:\n",
    "\n",
    "- `numba` implementation\n",
    "-  H and G matrix efficient updates (contribution by Michele Caraglio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@jitclass([(\"size_state_space\", int64[:]),           \n",
    "           (\"initial_prob_distr\", float64[:,:]),           \n",
    "           (\"fixed_policy\", float64[:,:]) ,\n",
    "           (\"h_matrix\", float64[:,:]) ,\n",
    "           (\"g_matrix\", float64[:,:]) ,\n",
    "           (\"h_0\", float64[:,:]),\n",
    "           (\"prefactor_1\", float64[:]),\n",
    "           (\"prefactor_2\", float64[:]),\n",
    "           (\"last_upd_G\", float64[:,:])\n",
    "          ])\n",
    "class Forager():\n",
    "    num_actions : int\n",
    "    gamma_damping : float\n",
    "    eta_glow_damping : float\n",
    "    policy_type : str\n",
    "    beta_softmax : float\n",
    "    num_percepts : int\n",
    "    agent_state : int\n",
    "    size_state_space : np.array\n",
    "    initial_prob_distr : np.array\n",
    "    fixed_policy : np.array    \n",
    "    h_matrix : np.array\n",
    "    g_matrix : np.array\n",
    "    h_0 : np.array\n",
    "    # Efficient H update\n",
    "    prefactor_1: np.array\n",
    "    prefactor_2: np.array\n",
    "    max_no_H_update : int\n",
    "    N_upd_H : int\n",
    "    # Efficient G update\n",
    "    last_upd_G: np.array\n",
    "    N_upd_G: int\n",
    "    \n",
    "    def __init__(self, num_actions, # Number of actions\n",
    "                 size_state_space, \n",
    "                 # List where each entry is the state space of each perceptual feature. \n",
    "                 # In general we only consider one perceptual feature (counter)\n",
    "                 gamma_damping=0.0, # Gamma of PS\n",
    "                 eta_glow_damping=0.0, # Glow of PS\n",
    "                 policy_type='standard', # Sampling of policy\n",
    "                 beta_softmax=3, # Parameters if policy is softmax\n",
    "                 initial_prob_distr = np.array([[],[]]), # Initial h-matrix\n",
    "                 fixed_policy=np.array([[],[]]), # If considering a fixed policy\n",
    "                 max_no_H_update = int(1e4) # maximum number of steps before an update of H and G matrices.\n",
    "                ):\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Updated version of the `rl_framework.Forager` class, with an efficient update both for the H-matrix and the G-matrix.\n",
    "\n",
    "        **Inputs**\n",
    "        \n",
    "        `num_actions` : \n",
    "            Number of actions\n",
    "            \n",
    "        `size_state_space` : \n",
    "             List where each entry is the state space of each perceptual feature. In general we only consider one perceptual feature (counter)\n",
    "        \n",
    "        `gamma_damping` :\n",
    "            Gamma of PS\n",
    "        \n",
    "        `eta_glow_damping` :\n",
    "            Glow of PS\n",
    "        \n",
    "        `policy_type` :\n",
    "            Sampling of policy\n",
    "        \n",
    "        `beta_softmax` :\n",
    "            Parameters if policy is softmax\n",
    "        \n",
    "        `initial_prob_distr` :\n",
    "            Initial h-matrix\n",
    "        \n",
    "        `fixed_policy` :\n",
    "            If considering a fixed policy\n",
    "        \n",
    "        `max_no_H_update` :\n",
    "            Maximum number of steps before an update of H and G matrices.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.agent_state = 0\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.size_state_space = size_state_space\n",
    "        self.num_percepts = int(np.prod(self.size_state_space)) # total number of possible percepts\n",
    "        \n",
    "        self.gamma_damping = gamma_damping\n",
    "        self.eta_glow_damping = eta_glow_damping\n",
    "        self.policy_type = policy_type\n",
    "        self.beta_softmax = beta_softmax\n",
    "        self.initial_prob_distr = initial_prob_distr\n",
    "        self.fixed_policy = fixed_policy       \n",
    "        \n",
    "        self.init_matrices()\n",
    "        \n",
    "        # # For H update\n",
    "        self.max_no_H_update = max_no_H_update      \n",
    "        self.N_upd_H = 0\n",
    "        self.prefactor_1 = (1-self.gamma_damping)**(np.arange(self.max_no_H_update+1)) \n",
    "\n",
    "        # This is the slow / easy to understand way of computing prefactor_2\n",
    "        # self.prefactor_2 = np.zeros(self.max_no_H_update+1)       \n",
    "        # for i in range(self.max_no_H_update):\n",
    "        #     self.prefactor_2[i+1] = self.gamma_damping*np.sum((1-self.gamma_damping)**np.arange(i+1))\n",
    "        # and this it the efficient way\n",
    "        sum_term = (1-self.gamma_damping)**np.arange(self.max_no_H_update+1)\n",
    "        self.prefactor_2 = self.gamma_damping*(np.cumsum(sum_term)-sum_term)\n",
    "        \n",
    "        \n",
    "            \n",
    "        # For G update\n",
    "        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\n",
    "        self.N_upd_G = 0\n",
    "                              \n",
    "        \n",
    "    def init_matrices(self):\n",
    "\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts)) #glow matrix, for processing delayed rewards\n",
    "\n",
    "        # initialize h matrix with different values\n",
    "        if len(self.initial_prob_distr[0]) > 0:          \n",
    "            self.h_0 = self.initial_prob_distr\n",
    "            self.h_matrix = self.h_0.copy()\n",
    "        else: \n",
    "            self.h_matrix = np.ones((self.num_actions, self.num_percepts), dtype=np.float64) #Note: the first index specifies the action, the second index specifies the percept.\n",
    "            \n",
    "    def _learn_post_reward(self, reward):\n",
    "        '''Given a reward, updates the whole H-matrix taking into account that we did not have updates\n",
    "        for the last N_upd_H steps.'''\n",
    "        # Update the full G matrix\n",
    "        self._G_upd_full()\n",
    "        \n",
    "        if self.N_upd_H == 0:\n",
    "            print('Counter for h_matrix is zero, check that your are properly updating it!')\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix = self.prefactor_1[self.N_upd_H ] * self.h_matrix + self.prefactor_2[self.N_upd_H] * self.h_0 + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix = self.prefactor_1[self.N_upd_H ] * self.h_matrix + self.prefactor_2[self.N_upd_H] + reward * self.g_matrix\n",
    "        self.N_upd_H = 0\n",
    "        \n",
    "    def _H_upd_single_percept(self, t, percept):\n",
    "        '''Given a percept and the time t passed since the last H-matrix update,\n",
    "        returns the corresponding --updated-- column of the H-matrix for all actions.\n",
    "        This updated is local and does no affect the H-matrix.'''\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] * self.h_0[:, percept]\n",
    "        else:\n",
    "            return self.prefactor_1[t] * self.h_matrix[:, percept] + self.prefactor_2[t] \n",
    "        \n",
    "    def _G_upd_single_percept(self, percept, action):\n",
    "        '''Given a percept-action tuple, updates that element of the G-matrix. Updates the last_upd_G\n",
    "        to keep track of when was the matrix updated.'''\n",
    "        # For the current (a,s) tuple, we damp and sum one\n",
    "        self.g_matrix[action, percept] = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G[action, percept])*self.g_matrix[action, percept] + 1\n",
    "        # Then update the last_upd matrix\n",
    "        self.last_upd_G[action, percept] = self.N_upd_G\n",
    "        \n",
    "    def _G_upd_full(self):\n",
    "        '''Given the current number of steps without an update, updates the whole G-matrix.\n",
    "        Then, resets all counters.'''\n",
    "        self.g_matrix = (1 - self.eta_glow_damping)**(self.N_upd_G - self.last_upd_G) * self.g_matrix\n",
    "        self.N_upd_G = 0\n",
    "        self.last_upd_G = np.zeros((self.num_actions, self.num_percepts))\n",
    "            \n",
    "            \n",
    "    def percept_preprocess(self, observation):\n",
    "        \"\"\"\n",
    "        Takes a multi-feature percept and reduces it to a single integer index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : ARRAY of integers >=0, of the same length as self.num_percepts_list\n",
    "            List that describes the observation. Each entry is the value that each feature takes in the observation.\n",
    "            observation[i] < num_percepts_list[i] (strictly)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percept : int\n",
    "            Percept index that corresponds to the input observation.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        percept = 0\n",
    "        for idx_obs, obs_feature in enumerate(observation):\n",
    "            percept += int(obs_feature * np.prod(self.size_state_space[:idx_obs]))  \n",
    "        return percept\n",
    "    \n",
    "    def deliberate(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action and records that choice in the g_matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "        action : optional, bool\n",
    "            Mostly for debugging, we can input the action and no deliberation takes place, but g_matrix is updated\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        \n",
    "        \n",
    "        # Probabilities must be of update h_matrix. We feed the prob distr the update h_matrix\n",
    "        # for the percept, but don't update the h_matrix\n",
    "        current_h_mat = self._H_upd_single_percept(self.N_upd_H, percept)\n",
    "        probs = self.probability_distr(percept, h_matrix = current_h_mat)        \n",
    "        action = rand_choice_nb(arr = np.arange(self.num_actions), prob = probs)\n",
    "        \n",
    "        # Update the G matrix for current (s,a) tuple\n",
    "        self._G_upd_single_percept(percept, action)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def probability_distr(self, percept, h_matrix = None):\n",
    "        \"\"\"\n",
    "        UPDATE (added the optional input)\n",
    "         \n",
    "        Given a percept index, this method returns a probability distribution over actions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        percept : int\n",
    "            Index of the given percept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probability_distr : np.array, length = num_actions\n",
    "            Probability for each action (normalized to unit sum), computed according to policy_type.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.policy_type == 'standard':\n",
    "            h_vector = self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            probability_distr = h_vector / np.sum(h_vector)\n",
    "        elif self.policy_type == 'softmax':\n",
    "            h_vector = self.beta_softmax * self.h_matrix[:, percept] if h_matrix is None else h_matrix\n",
    "            h_vector_mod = h_vector - np.max(h_vector)\n",
    "            probability_distr = np.exp(h_vector_mod) / np.sum(np.exp(h_vector_mod))\n",
    "        return probability_distr\n",
    "    \n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, this method updates the h matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Value of the obtained reward.\n",
    "        \"\"\"\n",
    "        if len(self.initial_prob_distr[0]) > 0:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - self.h_0) + reward * self.g_matrix\n",
    "        else:\n",
    "            self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + reward * self.g_matrix\n",
    "            \n",
    "    def reset_g(self):\n",
    "        \"\"\"\n",
    "        Resets the g_matrix.\n",
    "        \"\"\"\n",
    "        self.g_matrix = np.zeros((self.num_actions, self.num_percepts), dtype=np.float64)\n",
    "        \n",
    "    def deliberate_fixed_policy(self, observation):\n",
    "        \"\"\"\n",
    "        Given an observation , this method chooses the next action according to the fixed policy specified as attribute of the class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : list\n",
    "            List that describes the observation, as specified in percept_preprocess.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            Index of the chosen action.\n",
    "\n",
    "        \"\"\"\n",
    "        percept = self.percept_preprocess(observation) \n",
    "        if len(self.fixed_policy[0]) > 0:\n",
    "            action = rand_choice_nb(arr = np.arange(self.num_actions), prob = self.fixed_policy[percept])\n",
    "        else:\n",
    "            print('No fixed policy was given to the agent. The action will be selected randomly.')\n",
    "            action = np.random.choice(self.num_actions)\n",
    "    \n",
    "        self.g_matrix = (1 - self.eta_glow_damping) * self.g_matrix\n",
    "        self.g_matrix[action, percept] += 1 #record latest decision in g_matrix\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        Agent performs the given action.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int (0, 1)\n",
    "            1 if it changes direction, 0 otherwise\n",
    "        \"\"\"\n",
    "        \n",
    "        # If the agent changes direction   \n",
    "        if action == 1:\n",
    "            self.agent_state = 0\n",
    "        else:\n",
    "            self.agent_state += 1  \n",
    "            \n",
    "    \n",
    "    def get_state(self):  \n",
    "        ''' simplified to case of single forager. Returns list because is what deliberate needs'''\n",
    "        return np.array([self.agent_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# @jit\n",
    "def test_train_loop(efficient, agent, episodes):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        \n",
    "        if efficient:\n",
    "            agent.N_upd_H += 1\n",
    "            agent.N_upd_G += 1\n",
    "        \n",
    "        state = np.array([i])\n",
    "        \n",
    "        # Do determinist action to keep track\n",
    "        if i % 2 == 0:\n",
    "            action = 0\n",
    "        else: action = 1       \n",
    "        \n",
    "        # Because deterministic action, do update of G matrix by hand copying what is in agent.deliberate\n",
    "        agent.deliberate(state)\n",
    "        \n",
    "        if i == 2 or i == 6:\n",
    "            reward = 1\n",
    "        else: reward = 0\n",
    "        \n",
    "        if efficient:\n",
    "            if reward == 1:\n",
    "                agent._learn_post_reward(reward)\n",
    "        else:\n",
    "            agent.learn(reward)\n",
    "    \n",
    "    if efficient:\n",
    "        agent._learn_post_reward(reward)\n",
    "\n",
    "    \n",
    "            \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value testing**\n",
    "\n",
    "Because actions are random, the row of the value will change. But in each column, you should have consistent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.rl_framework import Forager as Forager_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.91, 0.  , 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.  , 1.  ],\n",
       "        [0.  , 0.92, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  ]]),\n",
       " array([[2.91192, 1.     , 2.95074, 1.96739, 1.97716, 1.98703, 1.997  ,\n",
       "         1.     , 1.     , 1.     ],\n",
       "        [1.     , 2.93123, 1.     , 1.     , 1.     , 1.     , 1.     ,\n",
       "         1.     , 1.     , 1.     ]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 10\n",
    "agent_noopt = Forager_classic(num_actions = 2,\n",
    "                              state_space = np.array([np.arange(eps)]), eta_glow_damping = 0.01, gamma_damping = 0.001)\n",
    "trained_noopt = test_train_loop(efficient = False, agent = agent_noopt, episodes = eps)\n",
    "trained_noopt.g_matrix.round(2), trained_noopt.h_matrix.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.  , 0.92, 0.  , 0.  , 0.  , 0.96, 0.  , 0.98, 0.99, 0.  ],\n",
       "        [0.91, 0.  , 0.93, 0.94, 0.95, 0.  , 0.97, 0.  , 0.  , 1.  ]]),\n",
       " array([[1.     , 2.93123, 1.     , 1.     , 1.     , 1.98703, 1.     ,\n",
       "         1.     , 1.     , 1.     ],\n",
       "        [2.91192, 1.     , 2.95074, 1.96739, 1.97716, 1.     , 1.997  ,\n",
       "         1.     , 1.     , 1.     ]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_opt = Forager(num_actions = 2,\n",
    "                    size_state_space = np.array([eps]), eta_glow_damping = 0.01, gamma_damping = 0.001)\n",
    "trained = test_train_loop(efficient=True, agent = agent_opt, episodes = eps)\n",
    "\n",
    "trained.g_matrix.round(2), trained.h_matrix.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runtime testing only H vs. full efficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = int(1e4); eta = 0.1\n",
    "agent_noopt = _Forager_original(num_actions = 2,\n",
    "                                state_space = np.array([np.arange(eps)]), \n",
    "                                eta_glow_damping = eta)\n",
    "agent_opt = Forager(num_actions = 2,\n",
    "                    size_state_space = np.array([eps]), \n",
    "                    eta_glow_damping = eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.6 ms ± 122 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_train_loop(efficient=True, agent = agent_opt, episodes = eps)\n",
    "# Runtime new Forager (efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 ms ± 32.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test_train_loop(efficient=False, agent = agent_noopt, episodes = eps)\n",
    "# Runtime old Forager "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch multi agent learning\n",
    "We now make use of the parallel option of `numba` to create launchers where many agents are trained at the same time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def _train_loop_original(episodes, # Number of episodes to train\n",
    "               time_ep, # Length of episode\n",
    "               agent, # Agent class\n",
    "               env # Environment class\n",
    "              ): # Rewards in each episode and time step and h_matrix of the trained agent \n",
    "    '''\n",
    "    Training loop for _Forager_original.\n",
    "    '''\n",
    "    \n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "\n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "\n",
    "                agent.learn(reward)                \n",
    "                    \n",
    "            save_rewards[ep, t] = reward\n",
    "      \n",
    "    \n",
    "    return save_rewards, agent.h_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def _train_loop_h_efficient(episodes, time_ep, agent, env, h_mat_allT = False):  \n",
    "    '''\n",
    "    Training loop for _Forager_h_efficient.\n",
    "    '''\n",
    "\n",
    "    if h_mat_allT: policy_t = np.zeros((episodes, agent.h_matrix.shape[-1]))\n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            agent.counter_upd += 1\n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "                if reward == 1 or agent.counter_upd > agent.max_no_update:\n",
    "                    agent._learn_post_reward(reward)\n",
    "                    \n",
    "            # Saving\n",
    "            save_rewards[ep, t] = reward\n",
    "            if h_mat_allT: policy_t[ep] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n",
    "      \n",
    "    return (save_rewards, policy_t) if h_mat_allT else (save_rewards, agent.h_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@jit(nopython = NOPYTHON)\n",
    "def train_loop(episodes : int, # Number of episodes to train\n",
    "               time_ep : int, # Length of episode\n",
    "               agent : object, # Agent class\n",
    "               env : object, # Environment class\n",
    "               h_mat_allT : bool = False # If True, returns the h_matrix at all times\n",
    "              )-> tuple: # Rewards and h-matrix of the trained agent      \n",
    "    '''    \n",
    "    Given an agent and environment, performs a loop train for the `TargetEnv` type of environment, by adequatly\n",
    "    updating the H and G counters, considering boundaries, etc... \n",
    "    '''\n",
    "\n",
    "    if h_mat_allT: policy_t = np.zeros((episodes, agent.h_matrix.shape[-1]))\n",
    "    save_rewards = np.zeros((episodes, time_ep))\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # print(f'starting episode {ep} for agent {n_agent}')\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent.agent_state = 0\n",
    "        agent.reset_g()\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            agent.N_upd_H += 1\n",
    "            agent.N_upd_G += 1\n",
    "            \n",
    "            #step to set counter to its min value n=1\n",
    "            if t == 0 or env.kicked[0]:\n",
    "                #do one step with random direction (no learning in this step)\n",
    "                env.update_pos(1)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent.agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                #get perception\n",
    "                state = agent.get_state()\n",
    "                #decide\n",
    "                action = agent.deliberate(state)\n",
    "                #act (update counter)\n",
    "                agent.act(action)\n",
    "\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #learn\n",
    "                if reward == 1:\n",
    "                    agent._learn_post_reward(reward)\n",
    "\n",
    "            if agent.N_upd_H == agent.max_no_H_update-1:\n",
    "                agent._learn_post_reward(reward)\n",
    "\n",
    "            # Saving\n",
    "            save_rewards[ep, t] = reward\n",
    "            \n",
    "        if h_mat_allT: policy_t[ep] = agent.h_matrix[0,:] / agent.h_matrix.sum(0)\n",
    "      \n",
    "    return (save_rewards, policy_t) if h_mat_allT else (save_rewards, agent.h_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TargetEnv(Nt = 100,\n",
    "               L = 100, \n",
    "               r = 0.5, \n",
    "               lc = np.array([[1.0],[1]]), \n",
    "               agent_step = 1, \n",
    "               destructive = False, \n",
    "               lc_distribution = 'constant')\n",
    "\n",
    "\n",
    "agent = Forager(num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "               size_state_space = np.array([100]))\n",
    "\n",
    "agent_nopt = _Forager_original(num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "                     state_space = np.array([np.arange(100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.7 ms ± 18.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _train_loop_original(episodes = 100, time_ep = 100, agent = agent_nopt, env = env)\n",
    "# Runtime without h-matrix efficient update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 ms ± 57.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_loop(episodes = 100, time_ep = 100, agent = agent, env = env)\n",
    "# Runtime with h-matrix efficient update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiple agents in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit(nopython = NOPYTHON, parallel = True)\n",
    "def run_agents(episodes, # Number of episodes\n",
    "               time_ep, # Length of episode\n",
    "               N_agents, # Number of agents               \n",
    "               h_mat_allT = False, # If to save the h_matrix at all times\n",
    "               efficient_agent = True, # If to consider efficient H and G agent (other options: 'only H' or False)\n",
    "               Nt = 100, # From here are props of the environment (see TargetEnv for details) \n",
    "               L = 100, \n",
    "               r = 0.5, \n",
    "               lc = np.array([[1.0],[1]]), \n",
    "               agent_step = 1, \n",
    "               destructive_targets = False, \n",
    "               lc_distribution = 'constant', \n",
    "               num_actions = 2, # From here are props of the agent (see Forager for details)\n",
    "               size_state_space = np.array([100]), \n",
    "               gamma_damping = 0.00001,\n",
    "               eta_glow_damping = 0.1,\n",
    "               initial_prob_distr = np.array([[],[]]),\n",
    "               policy_type = 'standard', \n",
    "               beta_softmax = 3,  \n",
    "               fixed_policy = np.array([[],[]]),\n",
    "               max_no_H_update = int(1e3) \n",
    "              ):\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, episodes))\n",
    "    if h_mat_allT:\n",
    "        save_h_matrix = np.zeros((N_agents, episodes, size_state_space[0]))  \n",
    "    else:        \n",
    "        save_h_matrix = np.zeros((N_agents, 2, size_state_space[0])) \n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = TargetEnv(Nt,L,r,lc,agent_step,1,destructive_targets,lc_distribution)\n",
    "\n",
    "\n",
    "        if efficient_agent == True: # Both G and H efficient update\n",
    "            agent = Forager(num_actions,size_state_space,gamma_damping,\n",
    "                            eta_glow_damping,policy_type,beta_softmax,\n",
    "                            initial_prob_distr,fixed_policy,max_no_H_update)\n",
    "            rews, mat = train_loop(episodes, time_ep, agent, env, h_mat_allT) \n",
    "        \n",
    "        elif efficient_agent == 'only H': # Only H efficient update\n",
    "            state_space = np.arange(size_state_space[0]).reshape(1, size_state_space[0])\n",
    "            agent = _Forager_efficient_H(num_actions,state_space,gamma_damping,\n",
    "                                        eta_glow_damping,policy_type,beta_softmax,\n",
    "                                        initial_prob_distr,fixed_policy,max_no_H_update)\n",
    "            rews, mat = _train_loop_h_efficient(episodes, time_ep, agent, env, h_mat_allT)  \n",
    "            \n",
    "        elif efficient_agent == False: # Old version without efficient updates            \n",
    "            state_space = np.arange(size_state_space[0]).reshape(1, size_state_space[0])\n",
    "            agent = _Forager_original(num_actions,state_space,gamma_damping,\n",
    "                            eta_glow_damping,policy_type,beta_softmax,\n",
    "                            initial_prob_distr,fixed_policy)\n",
    "            rews, mat = _train_loop_original(episodes, time_ep, agent, env)    \n",
    "    \n",
    "        for t in range(episodes):\n",
    "            save_rewards[n_agent, t] = np.mean(rews[t])\n",
    "        save_h_matrix[n_agent] = mat\n",
    "        \n",
    "    return save_rewards, save_h_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Runtime testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gorka/miniconda3/envs/rl_opts_main/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# For compiling and checking\n",
    "time_ep = 12000\n",
    "run_agents(episodes = 10, time_ep = time_ep, N_agents = 5, efficient_agent=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.26 s ± 35.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent=False)\n",
    "# Runtime original in numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 s ± 11.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit k = run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent='only H')\n",
    "# Runtime original with only H efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.66 s ± 20.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit k = run_agents(episodes = 100, time_ep = time_ep, N_agents = 10, size_state_space = np.array([100]), efficient_agent=True)\n",
    "# Runtime original with full H and G efficient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_opts_main",
   "language": "python",
   "name": "rl_opts_main"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
