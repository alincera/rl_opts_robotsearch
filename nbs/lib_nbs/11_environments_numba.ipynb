{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432b43b7-63c8-4360-83c9-098856e14cdf",
   "metadata": {},
   "source": [
    "# Reinforcement learning environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936db82-2862-41d4-83ea-6688b945127d",
   "metadata": {},
   "source": [
    "This notebook gathers the functions creating different kinds of environments for foraging and target search in various scenarios, adapted for their use in the reinforcement learning paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661f884-5d59-469a-90a5-c65f6f72a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rl_framework.numba.environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdde34f-4614-4fad-acc9-b49ef769db41",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc944947-aef5-4f8f-bc02-54b4abda36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba.experimental import jitclass\n",
    "from numba import float64, bool_, prange, njit\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e505d-55fb-4644-8f6e-639c3c7dca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5b6cb-6385-4c84-be00-cbace1d00d0c",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57c4a6-ebf8-4cb0-9395-a9a707f67415",
   "metadata": {},
   "source": [
    "## isBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36567cb6-7e16-4860-bd57-c43be26fa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def isBetween_c_Vec_numba(a, b, c, r):\n",
    "        \"\"\"\n",
    "        Checks whether point c is crossing the line formed with point a and b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : tensor, shape = (1,2)\n",
    "            Previous position.\n",
    "        b : tensor, shape = (1,2)\n",
    "            Current position.\n",
    "        c : tensor, shape = (Nt,2)\n",
    "            Positions of all targets.\n",
    "        r : int/float\n",
    "            Target radius.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask : array of boolean values\n",
    "            True at the indices of found targets.\n",
    "\n",
    "        \"\"\"\n",
    "        if (a == b).all():\n",
    "            return np.array([False]*c.shape[0])\n",
    "\n",
    "        mask = np.array([True]*c.shape[0])\n",
    "        \n",
    "        dotproduct = (c[:, 0] - a[0]) * (b[0] - a[0]) + (c[:, 1] - a[1])*(b[1] - a[1])\n",
    "        squaredlengthba = (b[0] - a[0])*(b[0] - a[0]) + (b[1] - a[1])*(b[1] - a[1])\n",
    "        \n",
    "        #exclude the targets whose vertical projection of the vector c-a w.r.t. the vector b-a is larger than the target radius.\n",
    "        idx = np.argwhere(np.abs(numba.np.arraymath.cross2d(b-a, c-a))/np.linalg.norm(b-a) > r) \n",
    "        for i1 in idx:\n",
    "            mask[i1] = False        \n",
    "        \n",
    "        #exclude the targets whose scalar product is negative (they are on the other side of the step direction)\n",
    "        for i2 in np.argwhere(dotproduct < 0):\n",
    "            mask[i2] = False\n",
    "\n",
    "        #exclude the targets that are beyond the step.\n",
    "        for i3 in np.argwhere(dotproduct > squaredlengthba):\n",
    "            mask[i3] = False\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b378d-3038-4805-81f5-310571527bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiling = isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85311de9-5c87-4128-b2b6-aa6b139fb829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.65 μs ± 25.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd700d0-2844-454c-a0ce-ab2a147c5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.utils import isBetween_c_Vec as oldbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f346fb-44b9-4d7f-8273-e2f9b71faca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.4 μs ± 177 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit oldbetween(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1ecda-8072-4072-9d0f-3eb8689a3f28",
   "metadata": {},
   "source": [
    "## Pareto sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023cf70-3639-4629-9ab1-76eb94bd20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def pareto_sample(alpha, xm, size=1):\n",
    "    samples = np.zeros(size)\n",
    "    for ii in range(size):\n",
    "        u = random.random()  # Uniform random variable between 0 and 1\n",
    "        x = xm / (u ** (1 / alpha))\n",
    "        samples[ii] = x\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099a4b2-2b60-4adb-bc60-beca05387032",
   "metadata": {},
   "source": [
    "## Random sampling from array with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5fee3-fac9-42fe-88a1-b12b0f4493be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@njit\n",
    "def rand_choice_nb(arr, prob):\n",
    "    \"\"\"\n",
    "    :param arr: A 1D numpy array of values to sample from.\n",
    "    :param prob: A 1D numpy array of probabilities for the given samples.\n",
    "    :return: A random sample from the given array with a given probability.\n",
    "    \"\"\"\n",
    "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0370360-425e-4698-9383-c1ff4461ddb5",
   "metadata": {},
   "source": [
    "# TargetEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f8518-7596-4126-ae87-9d48e4dbfbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"kicked\", float64[:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"lc\", float64[:,:]),\n",
    "           (\"mask\", bool_[:]),\n",
    "           (\"first_encounter\", float64[:,:])])\n",
    "class TargetEnv():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    lc : np.array\n",
    "    agent_step : float\n",
    "    num_agents : int\n",
    "    destructive_targets : bool\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    kicked : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    first_encounter : np.array\n",
    "    lc_distribution : str\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt = 10, # Number of targets.\n",
    "                 L = 1.3, #  Size of the (squared) world.\n",
    "                 r = 1.5, # Radius with center the target position. It defines the area in which agent detects the target.\n",
    "                 lc = np.array([[1.0],[1]]), # Cutoff length. Displacement away from target \n",
    "                 agent_step = 1, # Displacement of one step. The default is 1.\n",
    "                 num_agents = 1, # Number of agents that forage at the same time. The default is 1. > 1 not fully implemented\n",
    "                 destructive = False, # True if targets are destructive. The default is False.\n",
    "                 lc_distribution = 'constant' # Distribution from where to sample l_c. Can be 'power_law', 'pareto' or something else. See comments self.check_encounter for explanations\n",
    "                ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Class defining the a Foraging environment with multiple targets and two actions: continue in \n",
    "        the same direction and turn by a random angle.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.lc = lc\n",
    "        self.agent_step = agent_step \n",
    "        self.num_agents = num_agents\n",
    "        self.destructive_targets = destructive\n",
    "        self.lc_distribution = lc_distribution\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #signal whether agent has been kicked\n",
    "        self.kicked = np.zeros(self.num_agents)\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, \n",
    "                   change_direction, # Whether the agent decided to turn or not.\n",
    "                   agent_index = 0 # Index of the given agent. The default is 0. This is only keeped for future devs\n",
    "                  ):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if change_direction:\n",
    "            self.current_directions[agent_index] = random.uniform(0,1)*2*math.pi\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "       \n",
    "    def check_encounter(self,\n",
    "                       agent_index=0 # Index of the given agent. The default is 0. This is only keeped for future devs\n",
    "                       ): # True if the agent found a target, else False\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "        \"\"\"       \n",
    "        \n",
    "        encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        \n",
    "        if sum(encounters) > 0: \n",
    "            \n",
    "            #if there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "            if self.destructive_targets:\n",
    "                self.target_positions[first_encounter] = np.random.rand(2)*self.L\n",
    "            else:\n",
    "                #----KICK----\n",
    "                # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                kick_direction = np.random.uniform(low = 0, high = 2*np.pi)\n",
    "                for idx_first in first_encounter: # This is super weird!\n",
    "                    if self.lc_distribution == 'power_law':\n",
    "                        # when we have the power law, the first value of lc is considered to be the exponent.\n",
    "                        # The following samples from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0]                        \n",
    "                        current_lc = (1-random.uniform(0,1))**(-1/self.lc.flatten()[0])\n",
    "\n",
    "                    elif self.lc_distribution == 'pareto':\n",
    "                        # Sampling from Pareto. Here alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "                        current_lc = pareto_sample(self.lc[0,0], self.lc[1,0])[0]\n",
    "                    else:\n",
    "                        # if lc has a single element, take that one as lc, if not sample\n",
    "                        current_lc = self.lc.flatten()[0] if len(self.lc.flatten()) == 2 else rand_choice_nb(arr = self.lc[0], prob = self.lc[1])\n",
    "                    self.positions[agent_index][0] = self.target_positions[idx_first, 0] + current_lc*np.cos(kick_direction)\n",
    "                    self.positions[agent_index][1] = self.target_positions[idx_first, 1] + current_lc*np.sin(kick_direction)\n",
    "                self.kicked[agent_index] = 1\n",
    "                #------------\n",
    "                \n",
    "            #...and we add the information that this agent got to the target\n",
    "            self.current_rewards[agent_index] = 1              \n",
    "            return 1\n",
    "        \n",
    "        else: \n",
    "            self.kicked[agent_index] = 0\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "        \n",
    "    def check_bc(self):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of agent agent_index to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_index=0\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875fda0-3264-47ea-b5b2-5c36c6964b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "env = TargetEnv(Nt = 1000,\n",
    "                 L = 123,\n",
    "                 r = 50,\n",
    "                 lc = np.array([[0.1],[1]]),\n",
    "                 lc_distribution = 'pareto')\n",
    "env.check_encounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad681096-c21e-426a-9e10-2fbeb42807b9",
   "metadata": {},
   "source": [
    "#|hide \n",
    "#### Runtime testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb45255-2aa0-457d-a2da-855ff55d283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide \n",
    "env = TargetEnv(Nt = 1000,\n",
    "                 L = 123,\n",
    "                 r = 50,\n",
    "                 lc = np.array([[0.1],[1]]),\n",
    "                 lc_distribution = 'pareto')\n",
    "compiling = env.check_encounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99353d94-43b0-4273-9d2e-3b930e10c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18 μs ± 13.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#|hide \n",
    "%timeit env.check_encounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401468f-a199-4eb8-b6b9-c9466e8145d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.7 μs ± 849 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gorka/github/rl_opts/rl_opts/utils.py:36: RuntimeWarning: invalid value encountered in divide\n",
      "  mask[np.argwhere(np.abs(np.cross(b-a, c-a))/np.linalg.norm(b-a) > r)] = False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#|hide \n",
    "#|eval: false\n",
    "\n",
    "# from rl_opts.rl_framework.numpy import TargetEnv as oldEnv\n",
    "\n",
    "oenv = oldEnv(Nt = 100,\n",
    "                 L = 123,\n",
    "                 r = 0.2,\n",
    "                 lc = 1)\n",
    "\n",
    "%timeit oenv.check_encounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b9adf-a6ec-4d9d-b749-2ee572c5248b",
   "metadata": {},
   "source": [
    "## Walk from policy (TargetEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915aa85-b0de-4e9b-80bc-f5dd5b6b5bc6",
   "metadata": {},
   "source": [
    "These replicate what we were doing in `rl_opts.learn_and_bench.walk_from_policy` and help get efficiencies for fixed policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81be96-2696-46fc-a8a1-c6bd426f09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@njit\n",
    "def single_agent_walk(N_runs : int, # Total number of runs / episodes to evaluate\n",
    "                      time_ep : int, # Length of each run / episode\n",
    "                      policy : np.array, # Policy of the walker\n",
    "                      env : object# Environment where the walker moves\n",
    "                     )-> np.array :  # Array containing the number of targets from in each run\n",
    "\n",
    "    \"\"\"\n",
    "    Walk of a single in env of type TargetEnv given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_rewards = np.zeros(N_runs)\n",
    "    \n",
    "    for ep in range(N_runs):\n",
    "        #initialize environment and agent's counter and g matrix\n",
    "        env.init_env()\n",
    "        agent_state = 0\n",
    "\n",
    "        for t in range(time_ep):\n",
    "            \n",
    "            if t == 0 or env.kicked[0]:\n",
    "                # change direction\n",
    "                action = rand_choice_nb(arr = np.arange(1, agent.num_actions), prob = np.array([1/len(np.arange(1,agent.num_actions))]*len(np.arange(1,agent.num_actions))))\n",
    "                env.update_pos(action)\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                #reset counter\n",
    "                agent_state = 0\n",
    "                #set kicked value to false again\n",
    "                env.kicked[0] = 0\n",
    "                \n",
    "            else: \n",
    "                # decide\n",
    "                action = 0 if policy[0, agent_state] > np.random.rand() else 1\n",
    "                #update positions\n",
    "                env.update_pos(action)\n",
    "                #check if target was found + kick if it is\n",
    "                reward = env.check_encounter()\n",
    "                #check boundary conditions\n",
    "                env.check_bc()\n",
    "                # update agent_state\n",
    "                agent_state += 1\n",
    "                \n",
    "                save_rewards[ep] += reward\n",
    "                \n",
    "    return save_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2eb1e-f741-4dd1-840f-08a621a141e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@njit(parallel = True)\n",
    "def multi_agents_walk(N_runs : int, # Total number of runs / episodes to evaluate\n",
    "                      time_ep : int, # Length of each run / episode\n",
    "                      N_agents : int, # Number of agents to consider\n",
    "                      Nt = 100, # Number of targets in the environment\n",
    "                      L = 100, # Size of the environment\n",
    "                      r = 0.5, # Radius of the targets\n",
    "                      lc = 1.0, # Parameters of lc distribution or lc itself \n",
    "                      agent_step = 1, # Length of agent's step\n",
    "                      destructive_targets = False, # True if targets are destructive. The default is False. \n",
    "                      lc_distribution = 'constant', # lc distribution\n",
    "                      policy = [[1,1], [0,0]] # Policy of the agents\n",
    "              )-> np.array : # Array containing number of targets found for each agent at each run.\n",
    "    \"\"\"\n",
    "    Runs in parallel single_agent_walk. Due to numba props, we need to give all parameters as inputs (see source).\n",
    "    \"\"\"\n",
    "    \n",
    "    save_rewards = np.zeros((N_agents, N_runs))\n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = TargetEnv(Nt,L,r,lc,agent_step,1,destructive_targets,lc_distribution)\n",
    "        \n",
    "        rews = single_agent_walk(N_runs, time_ep, policy, env) \n",
    "    \n",
    "        save_rewards[n_agent] = rews\n",
    "        \n",
    "    return save_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32539df0-f46b-4ac8-b944-8faae5679b37",
   "metadata": {},
   "source": [
    "# ResetEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a55ce",
   "metadata": {},
   "source": [
    "### Search loop with fixed policy an arbitrary environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a38073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "@njit\n",
    "def reset_search_loop(T, # Number of steps \n",
    "                      reset_policy, # Reset policy\n",
    "                      env # Environment\n",
    "                      ):\n",
    "    '''\n",
    "    Loop that runs the reset environment with a given reset policy.\n",
    "    '''\n",
    "    \n",
    "    rewards = 0\n",
    "    tau = 0 # time since last reset\n",
    "    \n",
    "    for t in range(T):\n",
    "        \n",
    "        action = 0 if np.random.rand() > reset_policy[tau] else 1\n",
    "        rew = env.update_pos(action = action)\n",
    "        \n",
    "        if rew == 1 or action == 1:\n",
    "            tau = 0\n",
    "        else:\n",
    "            tau += 1\n",
    "        \n",
    "        rewards += rew\n",
    "    \n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf9375-7e97-43fe-9f91-0bff65d7767d",
   "metadata": {},
   "source": [
    "## 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f70be-ac30-4f9c-8c12-b0938eb60270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass\n",
    "class ResetEnv_1D():\n",
    "    L : float\n",
    "    D : float    \n",
    "    position : float    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 L = 1.3,\n",
    "                 D = 1.0,                    \n",
    "                ):        \n",
    "   \n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.position = 0\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = 0\n",
    "    \n",
    "    def update_pos(self, \n",
    "                   action # 0: continue walk, 1: reset to origin\n",
    "                  ): # Reward = 1 if crossed L, else = 0\n",
    "        \n",
    "        if action == 0:\n",
    "            self.position += np.random.randn()*np.sqrt(2*self.D)        \n",
    "        else: self.position = 0\n",
    "                \n",
    "        if self.position >= self.L: \n",
    "            self.init_env()\n",
    "            return 1\n",
    "        else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefb75f",
   "metadata": {},
   "source": [
    "### Parallel search loops for Reset 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset1D_sharp(T, resets, L, D):\n",
    "    '''\n",
    "    Runs the Reset 1D loop in parallel for different sharp resetting times.\n",
    "    '''\n",
    "    rews_reset = np.zeros_like(resets)\n",
    "    \n",
    "    for idxr in prange(len(resets)):\n",
    "        \n",
    "        env = ResetEnv_1D(L, D)        \n",
    "        reset_policy = np.zeros(resets[idxr])\n",
    "        reset_policy[resets[idxr]-1] = 1        \n",
    "        \n",
    "        rews_reset[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_reset\n",
    "\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset1D_exp(T, rates, L, D):\n",
    "    '''\n",
    "    Runs the Reset 1D loop in parallel for different exponential resetting rates.\n",
    "    '''\n",
    "    \n",
    "    rews_rate = np.zeros_like(rates)\n",
    "    for idxr in prange(len(rates)):\n",
    "        \n",
    "        env = ResetEnv_1D(L, D)        \n",
    "        reset_policy = np.ones(T)*rates[idxr]\n",
    "        \n",
    "        rews_rate[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c93b4-a288-47aa-aaf9-b4fb52fef424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.4 μs ± 2.34 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%timeit parallel_Reset1D_sharp(100, np.linspace(70, 150, 20).astype(np.int64), 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539cd73f-9893-4f05-863b-090ec19b0f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.2 μs ± 29 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%timeit parallel_Reset1D_exp(100, np.linspace(70, 150, 40).astype(np.int64), 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8309e-8b30-4205-9fb5-7ec1fa077da1",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ee9ec-6a02-452f-8017-778c9b81cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"position\", float64[:]),\n",
    "           (\"target_position\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:])\n",
    "          ])\n",
    "class ResetEnv_2D():\n",
    "    D : float    \n",
    "    dist_target :float\n",
    "    r: float\n",
    "    position : np.array    \n",
    "    target_position : np.array\n",
    "    previous_pos : np.array\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dist_target = 0.2, # Distance from init position and target\n",
    "                 radius_target = 0.5, # Radius of the target\n",
    "                 D = 1.0, # Diffusion coefficient of the walker              \n",
    "                ):    \n",
    "        \n",
    "\n",
    "        self.D = D\n",
    "        self.dist_target = dist_target\n",
    "        self.r = radius_target        \n",
    "        \n",
    "        self.target_position = np.array([self.dist_target*np.cos(np.pi/4), self.dist_target*np.sin(np.pi/4)])[np.newaxis, :]       \n",
    "        \n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = np.array([0.0,0.0])\n",
    "        self.previous_pos = self.position.copy()     \n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_pos(self, \n",
    "                   action # 0: continue walk, 1: reset to origin\n",
    "                  ): # Reward = 1 if encountered target, else = 0\n",
    "        \n",
    "        if action == 1:\n",
    "            self.init_env()\n",
    "            return 0\n",
    "        \n",
    "        elif action == 0:\n",
    "            \n",
    "            self.previous_pos = self.position.copy()            \n",
    "            self.position += np.random.randn(2)*np.sqrt(2*self.D)\n",
    "            \n",
    "            # Checking encounter\n",
    "            inside_target = np.linalg.norm(self.position-self.target_position) <= self.r\n",
    "            crossed_target = isBetween_c_Vec_numba(self.previous_pos, self.position, self.target_position, self.r)\n",
    "                        \n",
    "            if inside_target or crossed_target:\n",
    "                self.init_env()\n",
    "                return 1\n",
    "            else: \n",
    "                return 0\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1eda84",
   "metadata": {},
   "source": [
    "## Parallel search loops for Reset 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa95ae-5fc4-42e0-9157-2091de54fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset2D_sharp(T, resets, dist_target, radius_target, D):\n",
    "    rews_reset = np.zeros_like(resets)\n",
    "    \n",
    "    for idxr in prange(len(resets)): \n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)        \n",
    "        reset_policy = np.zeros(resets[idxr])\n",
    "        reset_policy[resets[idxr]-1] = 1        \n",
    "        \n",
    "        rews_reset[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_reset\n",
    "\n",
    "\n",
    "\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset2D_exp(T, rates, dist_target, radius_target, D):\n",
    "    \n",
    "    rews_rate = np.zeros_like(rates)\n",
    "    for idxr in prange(len(rates)):\n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)         \n",
    "        reset_policy = np.ones(T)*rates[idxr]\n",
    "        \n",
    "        rews_rate[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_rate\n",
    "\n",
    "@njit(parallel = True)\n",
    "def parallel_Reset2D_policies(T, reset_policies, dist_target, radius_target, D):\n",
    "    \n",
    "    rews_rate = np.zeros(reset_policies.shape[0])    \n",
    "    \n",
    "    for idx_policy in prange(reset_policies.shape[0]):    \n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)  \n",
    "                \n",
    "        rews_rate[idx_policy] = reset_search_loop(T = T, reset_policy = reset_policies[idx_policy], env = env)\n",
    "        \n",
    "    return rews_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7e68c-86d2-4ae5-abea-8b2b4df83122",
   "metadata": {},
   "source": [
    "# TurnResetEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b732f4-6bff-459a-b5ff-ef8c89e9e353",
   "metadata": {},
   "source": [
    "> Only 2D is considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fb886-db05-40b9-b569-b6b8a361ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"position\", float64[:]),\n",
    "           (\"target_position\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:])\n",
    "          ])\n",
    "class TurnResetEnv_2D():\n",
    "    agent_step : float    \n",
    "    dist_target :float\n",
    "    r: float\n",
    "    position : np.array    \n",
    "    target_position : np.array\n",
    "    previous_pos : np.array\n",
    "    current_direction : float\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dist_target = 0.2, # Distance from init position and target\n",
    "                 radius_target = 0.5, # Radius of the target\n",
    "                 agent_step = 1.0, # Diffusion coefficient of the walker              \n",
    "                ):   \n",
    "        \n",
    "        '''\n",
    "        Class defining a Foraging environment with a single target and three possible actions:\n",
    "\n",
    "        - Continue in the same direction\n",
    "        - Turn by a random angle\n",
    "        - Reset to the origin\n",
    "\n",
    "        The agent makes steps of constant length given by agent_step. \n",
    "        '''\n",
    "        \n",
    "\n",
    "        self.agent_step = agent_step\n",
    "        self.dist_target = dist_target\n",
    "        self.r = radius_target        \n",
    "        \n",
    "        self.target_position = np.array([self.dist_target*np.cos(np.pi/4), self.dist_target*np.sin(np.pi/4)])[np.newaxis, :]       \n",
    "        \n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = np.array([0.0,0.0])\n",
    "        self.current_direction = np.random.rand()*2*np.pi\n",
    "        self.previous_pos = self.position.copy()     \n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_pos(self, \n",
    "                   change_direction, # If True, the agent changes direction by a random angle\n",
    "                   reset # If True, the agent is reset to the origin\n",
    "                   ):        \n",
    "        \"\"\"\n",
    "        Updates position of the agent depending on its decision\n",
    "        \"\"\"\n",
    "        \n",
    "        if reset:\n",
    "            self.init_env()\n",
    "            return 0\n",
    "\n",
    "        else:\n",
    "            if change_direction:\n",
    "                self.current_direction = np.random.rand()*2*np.pi\n",
    "\n",
    "            self.position[0] += self.agent_step*np.cos(self.current_direction)\n",
    "            self.position[1] += self.agent_step*np.sin(self.current_direction)\n",
    "\n",
    "            # Checking encounter\n",
    "            inside_target = np.linalg.norm(self.position-self.target_position) <= self.r\n",
    "            crossed_target = isBetween_c_Vec_numba(self.previous_pos, self.position, self.target_position, self.r)\n",
    "                        \n",
    "            if inside_target or crossed_target:\n",
    "                self.init_env()\n",
    "                return 1\n",
    "            else:                \n",
    "                self.previous_pos = self.position.copy()\n",
    "                return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c1c49",
   "metadata": {},
   "source": [
    "## Search loop with fixed policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def search_loop_turn_reset_sharp(T, reset, turn, env):\n",
    "    \"\"\"\n",
    "    Runs a search loop of T steps. There is a single counter that works as follows:\n",
    "\n",
    "    - Starts at 0\n",
    "    - For each turn or continue action gets +1\n",
    "    - If reset or reach the target is set to 0\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    rewards = 0\n",
    "    counter = 0 \n",
    "\n",
    "    env.init_env()\n",
    "    \n",
    "    for t in range(T):        \n",
    "        counter += 1\n",
    "        # Reset\n",
    "        if counter == reset:\n",
    "            rew = env.update_pos(False, # change direction\n",
    "                                 True   # reset\n",
    "                                )           \n",
    "            counter = 0\n",
    "        \n",
    "        # Turn\n",
    "        elif counter == turn:\n",
    "            rew = env.update_pos(True, # change direction\n",
    "                                 False # reset\n",
    "                                )\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            rew = env.update_pos(False, # change direction\n",
    "                                 False # reset\n",
    "                                )\n",
    "        if rew == 1:\n",
    "            counter = 0\n",
    "            \n",
    "        rewards += rew\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d3bbd-1cf6-45ad-9c88-d87a0d83c134",
   "metadata": {},
   "source": [
    "# TargetDirectedMotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d9679-1eeb-47f3-9d49-c1c28c55207f",
   "metadata": {},
   "source": [
    "Differences with respect to the previous environment TargetEnv:\n",
    "\n",
    "- Actions: continue, rotate left, rotate right, rotate back. The 'left', 'right' and 'back' areas are areas of 120º ($2 \\pi/3$) each on the left, right and back of the agent, respectively. After picking an action that is not 'continue', the agent rotates in a random direction within the chosen area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fda37b-0f08-4f80-b469-ef928b1dec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"kicked\", float64[:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"lc\", float64[:,:]),\n",
    "           (\"mask\", bool_[:]),\n",
    "           (\"first_encounter\", float64[:,:])])\n",
    "class TargetDirectedMotion():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    lc : np.array\n",
    "    agent_step : float\n",
    "    num_agents : int\n",
    "    destructive_targets : bool\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    kicked : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    first_encounter : np.array\n",
    "    lc_distribution : str\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt = 10, # blabla\n",
    "                 L = 1.3,\n",
    "                 r = 1.5,\n",
    "                 lc = np.array([[1.0],[1]]),\n",
    "                 agent_step = 1,\n",
    "                 num_agents = 1,\n",
    "                 destructive = False,\n",
    "                 lc_distribution = 'constant'):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Class defining the foraging environment. It includes the methods needed to place several agents to the world.\n",
    "        \n",
    "        Updated from `rl_framework.TargetEnv`:        \n",
    "            > `lc_distribution`: now allows to consider different distributions. lc now means different things depending on the distribution.\n",
    "        \n",
    "        **Inputs**\n",
    "        \n",
    "        `Nt` : (int) \n",
    "            Number of targets.\n",
    "            \n",
    "        `L` : (int)\n",
    "            Size of the (squared) world.\n",
    "            \n",
    "        `r` : (int) \n",
    "            Radius with center the target position. It defines the area in which agent detects the target.\n",
    "            \n",
    "        `lc` \n",
    "            Cutoff length. Displacement away from target (to implement revisitable targets by displacing agent away from the visited target).\n",
    "            \n",
    "        `agent_step`: (int, optional)\n",
    "            Displacement of one step. The default is 1.\n",
    "            \n",
    "        `num_agents`: (int, optional)\n",
    "            Number of agents that forage at the same time. The default is 1.\n",
    "            \n",
    "        `destructive`: (bool, optional)\n",
    "            True if targets are destructive. The default is False.\n",
    "            \n",
    "        `lc_distribution`: (str) Chosee between 'power_law', 'pareto' and 'constant'. Depending on the previous choice, lc has different meanings:\n",
    "        \n",
    "        > `power_law` : lc is sampled from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0] \n",
    "        \n",
    "        > `pareto` : lc is sampled from a Pareto distribution with alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "        \n",
    "        > `constant` : if len(lc) == 1, then that's the lc. If len(lc) > 1, then samples an lc considering vals = lc[0] and probabilities = lc[1]\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.lc = lc\n",
    "        self.agent_step = agent_step \n",
    "        self.num_agents = num_agents\n",
    "        self.destructive_targets = destructive\n",
    "        self.lc_distribution = lc_distribution\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #signal whether agent has been kicked\n",
    "        self.kicked = np.zeros(self.num_agents)\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, action, agent_index = 0):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            0 (continue), 1 (left), 2 (right), 3 (back).\n",
    "        agent_index : int, optional\n",
    "            Index of the given agent. The default is 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if action == 0: #if agent continues, it does not change its direction.\n",
    "            pass\n",
    "        elif action == 1: #left\n",
    "            self.current_directions[agent_index] += random.uniform(0,1)*2*np.pi/3\n",
    "        elif action == 2: #right\n",
    "            self.current_directions[agent_index] -= random.uniform(0,1)*2*np.pi/3\n",
    "        elif action == 3: #back\n",
    "            self.current_directions[agent_index] += 2*np.pi/3 + random.uniform(0,1)*2*np.pi/3\n",
    "        else:\n",
    "            print('This action value is not defined')\n",
    "            \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "       \n",
    "    def check_encounter(self):\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_index : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True if the agent found a target.\n",
    "\n",
    "        \"\"\"       \n",
    "        agent_index = 0\n",
    "        encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        \n",
    "        if sum(encounters) > 0: \n",
    "            \n",
    "            #if there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "            if self.destructive_targets:\n",
    "                self.target_positions[first_encounter] = np.random.rand(2)*self.L\n",
    "            else:\n",
    "                #----KICK----\n",
    "                # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                kick_direction = np.random.uniform(low = 0, high = 2*np.pi)\n",
    "                for idx_first in first_encounter: # This is super weird!\n",
    "                    if self.lc_distribution == 'power_law':\n",
    "                        # when we have the power law, the first value of lc is considered to be the exponent.\n",
    "                        # The following samples from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0]                        \n",
    "                        current_lc = (1-random.uniform(0,1))**(-1/self.lc.flatten()[0])\n",
    "\n",
    "                    elif self.lc_distribution == 'pareto':\n",
    "                        # Sampling from Pareto. Here alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "                        current_lc = pareto_sample(self.lc[0,0], self.lc[1,0])[0]\n",
    "                    else:\n",
    "                        # if lc has a single element, take that one as lc, if not sample\n",
    "                        current_lc = self.lc.flatten()[0] if len(self.lc.flatten()) == 2 else rand_choice_nb(arr = self.lc[0], prob = self.lc[1])\n",
    "                    self.positions[agent_index][0] = self.target_positions[idx_first, 0] + current_lc*np.cos(kick_direction)\n",
    "                    self.positions[agent_index][1] = self.target_positions[idx_first, 1] + current_lc*np.sin(kick_direction)\n",
    "                self.kicked[agent_index] = 1\n",
    "                #------------\n",
    "                \n",
    "            #...and we add the information that this agent got to the target\n",
    "            self.current_rewards[agent_index] = 1              \n",
    "            return 1\n",
    "        \n",
    "        else: \n",
    "            self.kicked[agent_index] = 0\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "        \n",
    "    def check_bc(self):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of agent agent_index to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_index=0\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f523c-a1fc-45c7-aa67-23026b36b732",
   "metadata": {},
   "source": [
    "## Walk from policy (TargetDirectedMotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2baa665-8044-4158-8788-75624e0f54c9",
   "metadata": {},
   "source": [
    "The following function is the same as `single_agent_walk` with the following differences:\n",
    "- One environment (same target positions all the time)\n",
    "- It saves the trajectories and the target positions\n",
    "- The policy is the one with 4 actions (continue, left, right, back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b2aa1-f06f-4bb0-b634-04d8f7f1be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@njit\n",
    "def single_agent_trajectory(time_ep : int, # Length of each run / episode\n",
    "                            policy : np.array, # Policy of the walker (rows: states. columns:prob of doing each action)\n",
    "                            env : object # Environment where the walker moves\n",
    "                           )-> tuple: # number of visited targets, agent and target positions\n",
    "\n",
    "    '''\n",
    "    Walk of a single agent in env of type TargetDirectedMotion given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    '''\n",
    "    \n",
    "    number_visited_targets = 0\n",
    "    agent_positions = np.zeros((time_ep, 2))\n",
    "    \n",
    "    \n",
    "    #initialize environment and agent's counter and g matrix\n",
    "    env.init_env()\n",
    "    agent_state = 0\n",
    "\n",
    "    for t in range(time_ep):\n",
    "        \n",
    "        agent_positions[t,:] = env.positions[0] #position of agent with index 0\n",
    "\n",
    "        if t == 0 or env.kicked[0]:\n",
    "            # change direction\n",
    "            action = rand_choice_nb(arr = np.arange(1, len(policy[0])), prob = np.array([1/len(np.arange(1,len(policy[0])))]*len(np.arange(1,len(policy[0])))))\n",
    "            env.update_pos(action)\n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #reset counter\n",
    "            agent_state = 0\n",
    "            #set kicked value to false again\n",
    "            env.kicked[0] = 0\n",
    "\n",
    "        else: \n",
    "            # decide       \n",
    "            action = rand_choice_nb(arr = np.arange(len(policy[0])), prob = policy[agent_state])\n",
    "            #update positions\n",
    "            env.update_pos(action)\n",
    "            #check if target was found + kick if it is\n",
    "            reward = env.check_encounter()\n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            # update agent_state\n",
    "            if action == 0:\n",
    "                agent_state += 1\n",
    "            else:\n",
    "                agent_state = 0\n",
    "\n",
    "            number_visited_targets += reward\n",
    "                \n",
    "    return (number_visited_targets, agent_positions, env.target_positions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b214e24-7c5b-4404-93d7-85f1593dfc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@njit(parallel = True)\n",
    "def parallel_agents_trajectories(time_ep : int, # Length of each run / episode\n",
    "                                 N_agents : int, # Number of agents to consider\n",
    "                                 Nt = 100, # Number of targets in the environment\n",
    "                                 L = 100, # Size of the environment\n",
    "                                 r = 0.5, # Radius of the targets\n",
    "                                 lc = np.array([[1.0],[1]]), # Parameters of lc distribution or lc itself \n",
    "                                 agent_step = 1, # Length of agent's step\n",
    "                                 destructive_targets = False, # True if targets are destructive. The default is False. \n",
    "                                 lc_distribution = 'constant', # lc distribution\n",
    "                                 policies = np.array([[1.0,0.0,0.0,0.0], [0.0,1.0,0.0,0.0]]) # Policy of the agents\n",
    "                                )-> tuple: # number of visited targets, agent and target positions\n",
    "    \"\"\"\n",
    "    Runs in parallel single_agent_trajectory. Due to numba props, we need to give all parameters as inputs (see source).\n",
    "    \"\"\"\n",
    "    \n",
    "    agent_positions = np.zeros((N_agents, time_ep, 2))\n",
    "    number_visited_targets = np.zeros(N_agents)\n",
    "    target_positions = np.zeros((N_agents, Nt, 2))\n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = TargetDirectedMotion(Nt,L,r,lc,agent_step,1,destructive_targets,lc_distribution)\n",
    "        \n",
    "        num_tar, pos, tar_pos = single_agent_trajectory(time_ep, policies[n_agent], env) \n",
    "    \n",
    "        agent_positions[n_agent] = pos\n",
    "        number_visited_targets[n_agent] = num_tar\n",
    "        target_positions[n_agent] = tar_pos\n",
    "        \n",
    "    return (number_visited_targets, agent_positions, target_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad71862-8ae0-4d88-bd92-f5406ab7c2f8",
   "metadata": {},
   "source": [
    "# CollectiveSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b7216-2d93-4a2b-be3a-2d3ab64f52ef",
   "metadata": {},
   "source": [
    "Environment where several agents search for targets. Each agent can see neighbours within a certain visual radius. \n",
    "\n",
    "- Actions: continue, rotate left, rotate right, rotate back. The 'left', 'right' and 'back' areas are areas of 120º ($2 \\pi/3$) each on the left, right and back of the agent, respectively. After picking an action that is not 'continue', the agent rotates in a random direction within the chosen area.\n",
    "\n",
    "- States: the agent perceives the counter of steps walking straight and (TO DO) states related to the neighbours.\n",
    "\n",
    "- Instead of kick, here we implement a time delay $\\tau_i$ for each agent $i$. The agent needs to wait $\\tau_i$ time steps from the moment it gets a target until it can get the same target again. The equivalence between kick and time delay is $\\tau_i = lc + 2$.\n",
    "\n",
    "- When the agent crosses a target, it resets the counter of steps walking straight and its position is updated to the center of the found target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13892a57-89c2-45c6-a58f-389a58a8feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@jitclass([(\"target_positions\", float64[:,:]) ,\n",
    "           (\"current_rewards\", float64[:]) ,\n",
    "           (\"tau_allagents\", float64[:]) ,\n",
    "           (\"depleted\", float64[:,:]) ,\n",
    "           (\"current_directions\", float64[:]) ,\n",
    "           (\"positions\", float64[:,:]),\n",
    "           (\"previous_pos\", float64[:,:]),\n",
    "           (\"mask\", bool_[:])])\n",
    "           \n",
    "class CollectiveSearch():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    agent_step : float\n",
    "    num_agents : int\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    tau_allagents : np.array\n",
    "    depleted : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 Nt:int, # Number of targets.\n",
    "                 L:float, # Size of the (squared) world.\n",
    "                 r:float, # Radius with center the target position. It defines the area in which agent detects the target.\n",
    "                 tau:float, # Time it takes for the targets to replenish.\n",
    "                 agent_step:float=1.0, # Displacement of the agent when it performs one step.\n",
    "                 num_agents:int=1, # Number of agents that forage at the same time.\n",
    "                 ):\n",
    "        \n",
    "        \"\"\"        \n",
    "        Class defining the foraging environment with replenishable targets modeled with a time delay instead of a kick. \n",
    "        It includes the methods needed to place several agents to the world.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.agent_step = agent_step \n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # same replenishing time for all agents\n",
    "        self.tau_allagents = tau * np.ones(self.num_agents)\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #number of steps since last target encounter, for each agent.\n",
    "        self.depleted = np.zeros((self.Nt, self.num_agents))\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, action, agent_index = 0):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            0 (continue), 1 (left), 2 (right), 3 (back).\n",
    "        agent_index : int, optional\n",
    "            Index of the given agent. The default is 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if action == 0: #if agent continues, it does not change its direction.\n",
    "            pass\n",
    "        elif action == 1: #left\n",
    "            self.current_directions[agent_index] += random.uniform(0,1)*2*np.pi/3\n",
    "        elif action == 2: #right\n",
    "            self.current_directions[agent_index] -= random.uniform(0,1)*2*np.pi/3\n",
    "        elif action == 3: #back\n",
    "            self.current_directions[agent_index] += 2*np.pi/3 + random.uniform(0,1)*2*np.pi/3\n",
    "        else:\n",
    "            print('This action value is not defined')\n",
    "            \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "       \n",
    "    def check_encounter(self, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_index : int, optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        True if the agent found a target.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if there was a target encounter\n",
    "        encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        if sum(encounters) > 0: \n",
    "            # If there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "           \n",
    "            # If the agent is NOT waiting for the recently found target to replenish:\n",
    "            if not self.depleted[first_encounter, agent_index]:\n",
    "                # We add the information that this agent got a reward\n",
    "                self.current_rewards[agent_index] = 1\n",
    "                # The replenishing time for this target and this agent starts.\n",
    "                self.depleted[first_encounter, agent_index] += 1\n",
    "                \n",
    "                # The agent moves to the target's position.\n",
    "                self.positions[agent_index] = self.target_positions[first_encounter].copy()\n",
    "           \n",
    "                # The env issues a positive reward.\n",
    "                return 1\n",
    "            else:\n",
    "                self.current_rewards[agent_index] = 0\n",
    "                return 0\n",
    "                    \n",
    "        else:\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "    \n",
    "    def update_replenishing_times(self, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Updates the replenishing times of the targets for a given agent.\n",
    "        \"\"\"\n",
    "        self.depleted[np.argwhere(self.depleted[:, agent_index] != 0).flatten(), agent_index] += 1\n",
    "        self.depleted[:, agent_index] = self.depleted[:, agent_index] % (self.tau_allagents[agent_index] + 1)\n",
    "        \n",
    "    def check_bc(self, agent_index = 0):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of a given agent to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50e7aa-7e94-4368-8ef5-d1981ff9028e",
   "metadata": {},
   "source": [
    "## Walk from policy (CollectiveSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd08ed6-0577-4361-8f0b-1f0db1ab81a2",
   "metadata": {},
   "source": [
    "The following function is the same as `single_agent_walk` with the following differences:\n",
    "- One environment (same target positions all the time)\n",
    "- It saves the trajectories and the target positions\n",
    "- The policy is the one with 4 actions (continue, left, right, back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597414c0-f778-4bf0-ae6e-f728d1e6e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "@njit\n",
    "def single_agent_trajectory_tau(time_ep : int, # Length of each run / episode\n",
    "                            policy : np.array, # Policy of the walker (rows: states. columns:prob of doing each action)\n",
    "                            env : object # Environment where the walker moves\n",
    "                           )-> tuple: # number of visited targets, agent and target positions\n",
    "\n",
    "    '''\n",
    "    Walk of a single agent in env of type CollectiveSearch given a policy. Performance is evaluated as the number of targets found in a fixed time time_ep.\n",
    "    '''\n",
    "    \n",
    "    number_visited_targets = 0\n",
    "    agent_positions = np.zeros((time_ep, 2))\n",
    "    rew_per_timestep = np.zeros(time_ep)\n",
    "    \n",
    "    #initialize environment and agent's counter and g matrix\n",
    "    env.init_env()\n",
    "    agent_state = 0\n",
    "\n",
    "    for t in range(time_ep):\n",
    "        \n",
    "        agent_positions[t,:] = env.positions[0] #position of agent with index 0\n",
    "        \n",
    "        if t == 0 or env.current_rewards[0] == 1:\n",
    "            # change direction\n",
    "            action = rand_choice_nb(arr = np.arange(1, len(policy[0])), prob = np.array([1/len(np.arange(1,len(policy[0])))]*len(np.arange(1,len(policy[0])))))\n",
    "            env.update_pos(action)\n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #reset counter\n",
    "            agent_state = 0\n",
    "            #update current rewards\n",
    "            env.current_rewards[0] = 0\n",
    "            reward = 0\n",
    "            #update replenishing times\n",
    "            env.update_replenishing_times()\n",
    "            \n",
    "\n",
    "        else: \n",
    "            # decide       \n",
    "            action = rand_choice_nb(arr = np.arange(len(policy[0])), prob = policy[agent_state])\n",
    "            #update positions\n",
    "            env.update_pos(action)\n",
    "            #update replenishing times\n",
    "            env.update_replenishing_times()\n",
    "            \n",
    "            #check if target was found \n",
    "            reward = env.check_encounter()\n",
    "            \n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            \n",
    "            # update agent_state\n",
    "            if action == 0: #continue\n",
    "                agent_state += 1\n",
    "            else: #turn\n",
    "                agent_state = 0\n",
    "\n",
    "            number_visited_targets += reward\n",
    "            rew_per_timestep[t] += reward\n",
    "                \n",
    "    return (number_visited_targets, agent_positions, env.target_positions, rew_per_timestep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a031b-180e-4901-97e9-d9ad1f97c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CollectiveSearch(Nt = 100,\n",
    "                       L = 100, \n",
    "                       r = 1, \n",
    "                       tau = 3)\n",
    "\n",
    "time_ep = 300\n",
    "#initial policy with 4 actions\n",
    "policy = (np.array([0.988, 0.012/3, 0.012/3, 0.012/3])*np.ones((4, time_ep)).transpose())\n",
    "policy[1] = np.array([0.1, 0, 0, 0.9])\n",
    "\n",
    "num_targets, agent_pos, target_pos, rew_t = single_agent_trajectory_tau(time_ep = time_ep, # Length of each run / episode\n",
    "                                                                policy = policy, # Policy of the walker (rows: states. columns:prob of doing each action)\n",
    "                                                                env = env # Environment where the walker moves\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36037939-05bc-45da-a88b-2196b20c36b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81.0, 84.0, 6.0, 11.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent_index = 0\n",
    "r = 1 #target radius\n",
    "x_min = 81; x_max = 84 ; y_min = 6 ; y_max = 11;\n",
    "fig, ax = plt.subplots()\n",
    "for target in target_pos:\n",
    "    circle = plt.Circle((target[0], target[1]), r, fill=False)\n",
    "    ax.add_patch(circle)\n",
    "#plt.scatter(target_pos[:,0], target_pos[:,1], color = 'g', s=15)\n",
    "ax.plot(agent_pos[:,0], agent_pos[:,1], color='k', alpha=0.2, linewidth=1)\n",
    "\n",
    "ax.scatter(agent_pos[:,0],agent_pos[:,1], color='k', alpha=0.2, s=10)\n",
    "\n",
    "ax.axis([x_min, x_max, y_min, y_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5899e70-72ac-4f8e-899c-554ffe22e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@njit(parallel = True)\n",
    "def parallel_agents_trajectories_tau(time_ep : int, # Length of each run / episode\n",
    "                                 N_agents : int, # Number of agents to consider\n",
    "                                 Nt = 100, # Number of targets in the environment\n",
    "                                 L = 100, # Size of the environment\n",
    "                                 r = 0.5, # Radius of the targets\n",
    "                                 tau = 3, # Time delay until target is available again\n",
    "                                 agent_step = 1, # Length of agent's step\n",
    "                                 policies = np.array([[1.0,0.0,0.0,0.0], [0.0,1.0,0.0,0.0]]) # Policy of the agents\n",
    "                                )-> tuple: # number of visited targets, agent and target positions\n",
    "    \"\"\"\n",
    "    Runs in parallel single_agent_trajectory_tau. Due to numba props, we need to give all parameters as inputs (see source).\n",
    "    \"\"\"\n",
    "    \n",
    "    agent_positions = np.zeros((N_agents, time_ep, 2))\n",
    "    number_visited_targets = np.zeros(N_agents)\n",
    "    target_positions = np.zeros((N_agents, Nt, 2))\n",
    "    reward_per_timestep = np.zeros((N_agents, time_ep))\n",
    "    \n",
    "    for n_agent in prange(N_agents):\n",
    "        \n",
    "        env = CollectiveSearch(Nt,L,r,tau,agent_step,1)\n",
    "        \n",
    "        num_tar, pos, tar_pos, rew_t = single_agent_trajectory_tau(time_ep, policies[n_agent], env) \n",
    "    \n",
    "        agent_positions[n_agent] = pos\n",
    "        number_visited_targets[n_agent] = num_tar\n",
    "        target_positions[n_agent] = tar_pos\n",
    "        reward_per_timestep[n_agent] = rew_t\n",
    "        \n",
    "    return (number_visited_targets, agent_positions, target_positions, reward_per_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b2eb3",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a2271-1a58-457e-86ca-b8185315d3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
