{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train an RL agent, we need to have (i) an environment and (ii) a learning method. In this work, we define a foraging environment where the goal of the agent is to find as many targets as possible in a given time. We consider environments with non-destructive -or replenishable- targets, which we implement by displacing the agent a distance $l_\\textrm{c}$ from the center of the found target.\n",
    "\n",
    "As for the agent, we use Projective Simulation (PS) to model its decision making process and learning method. However, other algorithms that work with stochastic policies can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the classes that define the environment (`TargetEnv`), the forager dynamics (`Forager`), and its learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rl_opts.rl_framework import TargetEnv, Forager\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the class `Forager` as it currently is inherits the methods of a PS agent for decision making and learning. However, other learning algorithms can be directly implemented by changing this inheritance. The learning algorithm should contain a method for decision making, called `deliberate`, which inputs a state; and another one for updating the policy, called `learn`, which inputs a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the parameters defining the length of the episodes (number of RL steps) and the number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_EP = 200 #time steps per episode\n",
    "EPISODES = 1200 #number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment parameters\n",
    "Nt = 100 #number of targets\n",
    "L = 100 #world size\n",
    "r = 0.5 #target detection radius\n",
    "lc = 1 #cutoff length\n",
    "\n",
    "#Initialize environment\n",
    "env = TargetEnv(Nt, L, r, lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the agent. As states, the agent perceives the value of an internal counter that keeps track of the number of small steps that it has performed without turning. The possible actions are continue walking in the same direction or turning. The agent performs a small step of length $d=1$ in any case after making a decision. Let's define the parameters of the PS forager agent and initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 2 # continue in the same direction, turn\n",
    "STATE_SPACE = [np.linspace(0, TIME_EP-1, TIME_EP), np.arange(1), np.arange(1)] # one state per value that the counter may possibly have within an episode.\n",
    "#--the last two entries are just placeholders here, but the code is general enough to implement ensembles of interacting agents that forage together.--\n",
    "GAMMA = 0.00001 #forgetting parameter in PS\n",
    "ETA_GLOW = 0.1 #glow damping parameter in PS\n",
    "INITIAL_DISTR = [] #set a different initialization policy\n",
    "for percept in range(TIME_EP): \n",
    "    INITIAL_DISTR.append([0.99, 0.01]) \n",
    "    \n",
    "\n",
    "#Initialize agent\n",
    "agent = Forager(num_actions=NUM_ACTIONS,\n",
    "                state_space=STATE_SPACE,\n",
    "                gamma_damping=GAMMA,\n",
    "                eta_glow_damping=ETA_GLOW,\n",
    "                initial_prob_distr=INITIAL_DISTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc15d4b2c4534d8ea21af1bf8f97b161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in tqdm(range(EPISODES)):\n",
    "        \n",
    "    #restart environment and agent's counter and g matrix\n",
    "    env.init_env()\n",
    "    agent.agent_state = 0\n",
    "    agent.reset_g()\n",
    "\n",
    "    for t in range(TIME_EP):\n",
    "        \n",
    "        #step to set counter to its min. value n=1\n",
    "        if t == 0 or env.kicked[0]:\n",
    "            #do one step with random direction (no learning in this step)\n",
    "            env.update_pos(1)\n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #reset counter\n",
    "            agent.agent_state = 0\n",
    "            #set kicked value to false again\n",
    "            env.kicked[0] = 0\n",
    "            \n",
    "        else:\n",
    "            #get perception\n",
    "            state = agent.get_state()\n",
    "            #decide\n",
    "            action = agent.deliberate(state)\n",
    "            #act (update counter)\n",
    "            agent.act(action)\n",
    "            \n",
    "            #update positions\n",
    "            env.update_pos(action)\n",
    "            #check if target was found + kick if it is\n",
    "            reward = env.check_encounter()\n",
    "                \n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #learn\n",
    "            agent.learn(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the code can directly accomodate environments with several agents that interact. For this reason, you will find methods in both the environment class `TargetEnv` and the forager class `Forager` that deal with agents that have visual cones and can perceive the presence of other agents in their surroundings. However, these features are not used in this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we explain how to reproduce the results of the paper that concern the training of RL agents in the foraging environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we run the training as detailed above, and we save the agent's memory periodically, together with other useful data. The results of the training are saved by default in the directory 'results/learning/'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reproduce the training by running the following command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "python run_learning.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first imports a configuration file that contains the parameters to initialize both the environment and the agent. For each set of parameters we ran, there is an experiment name (by default 'learning'), and an identifier of the form \"exp_numconfig\" (e.g. exp_0) that uniquely identifies the config file and the folder that contains the saved data. The config files for the experiments that give the results of the paper can be found in the directory 'configurations/learning/'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment name (--experiment), configuration number (--num_config) and agent identifier (--run) can be directly parsed when running the file. Here, you can see a snippet of the code that shows what you can modify when parsing these arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num_config'], dest='num_config', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, required=False, help='number of the configuration file', metavar=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import argv\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--experiment', type=str, default='learning', help='type of experiment')        \n",
    "parser.add_argument('--run', type=int, default=0, help='run id')\n",
    "parser.add_argument('--num_config', type=int, default=0, help='number of the configuration file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the configuration is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl_opts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 2\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[43mnum_config\u001b[49m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.cfg\u001b[39m\u001b[38;5;124m'\u001b[39m, experiment)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_config' is not defined"
     ]
    }
   ],
   "source": [
    "from rl_opts.utils import get_config\n",
    "config = get_config('exp_'+str(num_config)+'.cfg', experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are extracted from the config dictionary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = config['NUM_ACTIONS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters that you can find in the config files:\n",
    "\n",
    "`NUM_TARGETS` : number of targets \\\n",
    "`WORLD_SIZE` : side of the square that defines the world (with periodic boundary conditions) \\\n",
    "`r` : target detection radius \\\n",
    "`lc` : cutoff length \\\n",
    "`MAX_STEP_L` : maximum value of the step counter (which coincides with the number of RL steps per episode) \\\n",
    "`NUM_BINS` : number of bins in which the state space is split. This is set to have one state per value of the counter \\\n",
    "`NUM_ACTIONS` : number of actions \\\n",
    "`GAMMA` : forgetting parameter $\\gamma$ in PS \\\n",
    "`ETA_GLOW` : glow damping parameter $\\eta_g$ in PS \\\n",
    "`PI_INIT` : policy initialization $\\pi_0$ ($\\forall n$). Note that it is given as $\\pi_0(\\uparrow|n)$ \\\n",
    "`NUM_EPISODES` : number of episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the training, we periodically save the PS agent's memory (h matrix) by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the results folder\n",
    "results_path = 'results/'+experiment+'/'+'exp_'+str(num_config)+'/'\n",
    "# save data\n",
    "np.save(results_path+'memory_agent_'+str(run)+'_episode_'+str(e+1)+'.npy', agent.h_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We study foraging in enviroments with different cutoff lengths $l_\\textrm{c}$. Exp_0 corresponds to $l_\\textrm{c}=0.6$. Exp_1..10 correspond to $l_\\textrm{c}=1..10$, respectively. In experiments exp_0..10, the initialization policy is $\\pi_0(\\Rsh|n)=0.01$ $\\forall n$. Exp_11 and exp_12 correspond to experiments where the initialization policy is $\\pi_0(\\Rsh|n)=0.5$ $\\forall n$. Each experiment is run with 10 independent, different agents (--run {0->9})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, you can train an agent with identifier 3 in experiment exp_8 by running the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python run_learning.py --num_config 8 --run 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also separately define (i) a config dictionary with the parameters detailed above, (ii) a results path and (iii) an agent identifier and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'asdf': 23}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.utils import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_config('exp_0.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NUM_TARGETS': 100,\n",
       " 'WORLD_SIZE': 100,\n",
       " 'r': 0.5,\n",
       " 'lc': 0.6,\n",
       " 'MAX_STEP_L': 20000,\n",
       " 'NUM_BINS': 20000,\n",
       " 'NUM_ACTIONS': 2,\n",
       " 'GAMMA': 1e-05,\n",
       " 'ETA_GLOW': 0.1,\n",
       " 'PI_INIT': 0.99,\n",
       " 'NUM_EPISODES': 12000}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.learn_and_bench import learning\n",
    "learning(config, results_path, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postlearning analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fairly compare the performance of the RL agents throughout the training with that of the benchmark models (Fig. 2), we need to run the same number of walks. In the training, the agent's policy changes from one episode to the next one, and taking the efficiency of just one episode -i.e. one walk- is not enough since we consider $10^4$ walks for the benchmark policies. Thus, we save the agent's policy at different stages of the training and then, in a postlearning analysis, we run $10^4$ walks with that frozen policy to get a more accurate evaluation of its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce the results of the postlearning analysis for, say, agent with identifier 3 in experiment exp_8, you can run the following command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python run_statistics_postlearning.py --num_config 8 --run 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the performance of the agent is evaluated every $2000$ episodes (--episode_interval), by taking the policy that the agent had in that episode, freezing it and letting the agent do $10^4$ independent walks (--num_walks) following that policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can first define (i) the experiment name, (ii) config number, (iii) agent identifier, (iv) number of walks and (v) an episode interval. Parameters (i) to (iii) are needed to identify the agent that you want to analyze, and load its previously saved policies from the correct folder. Then, you run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl_opts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearn_and_bench\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m agent_efficiency\n\u001b[0;32m----> 2\u001b[0m agent_efficiency(\u001b[43mexperiment_name\u001b[49m, num_config, run, num_walks, episode_interval)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment_name' is not defined"
     ]
    }
   ],
   "source": [
    "from rl_opts.learn_and_bench import agent_efficiency\n",
    "agent_efficiency(experiment_name, num_config, run, num_walks, episode_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, this analysis is carried out by the method `walk_from_policy`, which inputs a policy (that is not changing) and runs the walks in parallel. It outputs a list ('rewards') with the efficiency achieved in each walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.utils import get_config, get_policy\n",
    "from rl_opts.utils_env import walk_from_policy\n",
    "\n",
    "# path of the results folder\n",
    "results_path = 'results/'+experiment_name+'/'+'exp_'+str(num_config)+'/'\n",
    "\n",
    "# get policy from the stored h matrix at the given training_episode\n",
    "frozen_policy = get_policy('results/'+experiment_name+'/', 'exp_'+str(num_config), run, training_episode)\n",
    "            \n",
    "# run the 10^4 walks (in parallel) with the same policy\n",
    "rewards = walk_from_policy(policy=frozen_policy, \n",
    "                           time_ep=config['MAX_STEP_L'], \n",
    "                           n=num_walks, \n",
    "                           L=config['WORLD_SIZE'], \n",
    "                           Nt=config['NUM_TARGETS'], \n",
    "                           r=config['r'], \n",
    "                           lc=config['lc'])\n",
    "# save results\n",
    "np.save(results_path+'performance_post_training_agent_'+str(run)+'_episode_'+str(training_episode)+'.npy', rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the method `get_policy` takes the saved agent's memory at the given `training_episode` and transforms it into a policy. \n",
    "\n",
    "Note: in the code, the policies are always given as $\\pi(\\uparrow|n)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimal_search",
   "language": "python",
   "name": "optimal_search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
