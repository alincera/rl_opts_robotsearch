{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea61ace-f7d4-415c-bda5-b94b76773ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNAgent_constraints:\n",
    "    def __init__(self, constraints_size, N, model, criterion, \n",
    "                 optimizer, device, egreedy, egreedy_decay, \n",
    "                 egreedy_min, gamma, weight_E, weight_params, weight_simp = 0):       \n",
    "        \n",
    "        # Input size\n",
    "        self.state_size = constraints_size\n",
    "\n",
    "        # Output size\n",
    "        self.constraints_size = constraints_size\n",
    "        self.N = N\n",
    "        self.action_size = self.constraints_size \n",
    "        \n",
    "        \n",
    "        # Parameters\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = gamma    # discount rate\n",
    "        self.epsilon = egreedy  # exploration rate\n",
    "        self.epsilon_min = egreedy_min\n",
    "        self.epsilon_decay = egreedy_decay\n",
    "        self.model = model   \n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):      \n",
    "        \n",
    "        info = [torch.FloatTensor(state).reshape(1, self.state_size).to(self.device), \n",
    "                torch.tensor([action], device = self.device), \n",
    "                torch.FloatTensor([reward]).to(self.device), \n",
    "                torch.FloatTensor(next_state).reshape(1, self.state_size).to(self.device), \n",
    "                done]\n",
    "        self.memory.append(self.Transition(*info))\n",
    "        \n",
    "    def act(self, state):        \n",
    "        state = torch.FloatTensor(state).reshape(1, self.state_size).to(self.device)\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:            \n",
    "            action = random.randrange(self.constraints_size)                                     \n",
    "            return action\n",
    "                \n",
    "        action = int(torch.argmax(self.model(state)))\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def replay_with_loop(self, batch_size):\n",
    "        ''' Same as replay but with a for loop, which is less optimized'''\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward            \n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state))\n",
    "             \n",
    "            output = self.model(state)\n",
    "            target_f = output.clone()\n",
    "            target_f[0, action] = target\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(output, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def replay(self, batch_size):    \n",
    "        transitions = random.sample(self.memory, batch_size)        \n",
    "        batch = self.Transition(*zip(*transitions))    \n",
    "        \n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.done)), \n",
    "                                      device=self.device, dtype=torch.uint8)        \n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                        if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch.reshape(batch_size, 1))\n",
    "\n",
    "        next_state_values = reward_batch\n",
    "\n",
    "        next_state_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()* self.gamma + reward_batch[non_final_mask]        \n",
    "                \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(state_action_values, next_state_values.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()    \n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def past_reward(self, mem_array, value, mean = True, en = False):        \n",
    "        ''' Calculates the reward for a given feature cccc. If mean = True, it \n",
    "        it gives reward if the current value is < than the mean of previous. If \n",
    "        mean = False, it searches for the minimal value (this one does not work\n",
    "        very well yet...)'''  \n",
    "        \n",
    "        if mean: # Comparing to previous mean  \n",
    "            if en: # We want to maximize energy\n",
    "                if value > sum(mem_array)/len(mem_array):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0 \n",
    "            else: # We want to minimize parameters\n",
    "                if value < sum(mem_array)/len(mem_array):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0 \n",
    "        else: # Searching for minimal\n",
    "            \n",
    "            ''' IMPLEMENT MAXIMIZING ENERGY '''\n",
    "            if value <= min(mem_array):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0 \n",
    "            \n",
    "    def init_dense_reward(self, freeP, energy):\n",
    "        ''' Initiates variables that will be used in dense_reward.'''\n",
    "        self.prevP = freeP\n",
    "        self.prevE = energy\n",
    "            \n",
    "    def discrete_mean_reward(self, env, freeP, energy, simp, err): \n",
    "        ''' This function gives a reward by comparing the current value of a variable\n",
    "        to the mean value of the previous values, through the past_reward function.\n",
    "        Here, the rewards rew_t and rew_E will be discrete.'''\n",
    "        if err == 0: # if there was error calculating the energy\n",
    "            rew = 0\n",
    "        else: # if there was no error, we save the energy and the # of params for\n",
    "            # Time reward (now a function of params)\n",
    "            rew_t = self.past_reward(env.mem_freeP, freeP)\n",
    "            # Energy reward            \n",
    "            rew_E = self.past_reward(env.mem_E, energy, en = True)\n",
    "            # Total reward\n",
    "            rew = self.weight_E*rew_E + self.weight_params*rew_t + self.weight_simp*simp         \n",
    "        return rew \n",
    "    \n",
    "    def discrete_previous_reward(self, env, freeP, energy, simp, err): \n",
    "        ''' This function gives a reward by comparing the current value of a variable\n",
    "        to the previous value. Here, the rewards rew_t and rew_E will be discrete.'''\n",
    "        if err == 0: # if there was error calculating the energy\n",
    "            rew = 0\n",
    "        else: # if there was no error, we save the energy and the # of params for\n",
    "            # Time reward (now a function of params)\n",
    "            rew_t = rew_E = 0\n",
    "            if self.prevP > freeP:\n",
    "                rew_t = 1\n",
    "            if self.prevE < energy:\n",
    "                rew_E = 1\n",
    "            # Total reward\n",
    "            rew = self.weight_E*rew_E + self.weight_params*rew_t + self.weight_simp*simp         \n",
    "        return rew \n",
    "\n",
    "    \n",
    "    def dense_reward(self, env, freeP, energy, simp, err):\n",
    "        ''' This functions gives rewards by comparing the current value of a variable\n",
    "        to the previous one. Here, the rewards rew_t and rew_E will be continuous.'''\n",
    "        if err == 0:\n",
    "            rew = 0\n",
    "        else: # We make it such that, the smaller the variable wrt previous value,\n",
    "              # the bigger the reward.\n",
    "            rew_t = self.prevP - freeP\n",
    "            rew_E = self.prevE - energy\n",
    "            rew = self.weight_E*rew_E + self.weight_params*rew_t + self.weight_simp*simp\n",
    "            \n",
    "        self.prevP = freeP\n",
    "        self.prevE = energy\n",
    "        \n",
    "        return rew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
