{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5fb3bd-5f21-4ab7-a168-76830ac2b9e7",
   "metadata": {},
   "source": [
    "# RL-OptS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2a706-236f-4b46-b77c-2b9a7fa527c4",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"350\" src=\"figs/index_fig.png\">\n",
    "</p>\n",
    "<h4 align=\"center\">Reinforcement Learning of Optimal Search strategies</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7ad7c-f68f-4881-8a57-3bf626dab4d9",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a href=\"https://doi.org/10.5281/zenodo.4775311\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4775311.svg\" alt=\"PyPI version\"></a>\n",
    "  <a href=\"https://badge.fury.io/py/andi-datasets\"><img src=\"https://badge.fury.io/py/andi-datasets.svg\" alt=\"PyPI version\"></a>\n",
    "  <a href=\"https://badge.fury.io/py/andi-datasets\"><img src=\"https://img.shields.io/badge/python-3.9-red\" alt=\"Python version\"></a>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "This library builds the necessary tools needed to study, replicate and develop the results of the paper:\n",
    "[\"Optimal foraging strategies can be learned and outperform Lévy walks\"](https://) by *G. Muñoz-Gil, A. López-Incera, L. J. Fiderer* and *H. J. Briegel*. \n",
    "\n",
    "### Installation\n",
    "\n",
    "You can access all these tools installing the python package `rl_opts` via Pypi:\n",
    "```python\n",
    "pip install rl-opts\n",
    "```\n",
    "You can also opt for cloning the [source repository](https://github.com/gorkamunoz/rl_opts) and executing the following on the parent folder you just cloned the repo:\n",
    "```python\n",
    "pip install -e rl_opts\n",
    "```\n",
    "This will install both the library and the necessary packages. \n",
    "\n",
    "### Tutorials\n",
    "\n",
    "In the left sidebar you will a Tutorials tab, with notebooks that will help you navigate the package as well as reproducing the results of our paper via minimal examples. In particular, we have three tutorials:\n",
    "\n",
    "- <a href=\"tutorials/tutorial_learning.ipynb\" style=\"text-decoration:none\">Reinforcement learning </a> : shows how to train a RL agent based on Projective Simulation agents to search targets in randomly distributed environments as the ones considered in our paper.\n",
    "- <a href=\"tutorials/tutorial_imitation.ipynb\" style=\"text-decoration:none\">Imitation learning </a> : shows how to train a RL agent to imitate the policy of an expert equipped with a pre-trained policy. The latter is based on the benchmark strategies common in the literature.\n",
    "- <a href=\"tutorials/tutorial_benchmarks.ipynb\" style=\"text-decoration:none\">Benchmarks </a> : shows how launch various benchmark strategies with which to compare the trained RL agents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Package structure\n",
    "\n",
    "The package contains a set of modules for:\n",
    "\n",
    "- <a href=\"lib_nbs/01_rl_framework.ipynb\" style=\"text-decoration:none\">Reinforcement learning framework (`rl_opts.rl_framework`)</a> : building foraging environments as well as the RL agents moving on them.\n",
    "- <a href=\"lib_nbs/02_learning.ipynb\" style=\"text-decoration:none\">Learning and benchmarking (`rl_opts.learn_and_bench`)</a> : training RL agents as well as benchmarking them w.r.t. to known foraging strategies.\n",
    "- <a href=\"lib_nbs/04_imitation_learning.ipynb\" style=\"text-decoration:none\">Imitation learning (`rl_opts.imitation`)</a>: training RL agents in imitation schemes via foraging experts.\n",
    "- <a href=\"lib_nbs/04_imitation_learning.ipynb\" style=\"text-decoration:none\">Analytical functions (`rl_opts.analytics)`</a>: builiding analytical functions for step length distributions as well as tranforming these to foraging policies.\n",
    "- <a href=\"lib_nbs/00_utils.ipynb\" style=\"text-decoration:none\">Utils (`rl_opts.utils)`</a>: helpers used throughout the package.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Cite\n",
    "\n",
    "We kindly ask you to cite our paper if any of the previous material was useful for your work, here is the bibtex info:\n",
    "\n",
    "```latex\n",
    "soon\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimal_search",
   "language": "python",
   "name": "optimal_search"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
