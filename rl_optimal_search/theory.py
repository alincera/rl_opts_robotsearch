# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/theoretical_policy.ipynb.

# %% auto 0
__all__ = ['prob_L', 'get_ps', 'pw_exp', 'ps_exp_cutoff', 'prob_L_composite', 'discrete_composite_sample', 'get_ps_composite',
           'prob_L_composite_cutoff', 'get_ps_composite_cutoff', 'walk_from_policy', 'simul_ps', 'simul_2actions',
           'discrete_pw_sample']

# %% ../nbs/theoretical_policy.ipynb 3
import numpy as np
from tqdm.auto import tqdm
from scipy.special import zeta

# %% ../nbs/theoretical_policy.ipynb 5
def prob_L(L, Ps):
    '''
    :L (scalar): number of steps
    :Ps (array): probability of staying, at each L
    '''    
    return (1-Ps[L])*np.prod(Ps[:L])

# %% ../nbs/theoretical_policy.ipynb 13
from scipy.special import zeta

def get_ps(L, alpha, ps_0 = 1):
    
    ps = np.zeros(L)
    ps[0] = ps_0    
    
    for l in range(2, L+1):
                
        # product
        prod = np.prod(ps[:l-1])
        # P(L-1)
        p_lm1 = (1/zeta(alpha+1, q = 1))*(l-1)**(-alpha-1)
        # all together
        ps[l-1] = 1-p_lm1/prod
        
    return ps

# %% ../nbs/theoretical_policy.ipynb 26
from mpmath import polylog

def pw_exp(L, alpha = 1, xi = 0.01):
    # This is discrete!
    return L**(-1-alpha)*np.exp(-xi*L)/float(polylog(1+alpha, np.exp(-xi)))

# %% ../nbs/theoretical_policy.ipynb 28
from scipy.special import zeta

def ps_exp_cutoff(L, alpha, xi, ps_0 = 1):
    
    ps = np.zeros(L)
    ps[0] = ps_0    
    
    for l in range(2, L+1):
                
        # product
        prod = np.prod(ps[:l-1])
        # P(L-1)
        p_lm1 = pw_exp(l-1, alpha, xi)
        # all together
        ps[l-1] = 1-p_lm1/prod
        
    return ps

# %% ../nbs/theoretical_policy.ipynb 33
def prob_L_composite(L, num_modes = 3, lambdas = None, probs = None):  
    # if lambdas  is None:
    #     probs = np.random.rand(num_modes)
    #     probs/= np.sum(probs)
    # if lambdas is None:
    #     lambdas = 10*np.random.rand(num_modes)
        
    return np.sum((probs)*(np.exp(1/lambdas)-1)*np.exp(-L/lambdas))

# %% ../nbs/theoretical_policy.ipynb 34
def discrete_composite_sample(lambdas = None, probs = None, num_modes = 2, L_max=1000, num_samples = 1):
    P_L = [prob_L_composite(L = l, num_modes = num_modes, lambdas = lambdas, probs = probs) for l in np.arange(1,L_max)]
    # normalize to take into accont L_max not infinity
    P_L /= np.sum(P_L)
    return np.random.choice(np.arange(1, L_max), p = P_L, size = num_samples)

# %% ../nbs/theoretical_policy.ipynb 37
def get_ps_composite(L, lambdas = None, probs = None, ps_0 = 1):
    
    ps = np.zeros(L)
    ps[0] = ps_0    
    
    for l in range(2, L+1):
                
        # product
        prod = np.prod(ps[:l-1])
        # P(L-1)
        p_lm1 = prob_L_composite(l-1, lambdas = lambdas, probs = probs)
        # all together
        ps[l-1] = 1-p_lm1/prod
        
    return ps

# %% ../nbs/theoretical_policy.ipynb 40
def prob_L_composite_cutoff(L, num_modes = 3, lambdas = None, probs = None):  
    # if lambdas  is None:
    #     probs = np.random.rand(num_modes)
    #     probs/= np.sum(probs)
    # if lambdas is None:
    #     lambdas = 10*np.random.rand(num_modes)
        
    return np.sum((probs)*(np.exp(1/lambdas)-1)*np.exp(-L/lambdas))

# %% ../nbs/theoretical_policy.ipynb 41
def get_ps_composite_cutoff(L, lambdas = None, probs = None, ps_0 = 1, Lmax = None):
    
    ps = np.zeros(L)
    ps[0] = ps_0    
    
    for l in range(2, L+1):
                
        # product
        prod = np.prod(ps[:l-1])
        # P(L-1)
        p_lm1 = discrete_composite_sample(lambdas = lambdas, probs = probs, num_modes = 2, L_max=Lmax, num_samples = 1)
        # all together
        ps[l-1] = 1-p_lm1/prod
        
    return ps

# %% ../nbs/theoretical_policy.ipynb 44
def walk_from_policy(T, policy, return_steps = False):
    '''Inputs:
    :T (int): Number of steps
    :policy (array): probability of staying in same direction at each time. If at some point
                     time > len(policy), the agent will rotate.
                     
     Ps(1) = 1 IS ALREADY CONSIDERED, DON'T INPUT IT!!!'''
    
    pos = np.zeros((T, 2))    
    direction = np.random.rand()*360 
    internal_clock = 0
    steps = []
    for t in range(1, T):   
        pos[t, 0] = pos[t-1, 0] + np.cos(direction)
        pos[t, 1] = pos[t-1, 1] + np.sin(direction)
        
        if np.random.rand() > policy[internal_clock]:
            steps.append(internal_clock)
            internal_clock = 0
            direction = np.random.rand()*360 
        else:
            internal_clock += 1
            
    if return_steps:
        return pos, steps
    else:
        return pos  

# %% ../nbs/theoretical_policy.ipynb 54
class simul_ps():
    def __init__(self, num_states, eta, gamma):     
        
        self.num_states = num_states
    
        self.h_matrix = np.ones(num_states)        
        self.eta = eta
        self.gamma_damping = gamma
        
        self.g_matrix = np.zeros(num_states)
    
    def reward(self, state, reward = 1):
        
        # updating the g-matrix
        self.g_matrix[:state] += 1        
        self.g_matrix[:state] = (self.eta**np.arange(state))[::-1]
        
        
        self.h_matrix =  self.h_matrix - self.gamma_damping * (self.h_matrix - 1.) + self.g_matrix*reward
        
        
        
        

# %% ../nbs/theoretical_policy.ipynb 55
class simul_2actions():
    def __init__(self, num_states, eta, gamma):     
        
        self.num_states = num_states
    
        self.h_matrix = np.ones((2, self.num_states)).astype(float)
        self.eta = eta
        self.gamma_damping = gamma
        
        self.reset()
        
    def reset(self):
        self.g_matrix = np.zeros((2, self.num_states)).astype(float)
        
    
#     def reward(self, state, reward = 1):
        
#         # updating the STAY part of g-matrix
#         # set to one all previous states
#         self.g_matrix[0, :state+1] += 1   
#         # multiply by eta**x all previous states
#         self.g_matrix[0, :state+1] *= (self.eta**np.arange(state+1))[::-1]
#         # multiply the rest of the matrix by number of steps don
#         self.g_matrix[0, state+1:] *= self.eta**state
        
#         # updating the ROTATE part of g-matrix
#         self.g_matrix[1, :] *= self.eta**state
#         self.g_matrix[1, state] = +1
        
#         for _ in range(state):
#             self.h_matrix -= self.gamma_damping*(self.h_matrix - 1.)
        
#         self.h_matrix += self.g_matrix*reward
        
        
        
    def reward(self, length, reward = 1):
            #NOTE: state is length-1 because counter starts in 0 
            #(but in 0, agent has already performed a step of length 1 -- from the previous action "rotate").

            factor = 1 - self.eta
            # updating the STAY part of g-matrix
            # damping before adding up the traversed edges.
            self.g_matrix[0, :length-1] *= (factor**np.arange(1,length))
            # set to one all previous states (adding up the traversed edges)
            self.g_matrix[0, :length-1] += 1   
            # multiply by eta**x all previous states
            self.g_matrix[0, :length-1] *= (factor**np.arange(1,length))[::-1]
            # multiply the rest of the matrix by number of steps don
            self.g_matrix[0, length-1:] *= factor**length

            # updating the ROTATE part of g-matrix
            self.g_matrix[1, :] *= factor**length
            self.g_matrix[1, length-1] += 1
            
            if self.gamma_damping > 0:
                for _ in range(length):
                    self.h_matrix -= self.gamma_damping*(self.h_matrix - 1.)

            self.h_matrix += self.g_matrix*reward

# %% ../nbs/theoretical_policy.ipynb 56
def discrete_pw_sample(alpha, L_max=1000, num_samples = 1):
    probs = (1/zeta(alpha+1, q = 1))*(np.arange(1, L_max).astype(float)**(-alpha-1))
    # normalize to take into accont L_max not infinity
    probs /= np.sum(probs)
    return np.random.choice(np.arange(1, L_max), p = probs, size = num_samples)
