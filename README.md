RL-OptS
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p align="center">
<img width="350" src="figs/index_fig.png">
</p>
<h4 align="center">
Reinforcement Learning of Optimal Search strategies
</h4>
<p align="center">
<a href="https://doi.org/10.5281/c"><img src="https://zenodo.org/badge/DOI/zzz/zenodo.xxx.svg" alt="DOI"></a>
<a href="https://badge.fury.io/py/a"><img src="https://badge.fury.io/py/xxx.svg" alt="PyPI version"></a>
<a href="https://badge.fury.io/py/b"><img src="https://img.shields.io/badge/python-3.9-red" alt="Python version"></a>
</p>

This library builds the necessary tools needed to study, replicate and
develop the results of the paper: [“Optimal foraging strategies can be
learned and outperform Lévy walks”](https://) by *G. Muñoz-Gil, A.
López-Incera, L. J. Fiderer* and *H. J. Briegel*.

### Installation

You can access all these tools installing the python package `rl_opts`
via Pypi:

``` python
pip install rl-opts
```

You can also opt for cloning the [source
repository](https://github.com/gorkamunoz/rl_opts) and executing the
following on the parent folder you just cloned the repo:

``` python
pip install -e rl_opts
```

This will install both the library and the necessary packages.

### Tutorials

In the left sidebar you will find a Tutorials tab, with notebooks that will
help you navigate the package as well as reproducing the results of our
paper via minimal examples. In particular, we have three tutorials:

- <a href="tutorials/tutorial_learning.ipynb" style="text-decoration:none">Reinforcement
  learning </a> : shows how to train a RL agent based on Projective
  Simulation agents to search targets in randomly distributed
  environments as the ones considered in our paper.
- <a href="tutorials/tutorial_imitation.ipynb" style="text-decoration:none">Imitation
  learning </a> : shows how to train a RL agent to imitate the policy of
  an expert equipped with a pre-trained policy. The latter is based on
  the benchmark strategies common in the literature.
- <a href="tutorials/tutorial_benchmarks.ipynb" style="text-decoration:none">Benchmarks
  </a> : shows how to obtain and launch the strategies used to benchmark the performance of the RL agents.

### Package structure

The package contains a set of modules for:

- <a href="lib_nbs/01_rl_framework.ipynb" style="text-decoration:none">Reinforcement
  learning framework (`rl_opts.rl_framework`)</a> : building foraging
  environments as well as the RL agents moving on them.
- <a href="lib_nbs/02_learning.ipynb" style="text-decoration:none">Learning
  and benchmarking (`rl_opts.learn_and_bench`)</a> : training RL agents
  as well as benchmarking them w.r.t. to known foraging strategies.
- <a href="lib_nbs/04_imitation_learning.ipynb" style="text-decoration:none">Imitation
  learning (`rl_opts.imitation`)</a>: training RL agents in imitation
  schemes via foraging experts.
- <a href="lib_nbs/04_imitation_learning.ipynb" style="text-decoration:none">Analytical
  functions (`rl_opts.analytics)`</a>: builiding analytical functions
  for step length distributions as well as tranforming these to foraging
  policies.
- <a href="lib_nbs/00_utils.ipynb" style="text-decoration:none">Utils
  (`rl_opts.utils)`</a>: helpers used throughout the package.

### Cite

We kindly ask you to cite our paper if any of the previous material was
useful for your work, here is the bibtex info:

``` latex
soon
```
